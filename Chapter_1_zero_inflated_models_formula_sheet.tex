\documentclass[11pt]{amsart}

\input{include.tex}
\input{Definitions.tex}

\begin{document}

\section{Appendix} 

\subsection{Log-likelihood}
\begin{align*}
\log p(\vy, \vtheta) &= \vy^\top \mR(\mC \vnu) - \vr^\top \exp{(\mC \vnu)} - \vone^\top \log \Gamma(\vy + \vone) \\
&+ \vr^\top \log{(\rho)} + (\vone - \vr)^\top \log{(\vone - \rho)} \\
&+ \frac{\nu}{2} \log |\mPsi| - \left( \frac{\nu + p + 1}{2} \right) \log{|\mSigma|} - \frac{1}{2} \tr (\mPsi \mSigma^{-1}) \\
&- \frac{1}{2} \log{|\sigma^2_\vbeta \mI_p|} - \frac{1}{2} \vbeta^\top \sigma_\vbeta^{-2} \mI_p \vbeta \\
&-\frac{1}{2} \log |\mG| - \frac{1}{2} \vu^\top \mG^{-1} \vu
\end{align*}

\subsection{Mean field update equations}
We are now in a position to calculate the mean field update equations for the factorised
variational approximation.
Assuming that $q(r_i) \sim \Bernoulli{(p_i)}$ then,

% Mean field update for q(\lambda)
$$
\begin{array}{rl}
	q^*(\lambda)
	  & \propto                                                            
	\lambda^{\alpha_\lambda+\vone^T\vy - 1} 
	\exp\left\{ 
	\bE_{-q(\lambda)} \left[
	-(\beta_\lambda + \vone^T\vr) \lambda 
	\right] 
	\right\} 
	\\ [0.5ex]
	  &                                                                    
	\propto \lambda^{\alpha_\lambda+\vone^T\vy - 1} \exp{\left \{-(\beta_\lambda + \vone^T\vp)\lambda \right \} } 
	\\
	  & = \myGamma{(\alpha_\lambda+\vone^T\vy, \beta_\lambda+\vone^T\vp)}, 
\end{array}
$$

% Mean field update for q(\rho)
$$
\begin{array}{rl}
	\log{q^*(\rho)} 
	  &                                                                 
	\propto \left\{ 
	\bE_{-q(\rho)}\left[ 
	\vone^T\vr \log{(\rho)} 
	+ (n - \vone^T\vr) \log{(1 - \rho)} 
	\right] 
	+ \alpha_\rho \log{(\rho)} 
	+ \beta_\rho \log{(1 - \rho)} 
	\right\} 
	\\ [0.5ex]
	  &                                                                 
	\propto \exp \left( 
	(\vone^T\vp + \alpha_\rho) \log{(\rho)} 
	+ (n - \vone^T\vp + \beta_\rho) \log{(1 - \rho)} 
	\right) 
	\\ [0.5ex]
	  & = \Beta(\alpha_\rho + \vone^T\vp, \beta_\rho + n - \vone^T\vp), 
\end{array}
$$

\noindent and

$$
\begin{array}{rl}
	\ds \log{q^*(r_i)} & \propto -\bE_{q(\lambda)} [\lambda ] r_i + y_i \log{(r_i)} + r_i \bE_{q(\rho)} \left[\log{\left(\frac{\rho}{1 - \rho}\right)}\right]          \\[0.5ex]
	                   & \ds = -r_i \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + y_i \log{(r_i)} + r_i \left(\psi(\alpha_{q(\rho)}) - \psi(\beta_{q(\rho)})\right) \\ [0.5ex]%
	                   & \ds = \text{Bernoulli}(p_i)                                                                                                                   
\end{array}
$$

\noindent where

$$
\begin{array}{rl}
	\ds p_i                                               
	= \frac{\exp{(\eta_i)}}{I(y_i = 0) + \exp{(\eta_i)}}  
	= \text{expit}(\eta_i), \quad \mbox{(when $y_i = 0$)} 
\end{array}
$$

\noindent and $\eta_i = - \alpha_{q(\lambda)}/\beta_{q(\lambda)} + \psi(\alpha_{q(\rho)}) - \psi(\beta_{q(\rho)})$.
%
%Note that if $y_i = 0$ then
%$y_i \log{(r_i)} = \log{(y_i^{r_i})} = \log{(0^0)} = \log{(1)} = 0$. Dr John %Ormerod hit
%upon the idea of side-stepping this problem by writing the q-likelihood as
%
%$$
%q^*(r_i) \propto r_i^{y_i} \exp{(r_i \eta_i)}
%$$
%
%
%Now, either $y_i = 0$ or $y_i \ne 0$.
%
%So
%
%$$
%q^*(r_i) = \text{Bernoulli}(p_i)
%$$
%
% Why do you care about these when you have the algorithm?
%\noindent So the mean field update equations are \ldots
%
%$$
%\begin{array}{ll}
%p_i &= \ds \frac{\exp{(\eta_i)}}{I(y_i = 0) + \exp{(\eta_i)}} %\\
%&= \text{expit}(\eta_i)
%\end{array}
%\begin{array}{cl}
%\alpha_{q(\rho)} &\leftarrow \alpha_\rho + \vone^T\vp \\
%\beta_{q(\rho)} &\leftarrow \beta_\rho + n - \vone^T\vp \\
%\eta &\leftarrow -\frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + \psi(\alpha_{q(\rho)}) - \psi(\beta_{q(\rho)}) \\
%\vp_{q(\vr_0)} &\leftarrow \expit{(\eta)} \\
%\alpha_{q(\lambda)} &\leftarrow \alpha_\lambda+ \vone^T\vy \\
%\beta_{q(\lambda)} &\leftarrow \beta_\lambda+ \vone^T\vp
%\end{array}
%$$
%
% The formatting of this entire section is horrible.
% It should be organised into subterms, as you will for the multivariate
% model.

% Include derivations for mean field updates at the end?

\noindent The optimal approximation for $\vr$ is
$$
\begin{array}{rl}
	q(\vr) & \propto \exp{\{\bE_{-q(\vr)}y^T\mR(\mC\vmu) - \vr^T\exp{(\mC\vnu)}-\half \vnu^T \text{diag}(\sigma_{\vu}^2)^{-1} \vnu\}} 
\end{array}
$$

$$
\begin{array}{rl}
	  & \bE_{-q(\vr)} [\vy^T\mR(C\vnu) - \vr^T\exp{(\mC\vnu)}-\half \vnu^T \text{diag}(\sigma_{\vu}^2)^{-1} \vnu]                                                 \\
	= & \vy^T\mR\mC \vmu - \vp^T \exp{\{\mC \vmu + \half \text{diag}(\mC \mLambda \mC^T)\}} - \half \vmu^T \hat{\mD} \vmu - \half \text{tr}(\mLambda \hat{\mD} )) 
\end{array}
$$

Recall that $\E \vu^\top \mA^{-1} \vu = \E \tr(\vu^\top \mA^{-1} \vu) = \tr [\E (\vu \vu^\top) \mA^{-1}]$, and
that $\mG = \text{Cov}(\vu) = \text{blockdiag}_{1 \leq i \leq m} \mSigma = \mI_m \otimes \mSigma.$ Then the
optimal approximation for $q^*(\mSigma)$ is

\begin{align*}
q^*(\mSigma) &\propto \exp \left \{ -\frac{1}{2} \tr (\mPsi \mSigma^{-1}) - \frac{1}{2} \E \vu^\top \mG^{-1} \vu
											- \frac{\nu + p + 1}{2} \log |\mSigma| - \frac{1}{2} \log |\mG| \right \} \\
						&=\exp \left \{ -\frac{1}{2} \tr \left[\mPsi + \sum_{i=1}^m (\vmu_i \vmu_i^\top + \mLambda_i) \mSigma^{-1}\right]
											- \frac{\nu + p + m + 1}{2} \log |\mSigma| \right \}.
\end{align*}

Hence, $q(\mSigma) = \text{Inverse Wishart}(\mPsi + \sum_{i=1}^m (\vmu_i \vmu_i^\top + \mLambda_i), \nu + m)$.

\subsection{Lower bound}
The lower bound $\log{\underline{p}(\vtheta)}$ is
$$
\bE_q[\log{p(\vy, \vtheta)} - \log{q(\vtheta)}]
$$

\noindent where $q(\vtheta) = q(\lambda) q(\rho) \prod_{i=1}^n q(r_i)$,
$q(\lambda) \sim \text{Gamma}{(\alpha_{q(\lambda)}, \beta_{q(\lambda)})}$,
$q(\rho) \sim \text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})$ and
$q(r_i) = 1$ if $y_i \ne 0$, and $p_i$ if $y_i = 0$, where $p_i$ is
calculated for each iteration as specified in Algorithm \ref{algorithm1}.

The lower bound can be calculated directly to be
$$
\begin{array}{rl}
	\bE_q \left\{ \log{p(\vy, \vr, \lambda, \rho)} - \log{q(\vr, \lambda, \rho)} \right\} & = T_1 + T_2 \\
\end{array}
$$

\noindent where
$$
\begin{array}{rl}
	T_1 & \ds =                                                                                                                          
	\alpha_\lambda \log{(\beta_\lambda)} + (\alpha_\lambda - 1) \bE_q[\log{(\lambda)}] - \beta_\lambda \bE_q [\lambda] - \log\Gamma(\alpha_\lambda) \\
	    & \ds \quad + \sum_{i=1}^n ( -\bE_q [\lambda] \bE_q [r_i] + \bE_q[y_i \log{(\lambda r_i)}] - \log\Gamma(y_i+1)) \quad \mbox{and} 
	\\ [1ex]
	T_2 & =\bE_q[r_i] \bE_q[\log{(\rho)}] + \bE_q[(1 - r_i)] \bE_q[\log{(1 - \rho)}]                                                     
	- \bE_q[\log{q(r)}] 
	- \bE_q[\log{q(\lambda)}] 
	- \bE_q[ \log{q(\rho)}]
\end{array}
$$

\noindent with 
$\bE[\lambda] = \alpha_{q(\lambda)}/\beta_{q(\lambda)}$,
$\bE [\lambda] = \alpha_{q(\lambda)}/\beta_{q(\lambda)}$,
% T_1 terms
$$
\begin{array}{rl}
	\bE [\log{\lambda}] & \ds = -\{ \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma(\alpha_{q(\lambda)})} + (1 - \alpha_{q(\lambda)}) \psi{(\alpha_{q(\lambda)})} \},        \\
	\\
	\bE[\log{\rho}]     & \ds = - \{ \log{\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\psi{(\beta_{q(\rho)})}  \\
	                    & \ds \qquad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \},                                                               
	\\
	\bE[r_i]            & \ds =                                                                                                                                                             
	\begin{cases}
	1                   & \text{if } y_i \ne 0                                                                                                                                              \\
	p_i                 & \text{if } y_i = 0,                                                                                                                                               \\
	\end{cases}
	\\
	-\bE_q[\log{q(r)}]  & \ds = \sum_{i=1}^n I(y_i = 0) \log{(p_i)}                                                                                                                         \\ [0.5ex]
	-\bE_q[\log{q(\lambda)}] 
	                    & \ds = \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma{(\alpha_{q(\lambda)})}} + (1 - \alpha_{q(\lambda)}) \psi{(\alpha_{q(\lambda)})},             \\ [0.5ex]
	\mbox{and}\quad \bE_q[\log{q(\rho)}] 
	                    & \ds = - \{ \log{(\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\psi{(\beta_{q(\rho)})} \\ [0.5ex]
	                    & \ds \quad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \}.                                                                \\ [0.5ex]
\end{array}
$$

\section{Appendix - Algebraic derivations of conditional likelihoods}

The joint likelihood is:

$$
p(\vy, \vr, \lambda, \rho) = \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)}}{\Gamma{(a)}} \prod_{i=1}^n \frac{\exp{(-\lambda r_i)} (\lambda r_i)^{y_i}}{y_i !} \rho^{r_i} (1 - \rho)^{1 - r_i}.
$$

%\begin{align*}
%p(\lambda|\vy, \vr, \rho) &= \frac{\prod_{i=1}^n p(y_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)}{\int \prod_{i=1}^n p(y_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)) d \lambda}.
%\end{align*}
%
%Concentrating for now on the denominator in this expression, we re-arrange and collect
%like terms to obtain
%$$
%\prod_{i=1}^n \rho^r_i (1 - \rho)^{1 - r_i} \frac{r_i^{y_i}}{y_i !}
%    \int \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)} \lambda^{y_i} \exp{(-\lambda r_i)}}{\Gamma{(a)}} d \lambda
%$$
%
%The integral in this expression is
%\begin{align*}
%& \int \frac{b^a \lambda^{(a + y_i) - 1} \exp{(-\lambda(b + r_i))}}{\Gamma{(a + y_i)}} d \lambda \frac{\Gamma{(a+ y_i)}}{\Gamma{(a)} b^{-y_i}} \\
%=& \frac{\Gamma{(a+ y_i)}}{\Gamma{(a)} b^{-y_i}}.
%\end{align*}
%
%Collecting the multiplicands in the integral over $\lambda$ together, we obtain
%$$
%\prod_{i=1}^n \rho^{r_i} (1 - \rho)^{1 - r_i} \frac{r_i^{y_i}}{y_i!}
%    \int \frac{b^{na + \sum_{i=1}^n y_i} \lambda^{(na + \sum_{i=1}^n y_i) - 1} \exp{(-\lambda(nb + \sum_{i=1}^n r_i))}}{\Gamma{(na + \sum_{i=1}^n y_i)}} d \lambda
%    \frac{\Gamma{(na + \sum_{i=1}^n y_i)}}{\Gamma{(na)} b^{-\sum_{i=1}^n y_i}}.
%$$

%By cancelling like terms in the numerator and denominator of the full likelihood we arrive 
%at

$$
\beta_{\lambda}^{\alpha_\lambda+\vone^T\vy} \lambda^{(\alpha_\lambda + \vone^T\vy) - 1} \exp{\left(-(\beta_\lambda + \vone^T\vr) \lambda \right)}
$$

$$
\frac{\rho^{\vone^T\vr} (1 - \rho)^{\vone^T(\vone - \vr)}}{\Beta{\alpha_\rho + \vone^T \vr, \beta_\rho + \vone^T(\vone - \vr)}}
$$

$$
\begin{array}{ll}
	p(r_i | \text{rest}) & \ds \propto \frac{(\lambda r_i)^{y_i} \exp{(-\lambda r_i)}}{y_i !} \rho^{r_i} (1 - \rho)^{1 - r_i} \\
	                     & \ds \propto r_i^{y_i} (e^{-\lambda})^{r_i} \rho^{r_i} (1 - \rho)^{1 - r_i}                         
\end{array}
$$

We make use of the fact that if $y_i = 0$, $r_i = 0$, and if $y_i \ne 0$,
$r_i = 1$. So $y_i^{r_i} = I(x \ne 0)$ and hence the likelihood can be re-written as

$$
\begin{array}{ll}
	p(r_i | \text{rest}) & \ds = \frac{(e^{-\lambda + \logit{(\rho)}})^{r_i}}{I(y_i = 0) + (e^{-\lambda + \logit{(\rho)}})^{r_i}} 
\end{array}
$$

\end{document}