\documentclass{amsart}
\title{Meeting with Dr John Ormerod - 15/12/2015}

\input{Definitions.tex}

\begin{document}

\maketitle

Speed things up
Reviewers? INLA, Integrated Nested Laplace Approximation - discrete latent random variables? Zero-inflated
models?
Teach Sarah to write fast code? John things I should focus on this, and she should do something else.
Fixed point approach can fall over.
Journal: CSDA
Mail Florian re: GPUs
VB compartmentalising sub-models

Linear models
Logistic models - model selection
								- latent class
Direct extension of Linear models - Network models - Project \# 3
 - Nodal regression
$X_{ij} = \vbeta_{0, j} + \sum_{k \ne j} \vbeta_{k, j} X_{i, k} + \epsilon_j$
If $\vbeta_{k, j} \ne 0 \vbeta_{j, k} \ne 0$ (Symmetry constraint)
Differential networks
Classification using these networks - LDA

Samuel Mueller:
	Sure
	Screening methods
	Dimension reduction

John had some other ideas. In particular, he liked a different type of model selection which doesn't depend
on a tuning parameter.
It relies on random matrix theory
``No penalty, no tears'' - matrix theory
Detecting non-linear dependencies

\end{document}