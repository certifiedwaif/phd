\documentclass{amsart}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
% \setlength\parindent{0pt}
% \setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{cancel}
\usepackage{amsthm}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]

\title{Chapter 3 Collapsed Variational Bayes}
\author{Mark Greenaway, John T. Ormerod}

\input{include.tex}
\input{Definitions.tex}


\begin{document}

\maketitle

\section{Assignment 3 - Bayesian Data Analysis}

=Consider the model $\vy | \vbeta, \sigma^2 \sim N_n(\mX \vbeta, \sigma^2 \mI)$ with priors
\begin{equation*}
\begin{array}{ll}
\vbeta | \sigma^2, g &\sim N_p(\vzero, g \sigma^2(\mX^\top \mX)^{-1}) \\
p(\sigma^2) &= (\sigma^2)^{-1} I(\sigma^2 > 0) \\
p(g) &= \frac{g^b (1 + g)^{-a - b - 2}}{\text{Beta}(a + 1, b + 1)} I(g > 0)
\end{array}
\end{equation*}

Assume $\vx_j^\top\vone = \vzero$, and $\vx_j^\top \vx_j / n = 1$, $1 \leq j \leq p$.
Furthermore, assume $\vy^\top \vone = \vzero, \vy^\top \vy / n  = 1, n > p, (\mX^\top \mX)^{-1}$ exists.

\subsection{Question 1}

Show that $\int \exp{(-\half \vx^\top \mA \vx + \vb^\top \vx)} = |2 \pi \mSigma|^{\half} \exp{\{ -\half \vmu^\top \mSigma^{-1} \vmu \}}$ where $\vmu = \mA^{-1} \vb$ and $\mSigma = \mA^{-1}$.

Idea: Complete the square of the argument, then integrate the resulting normal to 1.

\begin{equation*}
\begin{array}{ll}
-\half \vx^\top \mA \vx + \vb^\top \vx &= -\half (\vx^\top \mA \vx - 2 \vb^\top \vx) \\
&= -\half (\vx^\top - 2 \vb^\top \mA^{-1})^\top \mA \vx
\end{array}
\end{equation*}

Now, to remind ourselves how to complete the square, let's look at the univariate case.

\begin{equation*}
\begin{array}{rcl}
& ax^2 \mp bx &= 0 \\
& a(x^2 \mp bx) &= 0 \\
& a(x \mp \frac{b}{2a})^2 + k &= 0 \\
& a(x^2 \pm \frac{b}{a} x + \frac{b^2}{4a^2}) + k &= 0 \\
\therefore & k &= \pm \frac{b^2}{4a}
\end{array}
\end{equation*}

Similiarly,

\begin{equation*}
\begin{array}{ll}
-\half \vx^\top \mA \vx + \vb^\top \vx &= -\half(\vx^\top -2\vb^\top \mA^{-1})^\top \mA \vx \\
&= -\half [(\vx^\top - \vb \mA^{-1})^\top \mA (\vx - \vb \mA^{-1}) + \vb^\top \mA^{-1} \mA \mA^{-1} \vb] \\
&= -\half [(\vx - \vmu)^\top \mSigma^{-1} (\vx - \vmu) + \vmu^\top \mSigma^{-1} \vmu]
\end{array}
\end{equation*}

So
\begin{equation*}
\int \exp{(-\half \vx^\top \mA \vx + \vb^\top \vx)} d\vx = |2 \pi \mSigma|^{\half} \exp{(-\half \vmu^\top \mSigma^{-1} \vmu)}
\end{equation*}
as required.
\subsection{Question 2}

rtp: $p(\vy | \sigma^2, g) = \exp{\{ - \frac{n}{2} \log (2 \pi \sigma^2) - \frac{p}{2} \log(1 + g) - \frac{1}{2 \sigma^2} \| \vy \|^2  + \frac{g}{2 \sigma^2 (1 + g)} \vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy \}}$

\begin{equation*}
\begin{array}{ll}
p(\vy | \sigma^2, g) &= \int_{\bR^p} p(\vy | \vbeta, \sigma^2) p(\vbeta | \sigma^2, g) d \vbeta \\
&= \int k \exp{\{ -\half (\vy - \mX \vbeta)^\top \sigma^{-2} \mI (\vy - \mX \vbeta) - \half \vbeta^\top \vbeta \}} d \vbeta - \frac{1}{2 \sigma^2} (\vy - \mX \vbeta)^\top (\vy - \mX \vbeta) -\half \vbeta^\top \vbeta d \vbeta \\
&= -[\frac{1}{2 \sigma^2} \vy^\top \vy - \frac{\vy^\top \mX \vbeta}{\sigma^2} + \mX \vbeta^\top \mX \vbeta] - \half \vbeta^\top \vbeta
\end{array}
\end{equation*}

Let $\vmu = -\vy^\top \mX$, $\mA = \mX^\top \mX$. Then the above becomes

\begin{equation*}
\begin{array}{ll}
&= k \exp{ \{ \frac{1}{2 \sigma^2} \| \vy \|^2 - \frac{1}{2 \sigma^2} \vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy \} }
\end{array}
\end{equation*}

\subsection{Question 3}

rtp: $R^2 = \frac{\vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy}{\| \vy \|^2}$

\begin{equation*}
\begin{array}{ll}
R^2 &= 1 - \frac{(\vy - \hat{\vy})^\top (\vy - \hat{\vy})}{\vy^\top \vy} \\
&= \frac{\vy^\top \vy - (\vy - \hat{\vy})^\top (\vy - \hat{\vy})}{\vy^\top \vy} \\
&= \frac{\cancel{\vy^\top \vy} - [\cancel{\vy^\top \vy} - 2 \vy^\top \hat{\vy} + \hat{\vy}^\top \hat{\vy}]}{\vy^\top \vy} \\
&= \frac{2 \vy^\top \hat{\vy} - \hat{\vy}^\top \hat{\vy}}{\vy^\top \vy}
\end{array}
\end{equation*}

Now $\hat{\vy} = \mX (\mX^\top \mX)^{-1} \mX^\top \vy$ and
\begin{equation*}
\begin{array}{ll}
\vy &= \vy - \hat{\vy} + \hat{\vy} \\
&= [\mI - \mX (\mX^\top \mX)^{-1} \mX^\top] \vy + \mX (\mX^\top \mX)^{-1} \mX^\top \vy.
\end{array}
\end{equation*}

So the above becomes

\begin{equation*}
\begin{array}{ll}
&= \frac{2[(\mI - \mH) \vy + \mH \vy]^\top \mH \vy - \mH \vy^\top \mH \vy}{\vy^\top \vy} \\
&= \frac{\mH \vy^\top \mH \vy}{\vy^\top \vy} \\
&= \frac{\vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy}{\| \vy \|^2} \text{ as required. } (\text{as } \mH^2 = \mH).
\end{array}
\end{equation*}

\subsection{Question 4}
\begin{equation*}
\begin{array}{ll}
p(\vy | g) &= \int_0^\infty p(\vy | \sigma^2, g) p(\sigma^2) d \sigma^2 \\
&= \int_0^\infty \exp{\{ -\frac{n}{2} \log (2 \pi \sigma^2) - \frac{p}{2} \log(1 + g)
	 - \frac{1}{2 \sigma^2} \| \vy \|^2
	 + \frac{g}{2 \sigma^2 (1 + g)} \vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy \}}
	 (\sigma^2)^{-1} I(\sigma^2 > 0) d \sigma^2.
\end{array}
\end{equation*}

Recall that $\vy^\top \mX(\mX^\top \mX)^{-1} \mX^\top \vy = \| \vy \|^2 R^2$. So
\begin{equation*}
\begin{array}{ll}
& \| \vy \|^2 -\frac{g}{1 + g} \vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy \\
&= \| \vy \|^2 \left(1 - \frac{g}{1 + g}R^2 \right) \\
&= \| \vy \|^2 \left(\frac{1 + g - g}{1 + g} R^2 \right) \\
&= \| \vy \|^2 \left\{ \frac{1}{1 + g} [ 1 + g(1 - R^2) ] \right\}
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{ll}
&= \int_0^\infty 2^{-\frac{n}{2}} (\pi)^{-\frac{n}{2}} (1 + g)^{-\frac{p}{2}} \exp{\{ \| \vy \|^2 \{ \frac{1}{1 + g} [1 + g(1 - R^2)] \}} ] \}
\end{array}
\end{equation*}

Let $\alpha = -\frac{n}{2}, \beta = \half [\| \vy \|^2 \{ \frac{1}{1 + g} [1 + g(1 - R^2)] \}]$.
\begin{equation*}
\begin{array}{ll}
&= \cancel{2^{- \frac{n}{2}}} (\pi)^{-\frac{n}{2}} (1 + g)^{-\frac{p}{2}} \frac{1}{\cancel{2^{-n/2}}} \frac{1}{(1 + g)^{-\frac{n}{2}}} \frac{1}{\| \vy \|^n} [1 + g(1 - R^2)]^{-\frac{n}{2}} \\
&= \frac{\gamma(\frac{n}{2})}{\pi^{\frac{n}{2}} \| \vy \|^n} (1 + g)^{\frac{n - p}{2}} [1 + g(1 - R^2)]^{-\frac{n}{2}}
\end{array}
\end{equation*}

as required, modulo the $\frac{1}{\sigma^2}$ above which should be $\sigma^2$ somehow, sign error in the
exponent.

\subsection{Question 5}

Assume $b = (\frac{n - p}{2}) - 2 - a$.

rtp: $p(\vy) = \frac{\Gamma(\frac{n}{2})}{\pi^{n/2} \| \vy \|^n} \frac{\text{Beta}(\frac{p}{2} + a + 1, b + 1)}{\text{Beta}(a + 1, b + 1)} (1 - R^2)^{-(b + 1)}$

\begin{equation*}
\begin{array}{ll}
p(y) &= \int_0^\infty p(y|g) p(g) dg \\
&= \int_0^\infty \frac{\Gamma(\frac{n}{2})}{\pi^{n/2} \| y \|^n} (1 + g)^{\frac{n - p}{2}}[1 + g(1 - R^2)]^{-\frac{n}{2}} \frac{g^b (1 + g)^{-a - b - 2}}{\text{Beta}(a + 1, b + 1)} I(g > 0) dg \\
&= \frac{\Gamma(\frac{n}{2})}{\text{Beta}(a + 1, b + 1) \pi^{n/2} \| y \|^n}
\int_0^\infty \frac{g^b (1 + g)^{(n - p)/2}}{(1 + g)^{a - b - 2}} \frac{1}{[1 + g(1 - R^2)]^{n/2}} dg
\end{array}
\end{equation*}

Let $\mu = b + 1$. Then

\begin{equation*}
\begin{array}{ll}
&=\frac{\Gamma(\frac{n}{2})}{\text{Beta}(a + 1,  b + 1) \pi^(n/2) \| y \|^n}
\int_0^\infty \frac{g^b(1 + g)^{(n - p)/2}}{(1 + g)^{(a - b - 2)}} \frac{1}{[1 + g(1 - R^2)]} dg \\
&= \frac{\Gamma(\frac{n}{2})}{\text{Beta(a + 1, b + 1)} \pi^{n/2) \| y \|^n}}
\int_0^\infty \frac{g^{\mu - 1}}{[1 + g (1 - R^2)]^{n/2}} dg
\end{array}
\end{equation*}

Now, $\int_0^\infty \frac{g^{\mu - 1}}{(1 + \beta g)^\nu} dg = \beta^{-\mu} \text{Beta}(\mu, \nu - \mu), \text{ for } \mu > 0, \nu > 0, \nu > \mu$.
\begin{equation*}
\begin{array}{ll}
&= \frac{\Gamma(\frac{n}{2})}{\text{Beta}(a + 1, b + 1) \pi^{n / 2} \| y \|^n} (1 - R^2)^{-(b + 1)} \text{Beta}(b + 1, \frac{n}{2} - b + 1) \\
&= \frac{\Gamma(\frac{n}{2})}{\text{Beta}(a + 1, b + 1) \pi^{n / 2} \| y \|^n} (1 - R^2)^{-(b + 1)} \text{Beta}(\frac{p}{2} + a + 1, b + 1)
\end{array}
\end{equation*}

by the symmetry of beta functions and the definitions of $a$ and $b$.

\section{Variational Bayes approximation}

\def \I {\text{I}}

Consider the model $\vy | \vbeta, \sigma^2 \sim \text{N}_n(\mX \mGamma \vbeta, \sigma^2 \mI)$ with
priors
\begin{equation*}
\begin{array}{ll}
\vbeta | \sigma^2, g &\sim \text{N}_p(\vzero, g \sigma^2(\mX^\top \mGamma \mX)^{-1}), \\
\vgamma_i &\sim \text{Bernoulli}(\rho), 1 \leq i \leq n, 0 \leq \rho \leq 1, \\
p(\sigma^2) &= (\sigma^2)^{-1} \I(\sigma^2 > 0) \text{ and }\\
p(g) &= \frac{g^b (1 + g)^{-a - b - 2}}{\text{Beta}(a + 1, b + 1)} I(g > 0).
\end{array}
\end{equation*}

Assume $\vx_j^\top\vone = \vzero$, and $\vx_j^\top \vx_j / n = 1$, $1 \leq j \leq p$.
Furthermore, assume $\vy^\top \vone = \vzero, \vy^\top \vy / n  = 1, n > p, (\mX^\top \mX)^{-1}$ exists.

Assume an approximation of the form

\begin{equation*}
q(\vtheta) = q(\vbeta) q(\vgamma) q(\sigma^2) q(g)
\end{equation*}
where
\begin{equation*}
\begin{array}{ll}
q^*(\vbeta) &= \text{N}(\vmu_{q(\vbeta), \mSigma_{q(\vbeta)}}), \\
q^*(\vgamma) &= \text{Bernoulli}(p_{q(\vgamma)}), \\
q^*(\sigma^2) &= \text{IG}(\alpha_{q(\sigma^2)}, \beta_{q(\sigma^2)}) \text{ and } \\
q^*(g) &= \text{Beta Prime}(\alpha_{q(g)}, \beta_{q(g)})). \\
\end{array}
\end{equation*}

\begin{equation*}
q_i^*(\vtheta_i) \propto \exp{ \bE_{-\vtheta_i} \log{p(\vy, \vtheta)} }, 1 \leq i \leq M
\end{equation*}

\begin{equation*}
\begin{array}{ll}
p(\vy, \vtheta) &= \quad p(\vy | \vbeta, \vgamma, \sigma^2) p(\vbeta | \vgamma, \sigma^2, g) p(\sigma^2) p(g) \\
\log{p(\vy, \vtheta)} &= \quad \log{p(\vy | \vbeta, \vgamma, \sigma^2)} + \log{p(\vbeta | \vgamma, \sigma^2, g)} + \log{p(\sigma^2)} + \log{p(g)} \\
&= -\frac{n}{2} \log{2 \pi} -\half \log |\sigma^2 \mI_n| - \half (\vy - \mX \text{diag}(\vgamma) \vbeta)^\top \sigma^{-2} \mI_n (\vy - \mX \text{diag}(\vgamma) \vbeta) \\
& \quad -\frac{p}{2} \log{2 \pi} - \half \log |g \sigma^2 (\mX^\top \text{diag}(\vgamma) \mX)^{-1}| -
	\half \vbeta^\top (g \sigma^2)^{-1} (\mX^\top \mX) \vbeta \\
& \quad + \vone^\top \vgamma \log(\rho) + \vone^\top (\vone - \vgamma) \log(1 - \rho)\\
& \quad - \log{(\sigma^2)} + \log \bI (\sigma^2 > 0) \\
& \quad + b \log g + (-a - b - 2) \log(1 + g) - \log \text{Beta}(a + 1,  b + 1) + \log \bI(g > 0)\\
& = -\frac{n + p}{2} \log(2 \pi) - \cancel{\half \log{\sigma^{2n}}} \cancel{|\mI_n|} 
		+\half \log{g^n \cancel{\sigma^{2n}}} |(\mX^\top \text{diag}(\vgamma) \mX)| \\
&\quad		-\half (\vy - \mX \text{diag}(\vgamma) \vbeta)^\top \sigma^{-2} \mI_n (\vy - \mX \text{diag}(\vgamma) \vbeta)\\
&\quad		-\half \vbeta^\top (g \sigma^2)^{-1} (\mX^\top \text{diag}(\vgamma)^{-1} \mX) \vbeta [= -\half (g \sigma^2)^{-1} (\mX \vbeta)^\top \text{diag}(\vgamma)^{-1} (X \vbeta)]\\
&\quad - \log(\sigma^2) + \cancel{\log \bI(\sigma^2 > 0)} \\
&\quad + b \log g + (-a - b - 2) \log(1 + g) - \log \text{Beta}(a + 1,  b + 1) + \cancel{\log \bI(g > 0)}
\end{array}
\end{equation*}

\[
	\E[\vbeta | \sigma^2, g, \vy] = \frac{g}{1 + g} \hat{\vbeta}_{\text{LS}}
\]

\begin{equation*}
\begin{array}{rl}
\E[\frac{g}{1 + g}] &= \frac{1}{\Beta(a +1, b + 1)} \int g^{b+1} (1 + g)^{-(a + b + 3)} dg \\
&= \frac{\Beta(a + 2, b + 2)}{\Beta(a + 1, b + 1)} \\
&= \frac{\Gamma(a + 2) \Gamma(b + 2)}{\Gamma(a + b + 4)} \frac{\Gamma(a + b + 2)}{\Gamma(a + 1) \Gamma(b + 1)} \\
&=  \frac{(a + 1) \cancel{\Gamma(a + 1)} (b + 2) \cancel{\Gamma(b + 1)}}{(a + b + 3) (a + b + 2) \cancel{\Gamma(a + b + 2)}} \frac{\cancel{\Gamma(a + b + 2)}}{\cancel{\Gamma(a + 1)} \cancel{\Gamma(b + 1)}} \\
&= \frac{(a + 1) (b + 1)}{(a + b + 3) (a + b + 2)}
\end{array}
\end{equation*}

using the well-known property of gamma functions that $\Gamma(n + 1) = n \Gamma(n)$.

Optimal choices of $a$ and $b$ are $a = -\frac{3}{4}$ and $b=\frac{n-p}{2} - 2 - a$.

\begin{equation*}
\begin{array}{rl}
q^*_g(a, b) &\propto \exp \{(\frac{n}{2} + b) \log (g) + [-(a + 1) - (b + 1)] \log{(1 + g)} \\
&\qquad \quad \frac{1 + g}{g} \E [\frac{1}{1 \sigma^2}] \E[\vbeta^\top \Gamma^\top \mX^\top \mX \Gamma \vbeta - 2 \mX \Gamma \vbeta^\top \vy + \vy^\top \vy] \} \\
&\propto \exp \{ (\frac{p}{2} + b) \log (g) - [(a + 1) + (b + 1)] \log (1 + g) - \frac{1}{g}[\tr(\mSigma_{q(\vbeta)} + \| \mX \vmu_{q(\vbeta)} \|^2)] \} \\
&= g^{v-1} (\lambda + g)^{\mu - 1} \exp{\left (-\frac{\beta}{g}\right )}
\end{array}
\end{equation*}

\def \Re {\text{Re}}

which is the special function ET II 234 (13) a when $\Re(1 - \mu) > \Re(\nu) > 0, | \arg \lambda | < \pi$. Here
$v = \frac{p}{2} + b + 1, 1 - \mu = (a + b + 2)$. Choose $a = \frac{3}{4}, b = \frac{n - p}{2} - 2 - a$.

\begin{equation*}
\begin{array}{rl}
q^*_\gamma (p) &\propto \exp \{ \quad \frac{n}{2} \log |\mX^\top \mGamma \mX|
															+ \half \E[\frac{1}{\sigma^2}] \E[\frac{1 + g}{g}] 
																\E[\vbeta^\top \mGamma^\top \mX^\top \mX \mGamma \vbeta - 2 \mX \mGamma \vbeta^\top y]\} \\
&= \exp \{\quad \frac{n}{2} \log{|\mX^\top \mX|} \vone^\top \log \vgamma - \half [\log (\beta_{q(\sigma^2)}) - \phi(\alpha_{q(\sigma^2)})] \\
&\qquad \quad + |2 \pi (\mX^\top \Gamma \mX)^{-1}|^{1/2} \exp(-\half \vy^\top \mGamma \mX^\top (\mX^\top \mGamma \mX)^{-1} \mX \mGamma \vy) \}
\end{array}
\end{equation*}

We will use the logit trick $\frac{\exp(\eta)}{1 + \exp(\eta)}$ to turn this into a likelihood.

\begin{equation*}
\begin{array}{rl}
q^*_\vbeta(\vmu_{q(\vbeta)}, \mSigma_{q(\vbeta)}) &\propto \exp \{ -\half \E [\frac{1}{\sigma^2}] \vbeta^\top \E[ \mGamma^\top \mX^\top \mX \mGamma ] \vbeta - 2 \mX \E \mGamma \vbeta^\top \vy \} \\
&= \exp\{ -\half \frac{\alpha_{q(\sigma^2)}}{\beta_{q(\sigma^2)}} [\vbeta^\top \mX^\top \mX \odot \E[\vgamma \vgamma^\top] \vbeta - 2 \mX \E \mGamma \vbeta^\top \vy] \} \\
&= \exp\{ -\half \frac{\alpha_{q(\sigma^2)}}{\beta_{q(\sigma^2)}} [\vbeta^\top \mX^\top \mX \odot \diag{(\vp (\vone - \vp))} \vbeta - 2 \mX \E \mGamma \vbeta^\top \vy] \}
\end{array}
\end{equation*}

\begin{equation*}
q^*_{\sigma^2}(\alpha_{q(\sigma^2)}, \beta_{q(\sigma^2)}) = 
\end{equation*}

\section{Variational Bayes with a different prior}

Consider the linear model

\begin{align*}
\vy | \vbeta, \sigma^2 \sim \N_n(\mX \vbeta, \sigma^2 \mI)
\end{align*}

with priors

\begin{align*}
\vbeta | \sigma^2, g &\sim \N_p(\vzero, g \sigma^2 (\mX^T \mX)^{-1}) \\
p(\sigma^2) &= (\sigma^2)^{-1} \I(\sigma^2 > 0) \\
p(g) &= \frac{g^b (1 + g)^{-[(a + 1) + (b + 1)]}}{\Beta(a + 1, b + 1)} \I(g > 0)
\end{align*}

The log likelihood of the full model is then

\begin{align*}
\log p(\vy, \theta) &= \quad\log p(\vy|\vbeta, \sigma^2, \vgamma) + \log p(\vbeta | \sigma^2, g) +  \log p(g) \\
&= -\half \log(|2 \pi \sigma^2 \mI_n|) - \half (\vy - \mX \vbeta)^\top \sigma^{-2} (\vy - \mX \vbeta) \\
&\quad -\half \log(|2 \pi g \sigma^2 (\mX^\top \mX)^{-1}|) -\half \vbeta^\top \sigma^{-2} g^{-1} (\mX^T \mX) \vbeta \\
&\quad - \log (\sigma^2) \\
&\quad + b \log g - [(a + 1) + (b + 1)] \log (1 + g) - \log \Beta(a + 1, b + 1)
\end{align*}

For convenience, we will denote $\tau_{\sigma^2} = \E_q \left[ \frac{1}{\sigma^2} \right]$ and $\tau_{g} =
\E_q \left[ \frac{1}{g} \right]$.


\subsection{$q(\vbeta)$}

\begin{align*}
\log q_\vbeta^*(\vmu_{q(\vbeta)}, \mSigma_{q(\vbeta)}) &\propto \E_q[-\half (\mX \vbeta - \vy)^\top \sigma^{-2} \mI_n (\mX \vbeta - \vy) - \half \vbeta^\top \sigma^{-2} g^{-1} \mI_p (\mX^\top \mX) \vbeta] \\
&= \E_q \left[-\frac{1}{2} \frac{1}{\sigma^2} (1 + g^{-1})\vbeta^\top \mX^\top \mX \vbeta + \frac{1}{\sigma^2}\vbeta^\top \mX^\top \vy \right]
\end{align*}

and thus
$q^*(\vbeta) \sim \N (\vmu_{q(\vbeta)}, \mSigma_{q(\vbeta)} )$
where $\vmu_{q(\vbeta)} = (1 + \tau_g)^{-1} (\mX^\top \mX)^{-1} \mX^\top \vy$,
$\mSigma_{q(\vbeta)} = [\tau_{\sigma^2}(1 + \tau_{g})]^{-1}(\mX^\top \mX)^{-1}$.

\subsection{$q(\sigma^2)$}
\begin{align*}
\log q_\sigma^2(\alpha_{q(\sigma^2)}, \beta_{q(\sigma^2)}) &\propto \E_{-\sigma^2} [-\half \log |2 \pi \sigma^2 \mI_n| - \half (\vy - \mX \vbeta)^\top \sigma^{-2} \mI_n (\vy - \mX \vbeta) - \half \log |2 \pi g \sigma^2 (\mX^\top \mX)^{-1}| - \half \vbeta^\top \sigma^{-2} \frac{1}{g} (\mX^\top \mX) \vbeta] \\
&= \E_{-\sigma^2} \left[- \frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2}(\vy - \mX \vbeta)^\top(\vy - \mX \vbeta) - \frac{p}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \frac{1}{g} \vbeta^\top (\mX^\top \mX) \vbeta - \log \sigma^2\right] \\
&= {-\left[\frac{n + p}{2} + 1\right] \log \sigma^2 -  \frac{1}{\sigma^2} \E_{q(\vbeta)} \left[ \frac{1}{2} \left(1 + \frac{1}{g} \right) (\mX \vbeta)^\top (\mX \vbeta) - (\mX \vbeta)^\top \vy \right]} \\
&= -\left[\frac{n + p}{2} + 1\right] \log \sigma^2  + \frac{1}{\sigma^2} \E - \frac{1}{2}\left[\left(1 + \frac{1}{g}\right)\vbeta^\top \mX^\top \mX \vbeta + \vy^\top \mX \E \vbeta \right] \\
&= -\left[ \frac{n + p}{2} + 1 \right]\log \sigma^2 + \frac{1}{\sigma^2} - \frac{1}{2} \left[ \left( 1 + \frac{1}{g} \right) [\vmu_{q(\vbeta)}^\top \mX^\top \mX \vmu_{q(\vbeta)} + \tr(\mX^\top \mX \mSigma_{q(\vbeta)})] + \vy^\top \mX \vmu_{q(\vbeta)} \right]
\end{align*}

\subsection{$q(g)$}
\begin{align*}
\log q(g) &\propto \E_{-g} [-\half \log g - \frac{1}{g} \frac{1}{2 \sigma^2} \vbeta^\top \mX^\top \mX \vbeta + b \log g - [(a + 1) + (b + 1)] \log(1 + g) - \log \Beta(a + 1, b + 1)]] \\
&= -\half \log g - \frac{1}{g} \half \E \frac{1}{\sigma^2} \E (\mX \vbeta)^\top (\mX \vbeta) + b \log g - [(a + 1) + (b + 1)] \log(1 + g) - \log \Beta(a + 1, b + 1)] \\
&= -\half \log g - \frac{1}{g} \half \E \frac{1}{\sigma^2} \tr[(\mSigma + \vmu \vmu^T) \mX^\top \mX] + b \log g - [(a + 1) + (b + 1)] \log(1 + g) - \log \Beta(a + 1, b + 1)] \\
\end{align*}

\section{Collapsed Variational Bayes}


\section{Variational Bayes with a different prior, with model selection}

Consider the linear model

\begin{align*}
\vy | \vbeta, \sigma^2 \sim \N_n(\mX \vbeta, \sigma^2 \mI)
\end{align*}

with priors

\begin{align*}
\vbeta | \sigma^2, g &\sim \N_p(\vzero, g \sigma^2 (\mX^T \mX)^{-1}) \\
p(\sigma^2) &= (\sigma^2)^{-1} \I(\sigma^2 > 0) \\
p(g) &= \frac{g^b (1 + g)^{-[(a + 1) + (b + 1)]}}{\Beta(a + 1, b + 1)} \I(g > 0)
p(\gamma) &= \text{???}
\end{align*}

The log likelihood of the full model is then

\begin{align*}
\log p(\vy, \theta) &= \quad\log p(\vy|\vbeta, \sigma^2, \vgamma) + \log p(\vbeta_\vgamma | \sigma^2, g) + \log(\vbeta_{-\vgamma}) + \log p(g) \\
&= -\half \log(|2 \pi \sigma^2 I_n|) - \half (\vy - X \mGamma \vbeta)^\top \sigma^{-2} (\vy - \mX \mGamma \vbeta) \\
&\quad -\half \log(|2 \pi g \sigma^2 (\mX^\top \mX)^{-1}|) -\half \vbeta^\top \sigma^{-2} (\mX^T \mX) \vbeta \\
&\quad - \log (\sigma^2) \\
&\quad + b \log g - [(a + 1) + (b + 1)] \log (1 + g) - \log \Beta(a + 1, b + 1)
\end{align*}

\subsection{$q(\vbeta)$}
\begin{align*}
\log q_\vbeta^*(\vmu_{q(\vbeta)}, \mSigma_{q(\vbeta)}) &\propto \E[-\half (\mX \Gamma \vbeta - \vy)^\top \sigma^{-2} I (\mX \Gamma \vbeta) - \half \vbeta^\top \sigma^{-2} (\mX^\top \mX) \vbeta] \\
&= \E[-\frac{1}{2 \sigma^2} (\mX \Gamma \vbeta)^\top (\mX \Gamma \vbeta) - 2 \mX \Gamma \vbeta^\top \vy + (\mX \vbeta)^\top (\mX \vbeta)] \\
&= \E[-\frac{1}{2 \sigma^2} (\mX \vbeta)^\top (\mX \vbeta) \odot \Gamma \Gamma^\top + (\mX \vbeta)^\top (\mX \vbeta) - 2 \mX \Gamma \vbeta^\top \vy] \\
&= \E[-\frac{1}{2 \sigma^2} (\mX \vbeta)^\top (\mX \vbeta) \odot (\vone + \Gamma \Gamma^\top) - 2 \mX \Gamma \vbeta^\top \vy] \\
&= -\half \E[\frac{1}{\sigma^2}] [(\mX \vbeta)^\top (\mX \vbeta) \odot (\vone + \E \Gamma \Gamma^\top) - 2 \E \Gamma \mX \vbeta^\top \vy]
\end{align*}

\subsection{$q(\sigma^2)$}
\begin{align*}
\log q_\sigma^2(\alpha_{q(\sigma^2)}, \beta_{q(\sigma^2)}) &\propto \E_{-\sigma^2} [-\half \log |2 \pi \sigma^2 \mI_n| - \half (\vy - \mX \Gamma \vbeta)^\top \sigma^{-2} \mI_n (\vy - \mX \Gamma \vbeta) - \half \log |2 \pi g \sigma^2 (\mX^\top \mX)^{-1}| - \half \vbeta^\top \sigma^{-2} (\mX^\top \mX) \vbeta] \\
&= \E_{-\sigma^2} [- \frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2}(\vy - \mX \Gamma \vbeta)^\top(\vy - \mX \Gamma \vbeta) - \frac{p}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \vbeta^\top (\mX^\top \mX) \vbeta - \log \sigma^2] \\
&= \E_{-\sigma^2} {-\frac{n + p + 1}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \E_{\vbeta, \Gamma} [(\mX \Gamma \vbeta)^\top (\mX \Gamma \vbeta) + (\mX \vbeta)^\top (\mX \vbeta) - 2 (\mX \Gamma \vbeta)^\top \vy]} \\
&= -\frac{n + p + 1}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \{\E [(\mX \vbeta)^\top (\mX \vbeta)] \odot (\E \Gamma \Gamma^\top + \vone) - 2\vy^\top \mX \E \Gamma \E \vbeta\}\\
&= -\frac{n + p + 1}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \{\tr[(\mSigma + \vmu \vmu^T) \mX^\top \mX] \odot (\E \Gamma \Gamma^\top + \vone) - 2\vy^\top \mX \mP \vmu\}\\
\end{align*}

\subsection{$q(g)$}
\begin{align*}
\log q(g) &\propto \E_{-g} [-\half \log g - \frac{1}{g} \frac{1}{2 \sigma^2} \vbeta^\top \mX^\top \mX \vbeta + b \log g - [(a + 1) + (b + 1)] \log(1 + g) - \log \Beta(a + 1, b + 1)]] \\
&= -\half \log g - \frac{1}{g} \half \E \frac{1}{\sigma^2} \E (\mX \vbeta)^\top (\mX \vbeta) + b \log g - [(a + 1) + (b + 1)] \log(1 + g) - \log \Beta(a + 1, b + 1)] \\
&= -\half \log g - \frac{1}{g} \half \E \frac{1}{\sigma^2} \tr[(\mSigma + \vmu \vmu^T) \mX^\top \mX] + b \log g - [(a + 1) + (b + 1)] \log(1 + g) - \log \Beta(a + 1, b + 1)] \\
\end{align*}

\subsection{$q(\gamma)$}
\begin{align*}
\log q(\gamma) &\propto \E_{-\gamma} -\half (\vy - \mX \Gamma \vbeta)^T \sigma^{-2} \mI_n (\vy - \mX \Gamma \vbeta)
\end{align*}

\section{Collapsed Variational Bayes}


\section{A nice theorem.}
Let
\begin{equation*}
R^2_{\mX, \mZ} = 
\begin{pmatrix}
\vy^\top \mX & \vy^\top \mZ
\end{pmatrix}
\begin{pmatrix}
\mX^\top \mX & \mX^\top \mZ \\
\mZ^\top \mX & \mZ^\top \mZ + \mZ^\top \mZ \odot (\mI - \mW^{-1})
\end{pmatrix}^{-1}
\begin{pmatrix}
\mX^\top \vy\\
\mZ^\top \vy
\end{pmatrix}.
\end{equation*}

Definition:
$f(x) = \BigO(g(x)) \text{ as } x \to a$

if and only if there exist positive numbers $\delta$ and $M$ such that

$|f(x)| \leq M |g(x)| \text{ for } |x - a| < \delta$.

Definition:
The Frobenius norm of a matrix $\mA$ is defined as
\begin{align*}
\| \mA \| = \tr{(\mA^\top \mA)}^{\frac{1}{2}}
\end{align*}

The trace of a projection matrix is equal to the dimension of the subspace that it projects onto, and thus
the Frobenius norm of a projection matrix is equal to the square root of the dimension of that subspace.

Lemma:
If $\mA$ and $\mB$ are symmetric, and their product $\mA \mB$ is symmetric, then $\mA$ and $\mB$ commute.
\begin{proof}
$\mA \mB = (\mA \mB)^T = \mB^T \mA^T = \mB \mA$
\end{proof}

Lemma:
If $\mB$ is symmetric, then
\[
(\mA \odot \mB)^{-1} = \mA^{-1} \odot \frac{1}{\mB}.
\]
\begin{proof}
\begin{align*}
(\mA \odot \mB)\left(\mA^{-1} \odot \frac{1}{\mB}\right)_{ij} &= \sum_{k=1}^n \mA_{ik} \mB_{ik} \mA^{-1}_{kj} \frac{1}{\mB_{kj}} \\
% &= \begin{cases}
% 1 \mB_{ik} \frac{1}{\mB_{kj}} \text{ when i=j, for \mB symmetric} \\
% 0 \text{ otherwise}
% \end{cases} \\
% &= \sum_{i=1}^n \mA_{ik} \mB_{ik} \mA^{-1}_{kj} \frac{1}{\mB_{kj}} \\
% &= \sum_{k=1}^n \cancel{\mB_{ik}} \frac{1}{\cancel{\mB_{ik}}} \mA_{ik} \mA^{-1}_{kj}, i = j \\
% &=\sum_{k=1}^n \mA_{ik} \mA^{-1}_{kj} \mB_{ik} \frac{1}{\mB_{kj}} \\
&=
\begin{cases}
\sum_{k=1}^n \cancel{\mB_{ik}} \frac{1}{\cancel{\mB_{ik}}} \mA_{ik} \mA^{-1}_{kj} = 1, \text{when $\mB$ is symmetric and $i = j$} \\
\sum_{i=1}^n \mA_{ik} \mA^{-1}_{kj} \mB_{ik} \frac{1}{\mB_{kj}}
= \mB_{i.} \odot (\mA^T_{i.} \mA^{-1}_{.j}) \odot \frac{1}{\mB_{.j}}
= \mB_{i.} \odot (0) \odot \frac{1}{\mB_{.j}}
= 0, \text{ when $i \ne j$}
\end{cases}\\
&= \mI
\end{align*}
\end{proof}

% \begin{theorem}
Theorem:
\[
R^2_{\mX, \mZ} = R^2_{\mX} + \BigO(w_*) \text{ where } w_* = \max{\vw}.
\]
% \end{theorem}

\begin{proof}
Let $n$ be the number of rows of both $\mX$ and $\mZ$, $p_\mX$ be the number of columns of $\mX$, and $p_\mZ$
be the number of columns of $\mZ$. Denote the projections onto the subspaces spanned by the columns of $\mX$
and $\mZ$ by $\mP_\mX$ and $\mP_\mZ$, and the projections onto the complementary subspaces by
$\mP_{\mX^\perp}$ and $\mP_{\mZ^\perp}$.

\small
\begin{equation*}
\begin{array}{ll}
&\vy^\top \vy R^2_{\mX, \mZ} \\
=&
\begin{pmatrix}
\vy^\top \mX & \vy^\top \mZ
\end{pmatrix}
\begin{pmatrix}
\mX^\top \mX & \mX^\top \mZ \\
\mZ^\top \mX & \mZ^\top \mZ - \mZ^\top \mZ \odot (\mI - \mW^{-1})
\end{pmatrix}^{-1}
\begin{pmatrix}
\mX^\top \vy\\
\mZ^\top \vy
\end{pmatrix} \\
=&
\begin{pmatrix}
\vy^\top \mX & \vy^\top \mZ
\end{pmatrix}
\begin{bmatrix}
\begin{pmatrix}
\mX^\top \mX & \mX^\top \mZ \\
\mZ^\top \mX & \mZ^\top \mZ
\end{pmatrix}
\odot
\begin{pmatrix}
\vone & \vone \\
\vone & \vone - (\mI - \mW^{-1})
\end{pmatrix}
\end{bmatrix}
^{-1}
\begin{pmatrix}
\mX^\top \vy\\
\mZ^\top \vy
\end{pmatrix} \\
=&
\begin{pmatrix}
\vy^\top \mX & \vy^\top \mZ
\end{pmatrix}
\begin{pmatrix}
(\mX^\top \mP_{\mZ^\perp} \mX)^{-1} & -(\mX^\top \mX)^{-1} \mX^\top \mZ (\mZ^\top \mP_{\mX^\perp} \mZ)^{-1} \\
-(\mZ^\top \mZ)^{-1} \mZ^\top \mX (\mX^\top \mP_{\mZ^\perp}) \mX)^{-1} & (\mZ^\top \mP_{\mX^\perp} \mZ)^{-1}
\end{pmatrix}
\odot
\begin{pmatrix}
\vone & \vone \\
\vone & \frac{1}{\vone - (\mI - \mW^{-1})}
\end{pmatrix}
\begin{pmatrix}
\mX^\top \vy\\
\mZ^\top \vy
\end{pmatrix} \\
=&
\begin{pmatrix}
\vy^\top \mX(\mX^\top \mP_{\mZ^\perp} \mX)^{-1} - \vy^\top \mZ(\mZ^\top \mZ)^{-1} \mZ^\top \mX (\mX^\top \mP_{\mZ^\perp} \mX)^{-1}\\
-\vy^\top \mX(\mX^\top \mX)^{-1} (\mZ^\top \mP_{\mX^\perp} \mZ)^{-1} + \vy^\top \mZ(\mZ^\top \mP_{\mX^\perp} \mZ)^{-1}
\end{pmatrix}
\odot
\begin{pmatrix}
\vone & \vone \\
\vone & \frac{1}{\vone - (\mI - \mW^{-1})}
\end{pmatrix}
^\top
\begin{pmatrix}
\mX^\top \vy \\
\mZ^\top \vy
\end{pmatrix} \\
=&\quad\vy^\top \mX (\mX^\top \mP_{\mZ^\perp} \mX)^{-1} \mX^\top \vy - \vy^\top \mZ(\mZ^\top \mZ)^{-1} \mZ^\top \mX (\mX^\top \mP_{\mZ^\perp}) \mX)^{-1} \mX^\top \vy \\
&-\vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \mZ (\mZ^\top \mP_{\mX^\perp} \mZ)^{-1} \mZ^\top \vy + \frac{1}{1 + \diag{(w^{-1} - 1)}} \odot \vy^\top \mZ (\mZ^\top \mP_{\mX^\perp} \mZ)^{-1} \mZ^\top \vy \\
=& \vy^\top \mP_{\mZ^\perp} \mX (\mX^\top \mP_{\mZ^\perp} \mX))^{-1} \mX^\top \mP_{\mZ^\perp} \vy
+ (\vone + \mW - \mI) \vy^\top \mP_{\mX^\perp} \mZ (\mZ^\top \mP_{\mX^\perp} \mZ)^{-1} \mZ^\top \mP_{\mX^\perp} \vy \\
\leq& \| \vy \|^2 [\sqrt{p_\mZ} + (w_* - 1) \sqrt{p_\mZ}] \\
=& \| \vy \|^2 \sqrt{p_\mZ} w_* \\
\leq& M w_* \text{ as required}.
\end{array}
\end{equation*}
 
\end{proof}

\end{document}