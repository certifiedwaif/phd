\documentclass{amsart}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
% \setlength\parindent{0pt}
% \setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{cancel}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{algorithm,algorithmic}

\newtheorem{theorem}{Theorem}[section]

\title{Chapter 3 Collapsed Variational Bayes}
\author{Mark Greenaway, John T. Ormerod}

\input{include.tex}
\input{Definitions.tex}

\begin{document}

\maketitle

\section*{Abstract}

% What is done in general

We develop mean field and structured variational Bayes approximations for Bayesian model selection on linear
models using Zellner's g prior. Our mean field updates only depend on a single variational parameter $\tau_g$
and other values which are fixed for each model considered. Using rank-1 updates, we are able to exhaustively
search the model space in $\BigO(2^p np^2)$ rather than $\BigO(2^p np^3)$. An algorithm is developed which
allows these models to be fit in parallel. Applications to a range of data sets are presented, showing 
empirically that our method performs well on real-world data. Our method is computationally more efficient 
than the exact Bayesian model.

\section{Introduction}

% Problem in general
The problem of model selection is one of the most important problems encountered in practice by
applied statistical practitoners. There are many approaches to model selection including approaches based on
functions of the residual sum of squares, lasso and L1 regression and Bayesian modelling approaches. A major
motivation for this field of research is the need for a computationally feasible approach to performing model 
selection on large scale problems where the number of covariates is large.

% Bayesian
The Bayesian approach to model selection has the advantage of not relying on a fixed choice of tuning
parameter, instead allowing the specification of a prior distribution on that parameter. In this sense, the 
choice of covariates for the optimal model selected can be considered to be automatic. This choice can be
made while inducing sparsity.

% Background


% VB in general

Variational Bayes (see \cite{Ormerod2010}) is a computationally efficient, deterministic method of fitting
Bayesian models to data. Variational Bayes approximates the true posterior $p(\vy, \vtheta)$ by minimising the
KL divergence between the posterior and the  approximating distribution $q(\vtheta)$.

% Application

% VB theory

% Our main contribution
In this paper, we develop variational Bayes approximations to model selection of linear models using
Zellner's g prior as in \cite{Liang2008}.

This article is organised as follows. In Section \ref{sec:model_selection}, we review previous Bayesian
approaches to model selection. In Section \ref{sec:methodology} we develop our approach. In Section
\ref{sec:num_exp} we perform a series of numerical experiments to show the accuracy of our approach. Finally,
in Section \ref{sec:conclusion}, we provide a Conclusion and Discussion.

\section{Bayesian linear model selection}
\label{sec:model_selection}

Model selection attempts to balance goodness of fit against model complexity, neither overfitting nor
underfitting. Several model selection criteria have been proposed to balance these competing concerns, such as
AIC (\cite{DeLeeuw1992}), BIC (\cite{Schwarz1978}), Mallow's $C_p$ and DIC. As \cite{Breiman1996} and
\cite{Efron2013} showed, while  the standard formulation of a linear model is unbiased, the goodness of fit of
these models is numerically  unstable. Breiman showed that by introducing a penalty on the size of the
regression co- efficients such as  in ridge regression, this numerical stability can be avoided. This reduces
the variances of the co-efficient estimates, at the expense of introducing some bias --- the bias--variance
trade--off.

Consider a normal linear model on $\vy$ with conjugate normal prior on $\vbeta$ with mean centred at $\vzero$,
and covariance $g \sigma^2 (\mX^\top \mX)^{-1}$ where the prior on $g$ is Zellner's g-prior on the covariance
matrices (Zellner 1986), as this yields a tractable posterior for $\vbeta$ as shown by \cite{Liang2008}. We choose $a$  and $b$ to be $-3/4$ and $(n - p)/2 - a$ respectively, as in \cite{Maruyama2011}.

\section{Methodology}
\label{sec:methodology}

% Definitions
Let $n > 0$ be the number of observations and $p > 0$ be the number of covariates. Let $\vy \in \R^n$ be the
vector of responses, $\vtheta \in \R^p$ be the vector of parameters and $\mX \in \R^{n \times p}$ be the
matrix of covariates. Let $p(\vtheta)$ be the prior distribution of $\vtheta$, $p(\vy, \vtheta)$ be the full
probability distribution of $\vy$, $p(\vtheta | \vy)$ the posterior distribution and $q(\vtheta)$ be the
approximating probability distribution.

We index the space of models by a $p$-dimensional vector of indicator variables for each variable considered 
for inclusion, $\vgamma$. For each model $\mathcal{M}_\vgamma$, the response vector $\vy$ is modelled by
\begin{equation*}
\mathcal{M}_\vgamma: \vmu = \vone_n \alpha + \mX_\vgamma \vbeta_\vgamma.
\end{equation*}

Let $\KL(q||p) = \int q(\vtheta) \log{\left( \frac{q(\vtheta)}{p(\vtheta|\vy)} \right)} d \vtheta$ be the
Kuhlback-Leibner divergence between $q$ and $p$, a measure of the distance between the probability
distributions $p$ and $q$.

\subsection{Variational Bayes}

The desired posterior distribution $p(\vtheta | \vy)$ typically requires the calculation of an analytically
intractable integral for all but the simplest models with conjugate priors. Variational Bayes approximates
the full posterior with a simplified approximating distribution $q(\vtheta)$. We relate the true and 
approximating distributions as follows:

\begin{align*}
\log p(\vy) &= \log p(\vy) \int q(\vtheta) d \vtheta = \int q(\vtheta) \log p(\vy) d \vtheta \\
&= \int q(\vtheta) \log \left\{ \frac{p(\vy, \vtheta) / q(\vtheta)}{p(\vy|\vtheta) / q(\vtheta)} \right\} d \vtheta \\
&= \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\} d \vtheta +
		\int q(\vtheta) \log\left\{ \frac{q(\vtheta)}{p(\vtheta|\vy)} \right\} d \vtheta \\
&= \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\} d \vtheta +
		\KL(q||p) \\
&\geq \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\} d \vtheta.
\end{align*}

as $\KL(q||p) \geq 0$ for all probability densities $p$ and $q$. The last quantity is the variational lower
bound $\underline{p}(\vtheta) \equiv \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\}
d\vtheta$. By the inequality above, this is guaranteed to bound the true probability distribution from
below.

The approximation is fit by iteratively maximising the variational lower bound using a sequence of mean field
updates, with each update guaranteed to increase the variational lower bound relative to the previous
iteration. This sequence of mean field updates reduces the KL divergence between the true probability
distribution $p(\vy)$ and the $q(\vtheta)$. The process converges when the variational lower bound no longer
increases, and the KL divergence between the posterior distribution and the approximating distribution is
minimised.

A popular form of approximation is to restrict $q(\vtheta)$ to a subclass of product densities by partitioning
$\vtheta = (\vtheta_1, \vtheta_2, \ldots, \vtheta_{M-1}, \vtheta_M)$ and assuming independence between the
partitioned parameters:

\begin{equation*}
q(\vtheta) \equiv q(\vtheta_1) q(\vtheta_2) \ldots q(\vtheta_{n-1}) q(\vtheta_n).
\end{equation*}

This allows the calculation of the optimal approximating densities $q_i^*(\vtheta_i)$ as

\begin{equation*}
q_i^*(\vtheta_i) \propto \exp \left \{ \E_{-\vtheta_i} \log p(\vy, \vtheta) \right \}, 1 \leq i \leq M,
\end{equation*}

\subsection{Model}

We consider a linear model with a g-prior, as in \cite{Liang2008}. Consider the linear model

\begin{align*}
\vy | \vbeta, \sigma^2 \sim \N_n(\mX \vbeta, \sigma^2 \mI)
\end{align*}

with priors

\begin{align*}
\vbeta | \sigma^2, g &\sim \N_p(\vzero, g \sigma^2 (\mX^T \mX)^{-1}) \\
p(\sigma^2) &= (\sigma^2)^{-1} \I(\sigma^2 > 0) \\
p(g) &= \frac{g^b (1 + g)^{-(a + b + 2)}}{\Beta(a + 1, b + 1)} \I(g > 0)
\end{align*}

We choose a factored Variational Bayes approximation of the form

\begin{align*}
q(\vtheta) = q(\vbeta) q(\sigma^2) q(g).
\end{align*}

Then $q(\vbeta) = \N(\vmu_{q(\vbeta)}, \mSigma_{q(\vbeta)})$, $q(\sigma^2) = \IG(\alpha_{q(\vbeta)}, \beta_{q(\vbeta)})$ and $q(g) = \text{Beta Prime}(\alpha_{q(g)}, \beta_{q(g)})$.

\subsection{Mean field updates}

\begin{algorithm}
\label{alg:algorithm_one}
\caption{Fit VB approximation of linear model}
\begin{algorithmic}
\REQUIRE $\nu_{q(g)} - 1 \leftarrow b - \frac{p}{2}$, $\mu_{q(g)} - 1 \leftarrow (a + b + 2)$ \\
\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
\STATE Calculate $\tau_{g}$ using numerical integration in Section \ref{sec:num_int}
\ENDWHILE
\STATE $\tau_{\sigma^2} \leftarrow \frac{n}{[1 - (1 + \tau_g)^{-1}] n R^2}$
\STATE $\beta_{q(g)} \leftarrow \left(\frac{n (1 + \tau_g)^{-1}}{[1 - (1 + \tau_g)^{-1}]} + p \right)$
\STATE $\vmu_{q(\vbeta)} \leftarrow (1 + \tau_g)^{-1} (\mX^\top \mX)^{-1} \mX^\top \vy$
\STATE $\mSigma_{q(\vbeta)} \leftarrow \tau_{\sigma^2}^{-1} (1 + \tau_{g})^{-1}(\mX^\top \mX)^{-1}$
\end{algorithmic}
\end{algorithm}

\subsubsection{Numerical integration of $\tau_g$}\label{sec:num_int}

Define $\tau_{\sigma^2} \equiv \E_q \left[ \frac{1}{\sigma^2} \right]$ and
$\tau_g \equiv \E_q \left[ \frac{1}{g} \right]$. Then we calculate $\tau_g$ numerically using an iterative 
scheme. First, we choose an initial guess $\tau_g = 1$. Then we use this initial guess to generate a new,
improved guess $\tau_g'$ by performing the numerical integration

\begin{align*}
\tau_g' \leftarrow \int_0^\infty \frac{1}{g} \exp \left \{ \left(b - \frac{p}{2}\right) \log g
- (a + b + 2) \log(1 + g)
- \frac{1}{g} \frac{1}{2} (1 + \tau_g)^{-1} [\tau_{\sigma^2} (1 + \tau_g)^{-1} n R^2 + p]
	  \right \} dg
\end{align*}

where $\tau_{\sigma^2} = \frac{1}{1 - (1 + \tau_g)^{-1} R^2}$, $\nu - 1 = b - \frac{p}{2}$, 
$\beta = \frac{1}{2} (1 + \tau_g)^{-1} (\tau_{\sigma^2} n R^2 + p)$, 
$\mu - 1 = (a + b + 2)$ and $\gamma = 1$. 


This process continues until convergence.

\subsection{Structured Variational Bayes}

Let $\vgamma \in \{0, 1\}^p$ be the vector of indicators of whether each of the $p$ covariates in our
covariate matrix $\mX$ is included in the model $\vgamma$. We perform model selection by using the above
Variational Bayes approximation to each  candidate linear model and averaging over the space of available
models.

\begin{align*}
\underline{p}(\vy) &= \sum_\vgamma \underline{p}(\vy|\vgamma) p(\vgamma) \\
p(\vgamma|\vy) &\approx \frac{\underline{p}(\vy|\vgamma) p(\vgamma)}{\underline{p}(\vy)} \\
p(\vbeta|\vy) &\approx \sum_\vgamma q(\vbeta|\vgamma) p(\vgamma|\vy) \\
\end{align*}

\subsection{Numerical issues}
Numerical difficulties, and how we circumvented them.

\section{Numerical experiments}
\label{sec:num_exp}
Hitters
Major League Baseball Data from the 1986 and 1987 seasons.
An Introduction to Statistical Learning with Applications in R

\begin{tabular}{llllllllllllllllllll}
$\vp$ & 0.13738&0.13011&0.25706&0.98161&0.92139&0.17273&0.63344&0.56192&0.62341&0.47961&0.44144&0.49936&0.19706&0.92644&0.13061&0.17386&0.12773&0.90111&0.85136\\
$\vq$ & 0.13686&0.12957&0.25713&0.98195&0.92221&0.17227&0.63546&0.56202&0.62490&0.47968&0.44020&0.49869&0.19666&0.92746&0.12996&0.17320&0.12713&0.90207&0.85272\\
\end{tabular}

Body Fat
Source: ????

\begin{tabular}{llllllllllllll}
$\vp$ & 0.93781&0.13643&0.18198&0.07181&0.07058&0.10752&0.14706&1.00000&0.13400&0.14061&0.32318&0.61941&0.22117\\
0.93857&0.13627&0.18209&0.07137&0.07020&0.10712&0.14649&1.00000&0.13364&0.14000&0.32292&0.62035&0.22085\\
\end{tabular}

Wage
An Introduction to Statistical Learning with Applications in R

\begin{tabular}{llllllllllllllllll}
$\vp$ & 1.00000&1.00000&0.01013&0.02354&1.00000&0.05359&0.08294&0.01856&0.01141&0.01031&0.01250&0.01353&0.01108&0.01429&0.05739&0.04201&0.03314\\
$\vq$ & 1.00000&1.00000&0.01013&0.02354&1.00000&0.05360&0.08297&0.01856&0.01141&0.01031&0.01250&0.01352&0.01108&0.01429&0.05741&0.04202&0.03315\\
\end{tabular}

Graduation Rate
An Introduction to Statistical Learning with Applications in R

\begin{tabular}{llllllllllllllllll}
$\vp$ & 0.91331&1.00000&0.09008&0.10810&0.10993&0.60189&0.12723&0.94518&0.99943&0.99905&0.20085&0.86391&0.26170&0.10466&0.14643&0.97650&0.43656\\
$\vq$ & 0.91365&1.00000&0.08993&0.10798&0.10982&0.60231&0.12712&0.94543&0.99944&0.99905&0.20079&0.86409&0.26150&0.10453&0.14626&0.97662&0.43683\\
\end{tabular}

US Crime

\begin{tabular}{llllllllllllllll}
$\vp$ & 0.22553&0.84921&0.99695&0.21573&0.50191&0.24350&0.35809&0.56912&0.32434&0.20179&0.42365&0.69649&0.86911&0.22875&0.65511\\
$\vq$ & 0.21951&0.85618&0.99742&0.21040&0.50701&0.23992&0.35835&0.57261&0.31827&0.19627&0.41761&0.69944&0.87592&0.22417&0.66056\\
\end{tabular}

\section{Conclusion and Discussion}
\label{sec:conclusion}

\bibliographystyle{elsarticle-harv}
\bibliography{references_mendeley}

\end{document}