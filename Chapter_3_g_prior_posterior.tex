% %\maketitle

\chapter{Numerical aspects of calculating Bayes factors for linear models using
	mixture $g$-priors}

% \begin{abstract}
We consider the numerical evaluation of Bayes factors for linear models using different mixture 
$g$-priors. In particular we consider several hyperpriors for $g$ including  the hyper-$g$ and 
hyper-$g/n$ priors of \cite{Liang2008}, the beta-prime prior of \cite{Maruyama2011}, the robust 
prior of \cite{Bayarri2012}, and the cake prior of \cite{OrmerodEtal2017}. In particular we 
describe how the Bayes factor under a hyper-$g$ and robust priors can be evaluated in efficient, 
accurate and numerically stable manner. We derive a closed form expression for the Bayes factor 
under the hyper-$g/n$. We implement an {\tt R} package for Bayesian model averaging and selection,
and discuss some associated computational issues. We illustrate the advantages of our implementation 
over several existing packages on several small datasets.
% \end{abstract}


\section{Introduction}

There has been a large amount of research in recent years into the appropriate choice of suitable 
and meaningful priors for linear regression models in the context of Bayesian model selection and 
averaging. Specification of the prior structure of these models must be made with great care in 
order for Bayesian model selection and averaging procedures to have good theoretical properties. 
A key problem in this context occurs when the models have differing dimensions and non-common 
parameters where inferences are typically highly sensitive to the choice of priors for the 
non-common parameters due to the Jeffreys-Lindley-Bartlett paradox \citep{Lindley1957,Bartlett1957,OrmerodEtal2017}.
Furthermore, this sensitivity does not necessarily vanish as the sample size 
grows \cite{Kass1995,Berger2001}.  

Bayes factors in the context of linear model selection 
\citep{Zellner1980,
	Zellner1980b,
	Mitchell1988,
	George1993,
	Fernandez2001,
	Liang2008,
	Maruyama2011,
	Bayarri2012}
have received an 
enormous amount of attention. A landmark paper in this field is \cite{Liang2008}.
\cite{Liang2008} considers particular priors structure for the model parameters. 
It particular they consider a Zellner's $g$-prior \cite{Zellner1980,Zellner1986} 
the regression coefficients where $g$ is a prior hyperparameter. The parameter $g$
requires special consideration. If $g$ is set to a large constant post of the posterior
mass is placed on the null model, a phenomenon they refer to as Bartlett's paradox.
Due to this problem they consider setting $g=n$ \citep{Kass1995b},  $g=p^2$ \cite{Foster1994},
and $g=\max(n,p^2)$ \citep{Fernandez2001}. \cite{Liang2008} showed that these choices
lead to what they call the information paradox where the posterior probability of the
true model does not tend to 1 as the sample size grows. \cite{Liang2008} also considers
a local and global empirical Bayes (EB) procedure for selecting $g$. In these cases \cite{Liang2008}
show that these EB procedures are model selection consistent except when the true model is the null
model (the model containing the intercept only). The above problems suggest that a prior should
be placed on $g$.

In this paper we review the prior structures that lead to closed form expressions for Bayes factors for linear models.
These include the hyper-$g$ prior of \cite{Liang2008}, the beta-prime prior of \cite{Maruyama2011}, the robust 
prior of \cite{Bayarri2012}, and most recently the cake prior of \cite{OrmerodEtal2017}. We concern ourselves with the efficient, accurate and numerically stable evaluation of Bayes factors, Bayesian model averaging ,
and Bayesian model selection  for linear models 
under the above choices of prior structures for the model parameters.


Our main contributions are as follows.
\begin{enumerate}
	\item To this list add the  hyper-$g/n$ prior of \cite{Liang2008} for which we
	derive a new
	a closed form expression for the Bayes factor in terms
	of the Appell hypergeometric function.
	
	\item We give an new alternative expression for the Bayes factor when using the
	robust prior of \cite{Bayarri2012}.
	
	\item We describe how the  Bayes factors corresponding to the hyper-$g$ prior of \cite{Liang2008} and robust 
	prior of \cite{Bayarri2012} can be calculated in an efficient, accurate and numerically stable manner.
	
	\item We make available a highly efficient and {\it numerically stable} package 
	
	\centering{
		{\tt BayesianLinearModelAveraging}
	} 

	for exact Bayesian model averaging
	using the above prior structures available from
	
	\begin{center}
	\url{GITHUB???}
	\end{center}
\end{enumerate}

\noindent 
We demonstrate the advantages of our implementation of exact Bayesian model
averaging over some existing {\tt R}
packages using several small datasets.



The article is organised as follows. Section \ref{sec:bma} describes Bayesian model averaging and model
selection. Section \ref{sec:model} outlines and justifies our chosen model and prior structure. Section 
\ref{sec:MarginalLikelihood} derives closed form expressions for various marginal likelihoods using 
different hyper priors for $g$ and describes how these may be evaluated in a numerically appropriate way.
In Section \ref{sec:implementation}, we discuss details of our implementation which made our approach 
computationally feasible.
In Section \ref{sec:numerical} we perform a series of numerical experiments to show the accuracy of our approach. 
Finally, in Section \ref{sec:conclusion} we provide a conclusion.









 



 
 

\section{Bayesian model selection and averaging}
\label{sec:bma}



Suppose $\vy = (y_1,\ldots,y_n)^T$ is a response vector of length $n$, $\mX$ is an $n$ by $p$ matrix 
of covariates where we anticipate a linear relationship between $\vy$ and $\mX$, but do not know
which of the columns of $\mX$ are important to the prediction of $\vy$.
Bayesian model averaging seeks to improve prediction by averaging over multiple
predictions over different choices of combinations of predictors.

We consider the linear model for predicting $\vy$ with design matrix $\mX$ via
\begin{equation}
\label{eq:linearModel}
\vy | \alpha, \vbeta, \sigma^2 \sim N(\vone_n\alpha + \mX \vbeta, \sigma^2 \mI_n),
\end{equation} 


\noindent where $\alpha$ is the model intercept, $\vbeta$ is a coefficient vector of length $p$, 
$\sigma^2$ is the residual variance, and $\mI_n$ is the $n \times n$ identity matrix. 
Without loss of generality, to simplify later calculations, we will standardize $\vy$ and $\mX$ 
so that $\overline{y} = 0$, 
$\|\vy\|^2 = \vy^T\vy = n$, $\mX_j^T\vone = 0$,  and $\|\mX_j\|^2 = n$ where $\mX_j$ is the $j$th 
column of $\mX$. 


Suppose that we wish to perform Bayesian model selection, model averaging or hypothesis 
testing where we are interested in comparing how different subsets of predictors 
(which correspond to different columns of the matrix $\mX$) have on the response $\vy$. To this end, 
let $\vgamma \in \{0, 1\}^p$ be a binary vector of indicators for the inclusion of the $p$th column 
of $\mX$ in the model where $\mX_\vgamma$ denotes the design matrix formed by including only the 
$j$th column of $\mX$ when $\gamma_j = 1$. 
In order to keep our exposition as general as possible we will assume a prior structure of
the from 
$p(\alpha,\vbeta_{\vgamma}|\vgamma)p(\vgamma)$ but is otherwise unspecified for
the time being. 

We adopt the a prior on $\vbeta_{-\vgamma}$  
of the form
\begin{equation}
\label{eq:spikeAndSlab}
\ds p(\vbeta_{-\vgamma}|\vgamma) = \prod_{j=1}^p \delta(\beta_j;0)^{1-\gamma_j},
\end{equation} 

\noindent where $\delta(x;a)$ is the Dirac delta function with location $a$.  
The prior on $\vbeta_{-\vgamma}$ in (\ref{eq:spikeAndSlab}) is the spike in
in what is referred to as a spike and slab prior where the prior on $\vbeta_{\vgamma}$ is assumed to be flat (the slab). There are several
several variants of the spike and slab prior initially used in
\cite{Mitchell1988}
and later refined in
\cite{George1993}. 
The above structure implies 
that $p(\vbeta_{-\vgamma}|\vy)$ is a point mass at $\vzero$
and leads to
algebraic and computational simplifications for components of $\vbeta$ when elements of $\vgamma$ equal 0.
Thus, $\gamma_j=0$ is equivalent to excluding the corresponding predictor $\mX_j$ from the model.


Exact Bayesian model averaging revolves around the posterior
 probability of a model $\vgamma$ using Bayes theorem
$$
\ds p(\vgamma|\vy) = \frac{p(\vy|\vgamma)p(\vgamma)}{\sum_{\vgamma'} p(\vy|\vgamma')p(\vgamma')} = \frac{p(\vgamma)\mbox{BF}(\vgamma)}{\sum_{\vgamma'} p(\vgamma')\mbox{BF}(\vgamma')}
\qquad \mbox{where} \qquad 
p(\vy|\vgamma) = \int p(\vy,\vtheta|\vgamma) \, d\vtheta,
$$

\noindent letting $\vtheta = (\alpha,\vbeta,\sigma^2)$, using $\sum_{\vgamma}$ to denote a combinatorial sum over all
$2^p$ possible values of $\vgamma$, and $\mbox{BF}(\vgamma) = p(\vy|\vgamma)/p(\vy|\vzero)$
is the null based Bayes factor for model $\vgamma$.
Note that the Bayes factor is a statistic commonly used in Bayesian hypothesis testing 
\cite{Kass1995,OrmerodEtal2017}.
Prediction is based on the 
the posterior distributions of $\alpha$ and $\vbeta$ where
$p(\vbeta|\vy) = \sum_{\vgamma} p(\vbeta|\vy,\vgamma) \cdot p(\vgamma|\vy)$
(with similar expressions for $\alpha$ and $\sigma^2$).

The 
posterior expectation of $\vgamma$ is given by
$\bE(\vgamma|\vy) = \sum_{\vgamma} \vgamma \cdot p(\vgamma|\vy)$.
If one is required to select a single model, say $\vgamma^*$, two common choices are the 
highest posterior model (HPM) which uses $\vgamma^* = \vgamma_{\mbox{\tiny HPM}} = \max_\vgamma \{ \, p(\vy,\vgamma) \, \}$, or the
median posterior model (MPM) where $\vgamma^*$ is obtained by
rounding $\bE(\vgamma|\vy)$ to the nearest integer.
The MPM has predictive optimality properties \cite{barbieri2004}.
If the MPM is used for model selection
the quantity $\bE(\vgamma|\vy)$ is sometimes referred to as the
posterior (variable) inclusion probability (PIP).

Ignoring for the moment the problems associated with specifying
$p(\alpha,\vbeta_{-\vgamma},\vgamma)$, all of the above quantities are conceptually
straightforward. In practice the computation of the quantities $p(\vgamma|\vy)$, $p(\vbeta|\vy)$ and 
$\bE(\vgamma|\vy)$ are only feasible for small values of $p$ (say around $p=30$). For large values of $p$ we need to pursue
alternatives to exact inference.

 
 
\section{Prior specification for linear model parameters}
\label{sec:model}

We now consider more carefully the specification of $p(\alpha,\vbeta,\sigma^2|\vgamma)$.
%For $p(\alpha,\vbeta,\sigma^2|\vgamma)$ we will adopt the following prior structure
Consider the prior structure
\begin{equation}
\label{eq:priorStructure}
\begin{array}{c}
\ds p(\alpha) \propto 1,  
\qquad 
\vbeta_\vgamma | \sigma^2, g, \vgamma \sim \N_p(\vzero, g \sigma^2 (\mX_\vgamma^T \mX_\vgamma)^{-1})
\quad \text{ and }  \quad 
\ds p(\sigma^2) \propto (\sigma^2)^{-1} I(\sigma^2 > 0),                      
\end{array}
\end{equation} 

\noindent where we have introduced a new prior hyperparameter $g$.
For the time being we will defer specification of $p(g)$ and $p(\vgamma)$.
We will now justify each element of the above prior structure.

The priors on $\alpha$ and $\sigma^2$ are improper Jeffreys priors and have been justified 
in \cite{Berger2012}. In the context Bayesian model selection, model averaging or hypothesis 
testing $\alpha$ and $\sigma^2$ appear in all models 
so that when comparing models the proportionality constants in the corresponding
Bayes factors cancel.

The prior on $\vbeta_\vgamma$ is Zellner's $g$-prior \citep[see for example,][]{Zellner1986} with prior 
hyperparameter $g$. This family of priors for Gaussian regression model where the prior covariance 
matrix of $\vbeta_\vgamma$ is taken to be a multiple of $g$ with the Fisher information matrix for $\vbeta$. 
This places the most prior mass for $\vbeta_\vgamma$ on the section of the parameter space where the data is 
least informative, and makes the marginal likelihood of the model scale-invariant. Furthermore, this 
choice of prior removes a log-determinant of $\mX_\vgamma^T\mX_\vgamma$ term from the expression for the marginal 
likelihood, which is an additional computational burden to calculate.
The prior on $\vbeta_\vgamma$ combined with the prior on $\vbeta_{-\vgamma}$
in (\ref{eq:priorStructure}) is constitutes  a spike and slab prior for $\vbeta$.
 
An alternative choice of prior on $\vbeta_\vgamma$ was proposed by \cite{Maruyama2011}. Let
$p_{\vgamma} = |\vgamma|$. We will now describe their prior on $\vbeta_\vgamma$ for the case where for the case
$p_{\vgamma} < n - 1$. Let $\mU\mLambda\mU^T$ be an eigenvalue decomposition of $\mX_\vgamma^T\mX_\vgamma$
where $\mU$ is an orthonormal $p_{\vgamma} \times p_{\vgamma}$ matrix, $\mLambda = \mbox{diag}(\lambda_1,\ldots,\lambda_{p_{\vgamma}})$ 
is a diagonal matrix of eigenvalues with $\lambda_1\ge\ldots,\ge \lambda_{p_{\vgamma}}>0$. Then \cite{Maruyama2011} 
propose a prior for $\vbeta_\vgamma$ of the form
\begin{equation}
\label{eq:priorBetaMG}
\vbeta_\vgamma | \sigma^2, g \sim N(\vzero, \sigma^2 (\mU\mW\mU^T)^{-1}),   
\end{equation} 

\noindent where $\mW = \mbox{diag}(w_1,\ldots,w_{p_{\vgamma}})$ with $ w_j = \lambda_j/[\nu_j(1 + g) - 1]$ for 
some prior hyperparameters $\nu_1 \ge \ldots \ge \nu_q$. \cite{Maruyama2011} suggest as a default 
choice for the $\nu_j$'s to use $\nu_j = \lambda_j/\lambda_{p_{\vgamma}}$, for $1\le j \le p_{\vgamma}$. 
This choice down-weights the prior on the rotated parameter space of $(\mU \vbeta)_j$ when the 
corresponding eigenvalue $\lambda_j$ is large and consequently prior standard errors are 
approximately the same size. Note that when $\nu_1 = \ldots = \nu_{p_{\vgamma}} = 1$ the prior 
(\ref{eq:priorBetaMG}) reduces to the prior for $\vbeta$ in (\ref{eq:priorStructure}). 

The choice between (\ref{eq:priorBetaMG}) and the prior for $\vbeta$ in (\ref{eq:priorStructure}) 
represents a trade-off over computational efficiency and desirable statistical properties. We choose
(\ref{eq:priorStructure}) because it avoids the computational burden of calculating an eigenvalue or a singular 
value decomposition of a $p_{\vgamma}\times p_{\vgamma}$ matrix for every model considered,
which typically costs $O(p_{\vgamma}^3)$. 
It also means that we can 
exploit efficient matrix updates to traverse the entire model space in a computationally efficient 
manner allowing this to be done feasibly when $p$ is less than around 30 on a standard 2017 laptop 
(see Section \ref{sec:implementation} for details).


The marginal likelihood for the
model  (\ref{eq:linearModel}) and under prior structure
(\ref{eq:priorStructure}). 
%Integrating out $\alpha$ and $\vbeta$ from 
%$p(\vy,\alpha,\vbeta|\sigma^2,g)$ we find
%\begin{equation}\label{eq:yGivenSigma2andG}
%\begin{array}{rl}
%\ds p(\vy|\sigma^2,g)
%& \ds = \int \exp\left[
%- \tfrac{n}{2}\log(2\pi\sigma^2) 
%- \tfrac{1}{2\sigma^2}\|\vy - \vone\alpha - \mX\vbeta\|^2
%- \tfrac{p}{2}\log(2\pi g\sigma^2) 
%+ \tfrac{1}{2}\log|\mX^T\mX|
%- \tfrac{1}{2g\sigma^2}\vbeta^T\mX^T\mX\vbeta  
%\right] d\alpha d\vbeta
%\\
%& \ds = \int \exp\left[
%- \tfrac{n}{2}\log(2\pi\sigma^2) 
%- \tfrac{n}{2\sigma^2}
%- \tfrac{n\alpha^2}{2\sigma^2} 
%+ \sigma^{-2}\vy^T\mX\vbeta
%- \tfrac{1}{2\sigma^2}(1 + g^{-1})\vbeta^T\mX^T\mX\vbeta 
%- \tfrac{p}{2}\log(2\pi g\sigma^2) 
%+ \tfrac{1}{2}\log|\mX^T\mX|
%\right] d\alpha d\vbeta
%\\
%& \ds = \int \exp\left[
%- \tfrac{n-1}{2}\log(2\pi\sigma^2) 
%- \tfrac{1}{2}\log(n)
%- \tfrac{n}{2\sigma^2}
%+ \sigma^{-2}\vy^T\mX\vbeta
%- \tfrac{1}{2\sigma^2}(1 + g^{-1})\vbeta^T\mX^T\mX\vbeta 
%- \tfrac{p}{2}\log(2\pi g\sigma^2) 
%+ \tfrac{1}{2}\log|\mX^T\mX|
%\right]  d\vbeta
%\\
%& 
%\ds = \exp\left[
%- \tfrac{n-1}{2}\log(2\pi\sigma^2) 
%- \tfrac{1}{2}\log(n)
%- \tfrac{p}{2}\log(1 + g)
%- \tfrac{n}{2 \sigma^2} \left( 1 - \tfrac{g}{1 + g} R^2 \right)  
%\right],
%\end{array} 
%\end{equation}
%
%\noindent 
%\joc{
%	Derivation of the above expression uses the identity
%	$
%	\int \exp\left\{ -\tfrac{1}{2}\vx^T\mA\vx + \vb^T\vx \right\} d \vx = |2\pi\mSigma|^{1/2} \exp\left\{ \tfrac{1}{2}\vmu^T\mSigma^{-1}\vmu \right\}
%	$
%	where $\vmu = \mA^{-1}\vb$, and $\mSigma = \mA^{-1}$.
%	It also uses the identities: $|c\mA| = c^d|\mA|$ and $|\mA^{-1}| = |\mA|^{-1}$ when $\mA \in\R^{d\times d}$.
%}
Integrating out $\alpha$, $\vbeta$, and $\sigma^2$ from $p(\vy,\alpha,\vbeta,\sigma^2|g,\vgamma)$ we
obtain
\begin{equation}\label{eq:yGivenG}
\begin{array}{rl}
\ds p(\vy|g,\vgamma)
%& \ds = \int \exp\left[
%- \tfrac{n-1}{2}\log(2\pi) 
%- \tfrac{1}{2}\log(n)
%- \tfrac{p}{2}\log(1 + g)
%- \left( \tfrac{n-1}{2} + 1\right)\log(\sigma^2) 
%- \left( \tfrac{n}{2} \tfrac{1 + g(1-R^2)}{1 + g} \right)\sigma^{-2} 
%\right]  d\sigma^2
%\\
%& 
\ds = K(n)
(1 + g)^{(n - p_\vgamma - 1)/2}(1 + g (1 - R_\vgamma^2))^{-(n-1)/2},
\end{array} 
\end{equation}

\noindent where $K(n) = [\Gamma( (n-1)/2 )]/[\sqrt{n}(n\pi)^{(n-1)/2}]$, and
$R_\vgamma^2 = \vy^T\mX_\vgamma^T(\mX_\vgamma^T\mX_\vgamma)^{-1}\mX_\vgamma^T\vy/n$ is 
the the usual R-squared statistic.
This is the same expression as \cite{Liang2008} Equation (5) 
after simplification. Note that
when $\vgamma = \vzero$, i.e., the null model, then $p_\vgamma = 0$, and
$R_\vgamma^2 = 0$ leading to the simplification $p(\vy|g,\vzero) = K(n)$
for all $g$. Hence, $p(\vy|\vzero) = K(n)$ provided the prior on $g$ is proper.

We will now discuss the specification of $g$.
The results of \cite{Liang2008} suggest that the only way to 
avoid the problems described in the introduction is to place a prior on $g$.

\section{Hyperpriors on $g$}

Here we outline some of the choices of hyperpriors for $g$ used in the literature, their
properties, and where possible how to implement these in a efficiently and 
accurately in
numerically stable manner. We cover the 
the hyper-$g$ and hyper-$g/n$ priors of \cite{Liang2008}, the beta-prime prior
of \cite{Maruyama2011}, the robust prior of \cite{Bayarri2012}, and the cake
prior of \cite{OrmerodEtal2017}.
We also considered prior structure implied by \cite{Zellner1980}, but were able to make no
meaningful progress for this case.


\subsection{The hyper-$g$ prior}

Initially, \cite{Liang2008} suggest the hyper $g$-prior where
\begin{equation}\label{eq:hyperG}
\ds p_{g}(g) = \frac{a - 2}{2}(1 + g)^{-a/2}I(g>0),
\end{equation}

\noindent for $a>2$. Combining (\ref{eq:yGivenG}) with (\ref{eq:hyperG}), we have
\begin{equation}\label{eq:hyperGmarginalIntegral}
p_{g}(\vy|\vgamma) = K(n) \frac{a - 2}{2}  \int_0^\infty 
\left( 1 + g \right)^{-a/2}
(1 + g)^{(n-p_\vgamma-1)/2} \left[ 1 + g (1 - R_\vgamma^2) \right]^{-(n-1)/2}  dg.
\end{equation}

\noindent After applying 
3.197(5) of \cite{Gradshteyn2007} (see Appendix A) leads to
\begin{equation}\label{eq:hyperGmarginal}
\ds \mbox{BF}_{g}(\vgamma) = \frac{p_{g}(\vy|\vgamma)}{p_{g}(\vy|\vzero)} =  \left( \frac{a - 2}{p_\vgamma + a - 2} \right) \cdot {}_2F_1\left( \frac{n-1}{2}, 1; \frac{p_\vgamma + a}{2}; R_\vgamma^2 \right),
\end{equation}

\noindent where ${}_2F_1(\,\cdot\,,\,\cdot\,;\,\cdot\,;\,\cdot\,)$ is the Gaussian hypergeometric function.

The Gaussian hypergeometric function is notoriously prone 
to overflow and numerical instability \citep{Pearson2017}. When such numerical issues arise 
\cite{Liang2008} uses Laplace's method to approximate $p(\vy|\vgamma)$. 
However, recently \cite{Nadarajah2015}
rendered such approximation unnecessary.
\cite{Nadarajah2015} showed that the numerical issues associated with (\ref{eq:hyperGmarginal}) can be avoided 
via identity 
\begin{equation}\label{eq:logGuassHypergeometric}
\ds {}_2F_1(b,1;c;x) = (c-1) x^{1-c} (1-x)^{c-b-1} B_x (c-1, b-c+1),
\end{equation}

\noindent derived in \cite{PrudnikovEtal1986} (vol. 3, sec. 7.3), where 
$B_x(\,\cdot\,,\,\cdot\,)$
is the incomplete beta function. Note that the case where $x=0$ is not handled by 
(\ref{eq:logGuassHypergeometric}) and the value $\ds {}_2F_1(b,1;c;0) = 1$ should be used instead
and also implicitly assumes that $c>1$. Numerical overflow can be avoided
since standard libraries exist for evaluating $B_x(\,\cdot\,,\,\cdot\,)$
on the log scale.
Unfortunately, \cite{Liang2008} also showed that
(\ref{eq:hyperGmarginal}) is not model selection consistent when the
true model is the null model (the model only containing the intercept) and so alternative priors should be used.




\subsection{The hyper-$g/n$ prior}

Given the problems with the hyper-$g$ prior, \cite{Liang2008} 
proposed a modified variation of the hyper-$g$ prior with
\begin{equation}\label{eq:hyperGonN}
\ds p_{g/n}(g) = \frac{a - 2}{2n}\left( 1 + \frac{g}{n} \right)^{-a/2}I(g>0),
\end{equation}

\noindent which they call the hyper-$g/n$ prior where again $a>2$.
They show that this prior leads to model selection consistency.
Combining (\ref{eq:yGivenG}) with (\ref{eq:hyperGonN}) the quantity $p(\vy|\vgamma)$ 
can be expressed as the integral
\begin{equation}\label{eq:hyperGonNmarginalIntegral}
\begin{array}{rl}
p_{g/n}(\vy|\vgamma) 
& \ds 
= K(n) \frac{a - 2}{2n}  \int_0^\infty 
\left( 1 + \frac{g}{n} \right)^{-a/2}
(1 + g)^{(n-p_\vgamma-1)/2} \left[ 1 + g (1 - R_\vgamma^2) \right]^{-(n-1)/2}  dg
\\ [2ex]
& \ds = K(n) \frac{a - 2}{2n}  \int_0^1 
(1 - u)^{p/2 + a/2 - 2  } \left[ 1 - u \left(1  -  \tfrac{1}{n} \right) \right]^{-a/2} \left(  1 - u R^2\right)^{-(n-1)/2} du,
\end{array} 
\end{equation}

\noindent where $u=g/(1+g)$. Employing 
Equation 3.211 of \cite{Gradshteyn2007} 
$$
F_1(a,b_1,b_2,c; x,y) = \frac{\Gamma(c)} {\Gamma(a)\Gamma(c-a)} 
\int_0^1 t^{a-1} (1-t)^{c-a-1} (1-xt)^{-b_1} (1-yt)^{-b_2} \, dt
$$

\noindent 
leads to
\begin{equation}\label{eq:hyperGonNmarginal}
\ds \mbox{BF}_{g/n}(\vgamma) =  \frac{a - 2}{n(p_\vgamma + a - 2)} F_1\left( 1, \frac{a}{2}, \frac{n-1}{2}; \frac{p_\vgamma + a}{2}; 1  -  \frac{1}{n}, R_\vgamma^2 \right),
\end{equation}

\noindent where
$F_1$ being the Appell hypergeometric function in two variables 
\cite{Weisstein2009}. 






Unfortunately, the expression (\ref{eq:hyperGonNmarginal}) is extremely
difficult to evaluate numerically since the second last argument of the above 
$F_1$ is asymptotically close to the radius of convergence of the $F_1$
function.
\cite{Liang2008} again suggest Laplace approximation 
for this choice of prior. 

 
\subsection{Beta-prime prior} 
 
Next we will consider the prior 
\begin{equation}\label{eq:betaPrime}
\ds p_{bp}(g) = \frac{g^{b}(1 + g)^{-(a+b+2)}}{\mbox{Beta}(a+1,b+1)} I(g>0),
\end{equation}

\noindent proposed by \cite{Maruyama2011} where $a>-1$ and $b>-1$. 
This is a Pearson Type VI or beta-prime distribution. More specifically, 
$g\sim \mbox{Beta-prime}(b+1,a+1)$ using the usual parametrization of 
the beta-prime distribution \citep{Johnson1995}.
Then combining (\ref{eq:yGivenG}) with (\ref{eq:betaPrime}) the quantity $p(\vy|\vgamma)$ 
can be expressed as the integral
$$
%\begin{array}{rl}
\ds p_{bp}(\vy|\vgamma) 
%& \ds = \int_0^\infty                                         
%\frac{g^{b}(1 + g)^{-a-b-2}}{\mbox{Beta}(a+1,b+1)}
%K(n) (1 + g)^{(n - p - 1)/2}\left[ 1 + g(1-R^2) \right]^{-(n-1)/2}
%dg
%\\
%& \ds 
=
\frac{K(n)}{\mbox{Beta}(a+1,b+1)}
\int_0^\infty             
g^{b}(1 + g)^{(n - p_\vgamma - 1)/2 - (a + b + 2)}  (1 + g (1-R_\vgamma^2) )^{-(n-1)/2}  
dg.
%\end{array}
$$

\noindent If we choose 
%$b$ such that
%$a+b+2 = (n - p_\vgamma - 1)/2$, implying
$b = (n - p_\vgamma - 5)/2 - a$, then the exponent of the $(1 + g)$ term in the equation above is zero.
Using Equation 3.194 (iii) of \cite{Gradshteyn2007}
(see Appendix A) we obtain
\begin{equation}\label{eq:marginalLikelihoodBetaPrime}
\begin{array}{rl}
\ds \mbox{BF}_{bp}(\vy|\vgamma) 
%& \ds =
%\frac{K(n)}{\mbox{Beta}(a+1,b+1)}
%\int_0^\infty g^{b} \left[ 1 + g(1-R^2) \right]^{-(n-1)/2}  
%dg
%\\ [2ex]
& \ds 
=   
\frac{\mbox{Beta}(p/2 + a + 1,b + 1)}{\mbox{Beta}(a+1,b+1)} (1-R_\vgamma^2)^{-(b + 1)}
%\\ [2ex]
%& \ds = \widetilde{K}(n,a)
%
%\Gamma(p/2 + a + 1)\Gamma(a + b + 2)
%(\widehat{\sigma}^2)^{-(b + 1)}
\end{array}
\end{equation}

\noindent which is a simplification of the Bayes factor proposed by
\cite{Maruyama2011}.


Note that (\ref{eq:marginalLikelihoodBetaPrime}) proportional
to a special case of the prior structure considered by \cite{Maruyama2011}
who refer to this special case as ZE as a model selection criterion (after Zellner's $g$ prior). This choice of $b$ also ensures that $g = \BigO(n)$ so that $\tr\{\Var(\vbeta | g, \sigma^2)\} = \BigO(1)$, preventing Bartlett's paradox. 
Note that in comparison to previously discussed priors
marginal likelihood only involves gamma functions which
are well behaved from a numerical analysis perspective. 
\cite{Maruyama2011} showed the prior (\ref{eq:betaPrime}) leads to model
selection consistency.
For derivation of the above properties and further discussion see \cite{Maruyama2011}.


\subsection{Robust prior}  

Next we will consider the robust prior for $g$ as proposed by \cite{Bayarri2012}. Using the default
choices of the prior hyperparameters \cite{Bayarri2012} uses the following hyperprior on $g$:
\begin{equation}\label{eq:robustPrior}
p_{{rob}}(g) = \frac{1}{2}\left(\frac{1+n}{1 + p_\vgamma}  \right)^{1/2} (1 + g)^{-3/2}I(g > L)
\end{equation}
 
\noindent where $L = (1 + n)/(1 + p_\vgamma) - 1$. 
Combining (\ref{eq:yGivenG}) with (\ref{eq:robustPrior})
leads to an expression for $p(\vy|\vgamma)$ of the form
\begin{equation}\label{eq:marginalLikelihoodRobust}
\ds p_{rob}(\vy|\vgamma)
\ds = \frac{K(n)}{2}\left(\frac{1+n}{1 + p_\vgamma}  \right)^{1/2} 
\int_L^\infty  (1 + g)^{(n - p_\vgamma)/2 - 2}(  1 + g \widehat{\sigma}_\vgamma^2)^{-(n-1)/2} dg,
\end{equation}

\noindent where $\widehat{\sigma}_\vgamma^2 = 1 - R_\vgamma^2$ is the MLE
for $\sigma^2$ for model (\ref{eq:linearModel}) when $\mX$ is replaced with $\mX_\vgamma$ under the standardization described in Section 2.
Using the substitution 
$x = (1 + L)/(g - L)$ followed by Equation 3.197(5) of \cite{Gradshteyn2007}
(see Appendix A) leads to
\begin{equation}\label{eq:yGivenGammaRobust}
\ds \mbox{BF}_{{rob}}(\vgamma)
 \ds = \left( \frac{n + 1}{ p_\vgamma + 1} \right)^{ - p_\vgamma/2} \frac{(\widehat{\sigma}_\vgamma^2)^{-(n-1)/2}}{p_\vgamma+1}
{}_2F_1\left( \frac{n-1}{2}, \frac{p_\vgamma+1}{2}; \frac{p_\vgamma+3}{2}  ; 
\frac{(1  - 1/\widehat{\sigma}_\vgamma^2)(p_\vgamma + 1)}{1 + n}  \right),
\end{equation}

\noindent 
which is the same expression as Equation 26. of \cite{Bayarri2012}
 modulo notation.

The expression (\ref{eq:yGivenGammaRobust}) is difficult to deal with numerically for two reasons. Firstly, either of the first 
two arguments of the ${}_2F_1$ function are large relative to the third will often lead to numerical overflow problems. Secondly,
and more problematically, when $\widehat{\sigma}_\vgamma^2$ becomes small the last argument
of ${}_2F_1$ function can become less than $-1$ which falls outside the radius of convergence
of the ${}_2F_1$ function. The {\tt BayesVarSel} package which implements
this choice of prior deals with these problems using numerical
integration.
 
Instead suppose we begin with $x = g - L$ 
%$$
%\ds p_{{rob}}(\vy|\vgamma)
%\ds = \frac{K(n)}{2}\left(\frac{1+n}{1 + p_\vgamma}  \right)^{1/2} %(\widehat{\sigma}_\vgamma^2)^{-(n-1)/2}
%\int_0^\infty  (1 + L + h)^{(n - p_\vgamma)/2 - 2}\left[  \frac{1 + %L\widehat{\sigma}_\vgamma^2}{\widehat{\sigma}_\vgamma^2} + h \right]^{-(n-1)/2} dh.
%$$
and then  employ of Equation 3.197(1) of \cite{Gradshteyn2007} leading to
\begin{equation}\label{eq:yGivenGammaRobust2}
\ds \mbox{BF}_{{rob}}(\vgamma)
\ds = \left( \frac{1 + n}{1 + p_\vgamma} \right)^{(n - p_\vgamma - 1)/2} \frac{\left( 1 + L\widehat{\sigma}_\vgamma^2 \right)^{-(n - 1)/2}}{1 + p_\vgamma}
{}_2F_1\left(  
\frac{n-1}{2}, 1; \frac{p_\vgamma+3}{2}; \frac{1 - \widehat{\sigma}_\vgamma^2}{1 + L\widehat{\sigma}_\vgamma^2}
 \right).
\end{equation}

\noindent This expression is numerically far easier to evaluate efficiently and accurately in a numerically stable manner. Due to simplifications
we have $0\le \widehat{\sigma}_\vgamma^2<1$, we also have $L>0$ so that the last argument
of the ${}_2F_1$ above is bounded in the unit interval. Furthermore, using
the identity (\ref{eq:logGuassHypergeometric}) means that (\ref{eq:yGivenGammaRobust2}) can be evaluated
on the log scale, avoiding numerical overflow.


%$$
%\int_0^\infty x^{\nu - 1}(\beta + x)^{-\mu}(x + \gamma)^{-\varrho} dx
%= \beta^{-\mu}\gamma^{\nu - \varrho}\mbox{Beta}(\nu,\mu-\nu + \varrho)
%{}_2F_1(\mu,\nu;\mu + \varrho; 1 - \gamma/\beta)
%$$

%$$
%\beta = \frac{1 + L\widehat{\sigma}_\vgamma^2}{\widehat{\sigma}_\vgamma^2}
%$$
%$$
%\gamma = 1 + L
%$$
%$$
%\nu = 1
%$$
%$$
%\mu = \frac{n-1}{2}
%$$
%$$
%\varrho = - (n - p_\vgamma - 4)/2
%$$


\subsection{BIC via cake priors}  

\cite{OrmerodEtal2017} develops cake priors which allow for arbitrarily diffuse priors
whilst avoiding Bartlett's paradox leading Bayes factors equal to the exponential of minus half
the BIC. Cake priors
can be thought of as a Jefferys prior in the limit
as the prior becomes increasingly
and enjoy nice theoretical properties including model
selection consistency. \cite{OrmerodEtal2017} 
departs from the prior structure (\ref{eq:priorStructure}) and instead uses
\begin{equation}\label{eq:proirs2}
\ds \alpha|\sigma^2,g \sim N(0,g\sigma^2), \quad 
\ds \vbeta_\vgamma|\sigma^2,g \sim N\left( \vzero,g\sigma^2\left( \tfrac{1}{n}\mX_\vgamma^T\mX_\vgamma\right)^{-1}\right)
\quad \mbox{and} \quad
p(g|\vgamma_j) = \delta(g; h^{1/(1 + p_\vgamma)})
\end{equation}

\noindent where $h$ is a common prior hyperparameter for all models. After marginalizing
out $\alpha$, $\vbeta$, $\sigma^2$ and $g$ the null based Bayes factor 
for model $\vgamma$ is of the form
$$
\begin{array}{rl}
\ds \mbox{BF}(\vgamma;h)
=
\exp\left[
\frac{n}{2}\log\left( 1 - \frac{h^{1/(1+p_\vgamma)}}{1+h^{1/(1+p_\vgamma)}} R_\vgamma^2 \right) 
- \frac{1}{2}\ln\left(n + h^{-1} \right)
+ \frac{1+p_\vgamma}{2}\ln\left(n + h^{-1/(1+p_\vgamma)} \right)
\right].
\end{array}
$$

\noindent Taking $h\to\infty$ we obtain a null based Bayes factor of
$$
\ds \mbox{BF}(\vgamma)
=
\exp\left[
\frac{n}{2}\log\left( 1 - R_\vgamma^2 \right) 
+ \frac{p_\vgamma}{2}\ln\left(n \right)
\right] = \exp\left[ \frac{1}{2}\mbox{BIC}(\vgamma)\right]
$$

\noindent where $\mbox{BIC}(\vgamma) = n\log\left( 1 - R_\vgamma^2 \right) + p_\vgamma \log(n)$. Note that as $h\to\infty$ the parameter posteriors become
$$\alpha|\vy,\vgamma \sim t_n(0,\widehat{\sigma}_{\vgamma}^2/n), \quad
\vbeta_{\vgamma}|\vy,\vgamma \sim t_n( \widehat{\vbeta}_{\vgamma}, \widehat{\sigma}_{\vgamma}^2 \left(\mX_\vgamma^T\mX_\vgamma  \right)^{-1} ),
\quad \mbox{and} \quad  
\sigma^2|\vy,\vgamma \sim \mbox{IG}\left( \tfrac{n}{2}, \tfrac{n}{2}\widehat{\sigma}_{\vgamma}^2 \right),
$$

\noindent 
where $\widehat{\vbeta}_{\vgamma}$
and $\widehat{\sigma}_{\vgamma}^2$ are the
MLEs corresponding to model $\vgamma$.

 
\section{Implementation}
\label{sec:implementation}

Key to the feasibility of the model selection and averaging is an efficient implementation of these procedures. We employ two main 
strategies to achieve computational efficiency (i) efficient software implementation using
highly optimized software libraries; and (ii) efficient calculation of
$R$-squared values for all models based on using a Gray code and appropriate
matrix algebraic simplifications.
For ease of use we 
implemented an {\tt R} package called {\tt BayesianLinearModelAveraging}.
This internals of {\tt BayesianLinearModelAveraging} is implemented
in {\tt C++} and uses the {\tt R} packages \texttt{Rcpp} and \texttt{RcppEigen} to enhance
computational performance.  

There are two main special functions used in the paper. The 
Gaussian hypergeometric function and the Appell hypergeometric function
of two variables. During the 
implementation process we tried several packages which implemented the
Gaussian hypergeometric function.
We found that the {\tt R} package {\tt gsl} was the most accurate, numerically
stable implementation amongst the packages we tried
\citep{Hankin2006}. Two {\tt R} packages 
implement the Appell
hypergeometric function these are the
{\tt appell} \citep{Bove2013}
and {\tt tolerance} \citep{Young2010}. The 
{\tt appell} function is faster, but rarely works for most of our examples.
The {\tt tolerance} is numerically more stable than {\tt appell} , but uses {\tt R}'s {\tt integrate}
function and is much slower.


 
\subsection{Gray code} 
\label{sec:GrayCode}


The Gray code was originally developed to aid in detecting errors in analog to digital conversions in
communications systems by Frank Gray in 1947 \cite[][Section 22.3]{PressEtal2007}. It is a sequence of binary numbers whose key feature is that
one and only one binary digit is different between binary numbers in the sequence. 

Gray codes can be constructed using a sequence of ``reflect'' and ``prefix'' steps.
Let $\mGamma_1 = (0,1)^T \in \{0,1\}^{2\times 1}$ be the first Gray code matrix matrix and let $\mGamma_k$ be the $k$th Gray code matrix. Then we can obtain the $(k+1)$st Gray code matrix given $\mGamma_k$ via 
$$
\ds \mGamma_{k+1} = \left[\begin{array}{cc}
\vzero & \mGamma_k \\
\vone  & \mbox{reflect}(\mGamma_k)
\end{array} \right]
$$ 

\noindent where $\mbox{reflect}(\mGamma_k)$ is the matrix obtained by reversing the order of rows of $\mGamma_k$, and the $\vzero$ and $\vone$ are vectors of zeros and ones
of length $2^k$ respectively. In {\tt C++} these Gray codes can be efficiently constructed
using bit-shit operations on binary strings in such a way that $\mGamma_{k}$
matrices are never computed and stored explicitly

Gray codes allows the enumeration of the entire model space in an order which only adds
or removes one covariate from the previous model at a time. We can then use standard matrix
inverse results to perform rank
one updates and downdates in the calculation of the $R^2$, $(\mX^T\mX)^{-1}$ and
$\widehat{\vbeta}$ values for each model in the
model space.


\subsection{Model updates and downdates} 

Both updates and downdates depend on the fact that
the inverse of a real symmetric matrix can be written as
\begin{eqnarray}
	\ds \left[ \begin{array}{cc}
		\mA   & \mB \\
		\mB^T & \mC
	\end{array} \right]^{-1}
	&  = &
	\ds \left[ \begin{array}{cc}
		\mI & \vzero \\
		-\mC^{-1}\mB^T &  \mI
	\end{array} \right]
	\left[ \begin{array}{cc}
		\widetilde{\mA} & \vzero \\
		\vzero & \mC^{-1}
	\end{array} \right]
	\left[ \begin{array}{cc}
		\mI    & -\mB\mC^{-1}\\
		\vzero & \mI
	\end{array} \right] \label{eq:blockdiag1}\\
	&  = &
	\ds\left[
	\begin{array}{cc}
		\widetilde{\mA}
		& - \widetilde{\mA}\mB\mC^{-1} \\
		-\mC^{-1}\mB^T\widetilde{\mA}
		& \mC^{-1} + \mC^{-1}\mB^T\widetilde{\mA}\mB\mC^{-1}
	\end{array}\right]\label{eq:blockdiag2}
\end{eqnarray}

\noindent where $\widetilde{\mA} = \left(\mA-\mB\mC^{-1}\mB^T\right)^{-1}$
provided all inverses in (\ref{eq:blockdiag1}) and
(\ref{eq:blockdiag2}) exist. 
For both the update and downdate formula we assume that the quantities
$\mX^T\vy$, $\mX^T\mX$, $(\mX_{\vgamma_i}^T\mX_{\vgamma_i})^{-1}$, 
$\widehat{\vbeta}_{\vgamma_i}$ and $R_{\vgamma_i}^2$ values have been computed from the previous step.

We want to update the model inverse matrix, coefficient vector and $R^2$ values for the model $\vgamma_{i+1}$ where $\mX_{\vgamma_{i+1}}$ is the matrix given by $\mX_{\vgamma_i}$ with a column $\vz$ inserted into the appropriate position.
For clarity of exposition we will assume that the column $\vz$ is inserted into the lasts position, i.e., $\mX_{\vgamma_{i+1}} = [\mX_{\vgamma_{i}},\vz]$.
Then the updates for the model inverse matrix, coefficient estimates, and $R^2$ values can be obtained by following the steps bellow.
\begin{enumerate}
	\item Calculate $\widehat{\vz} = (\mX_{\vgamma_i}^T\mX_{\vgamma_i})^{-1}\mX_{\vgamma_i}^T\vz$, 
	$\kappa 
	= 
	1/(\vz^T(\mI - \mX_{\vgamma_i}(\mX_{\vgamma_i}^T\mX_{\vgamma_i})^{-1}\mX_{\vgamma_i}^T)\vz) 
	= 1/(n - \vz^T\widehat{\vz})$, and  $s = \vy^T(\vz - \widehat{\vz})$.
	
	\item The model inverse matrix can be updated via  
	%The update for the $(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}$ using $(\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}$ is
	%given by the following. 
	$$
	\begin{array}{rl}
	%\left[ \begin{array}{cc}
	%\mX^T\mX & \mX^T\vz \\
	%\vz^T\mX & \vz^T\vz \\
	%\end{array} \right]^{-1}
	(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}
	%& \ds = 
	%\left[ \begin{array}{cc}
	%(\mX^T\mX)^{-1} + \kappa(\mX^T\mX)^{-1}\mX^T\vz\vz^T\mX(\mX^T\mX)^{-1}  & %-(\mX^T\mX)^{-1}\mX^T\vz \kappa \\
	%-\kappa \vz^T\mX(\mX^T\mX)^{-1}              
	%& \kappa 
	%\end{array} \right]
	%\\
	%& \ds = 
	%\left[ \begin{array}{cc}
	%(\mX^T\mX)^{-1} + \kappa\widehat{\vz}\widehat{\vz}^T  & - \widehat{\vz} \kappa \\
	%-\kappa \widehat{\vz}^T             
	%& \kappa 
	%\end{array} \right]
	%\\
	&\ds = 
	\left[ \begin{array}{cc}
	(\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}    & \vzero \\
	\vzero             
	& 0
	\end{array} \right] + \kappa \left[ \begin{array}{r}
	\widehat{\vz} \\
	-1 \\
	\end{array} \right] \left[ \begin{array}{r}
	\widehat{\vz} \\
	-1 \\
	\end{array} \right]^T.
	\end{array} 
	$$
	
	\item
	The coefficient estimators 
	%are given by
	$ 
	\ds \widehat{\vbeta}_{\vgamma_{i}} = (\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}\mX_{\vgamma_{i}}^T\vy$,
	and $\ds \widehat{\vbeta}_{\mbox{\scriptsize update}}  = (\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}\mX_{\vgamma_{i+1}}^T\vy$.  
	Then using the block inverse formula we have
	can be updated via
	$$
	\begin{array}{rl}
	\ds \widehat{\vbeta}_{\vgamma_{i+1}}
	%& \ds = \left[ \begin{array}{c}
	%\widehat{\vbeta} \\
	%0 
	%\end{array} \right] + \kappa  \left[ \begin{array}{cc}
	%(\mX^T\mX)^{-1}\mX^T\vz \left\{ \vz^T\mX(\mX^T\mX)^{-1}\mX^T\vy - \vz^T\vy\right\}  \\
	%\vz^T\vy - \vz^T\mX(\mX^T\mX)^{-1}\mX^T\vy
	%\end{array} \right]
	%\\
	& \ds 
	= \left[ \begin{array}{c}
	\widehat{\vbeta}_{\vgamma_{i}} \\
	0 
	\end{array} \right] - \kappa s  \left[ \begin{array}{r}
	\widehat{\vz}   \\
	- 1
	\end{array} \right].
	\end{array} 
	$$
	
	\item The $R^2$ value van be update via
	% for each model (under the 
	%standardization given in Section \ref{sec:model}) are given by
	$R_{\vgamma_{i}}^2 = \tfrac{1}{n} \vy^T\mX_{\vgamma_{i}}(\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}\mX_{\vgamma_{i}}^T\vy$
	%and
	%$R_{\vgamma_{i+1}}^2 = %\tfrac{1}{n}\vy^T\mX_{\vgamma_{i+1}}(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}\mX_{\vgamma_{i+1}}^T\vy$.
	Then using the block inverse formula
	$$
	\begin{array}{rl}
	\ds 
	R_{\vgamma_{i+1}}^2 
	%& \ds = \tfrac{1}{n} \vy^T\mX(\mX^T\mX)^{-1}\mX^T\vy
	%+ \tfrac{\kappa}{n}\left[ 
	%\widehat{\vy}^T\vz\vz^T\widehat{\vy}
	%- 2\widehat{\vy}^T\vz\vz^T\vy 
	%+ \vy^T\vz\vz^T\vy 
	%\right]
	= R_{\vgamma_{i}}^2
	+ \frac{\kappa s^2}{n}.
	
	\end{array}
	$$
	
	%\item
	%\noindent The model determinants are given by
	%$D = |\mX^T\mX|$
	%and
	%$D_{\mbox{\tiny update}} = |\mC^T\mC|$.
	%Using the block determinant formula we have
	%$D_{\mbox{\tiny update}} = D/c$.
\end{enumerate}

\noindent Presuming  have been precomputed
the above updates costs $O(p_{\vgamma_{i}}^2 + n)$ time.

Suppose want to downdate the model summary quantities for the model
$\vgamma_{i+1}$ where $\mX_{\vgamma_{i+1}}$ is the matrix given by 
$\mX_{\vgamma_i}$ with a column $\vz$ removed from the appropriate position.
Similarly as for updates for clarity of exposition we will assume that
$\vz$ will be removed from the last column of $\mX_{\vgamma_i}$, i.e., 
$\mX_{\vgamma_{i}} = [\mX_{\vgamma_{i+1}}, \vz]$. 
Then the downdates for model summary values are given by the following steps.
\begin{enumerate}
	\item 
	%The downdate for the model inverse matrix to obtain $(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}$
	%from $(\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}$ can be found using the block-inverse formula.
	Suppose we partition the matrix
	$(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}$ so that
	$$
	\ds (\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1} 
	= \left[ \begin{array}{cc}
	\mA   & \vb \\
	\vb^T & c \\
	\end{array} \right].
	%= 
	%\left[ \begin{array}{cc}
	%\mX^T\mX & \mX^T\vz \\
	%\vz^T\mX & \vz^T\vz \\
	%\end{array} \right]^{-1}
	$$
	
	\noindent The model inverse matrix downdate can be obtained
	via   $(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1} = \mA - c^{-1}\vb\vb^T$.
	
	\item Calculate
	$\widehat{\vz} = (\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}\mX_{\vgamma_{i+1}}^T\vz$ using step 1.,
	$\kappa = 1/(n - \vz^T\widehat{\vz})$,
	and $s = \vy^T(\vz - \widehat{\vz})$.
	
	\item 
	%Let $\widehat{\vbeta}_{\vgamma_{i+1}} = (\mX_{\vgamma_{i+1}}^T\mX)^{-1}\mX_{\vgamma_{i+1}}^T\vy$
	%and $\widehat{\vbeta}_{\vgamma_{i}} = %(\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}\mX_{\vgamma_{i}}^T\vy$.
	%Then
	The coefficient estimates downdate can be obtained
	via
	$$
	\widehat{\vbeta}_{\vgamma_{i+1}} = \left[ \widehat{\vbeta}_{\vgamma_{i}} \right]_{-|{\vgamma_{i}}|} + \kappa s\widehat{\vz},
	$$
	
	\noindent where $[ \widehat{\vbeta}_{\vgamma_{i}}]_{-|{\vgamma_{i}}|}$
	removes the last column from $\widehat{\vbeta}_{\vgamma_{i}}$.
	
	\item 
	%Let $R_{\vgamma_{i}}^2 = \tfrac{1}{n}\vy^T\mX_{\vgamma_{i}}(\mX_{\vgamma_{i}}^T\mX_{\vgamma_{i}})^{-1}\mX_{\vgamma_{i}}^T\vy$
	%and $R_{\vgamma_{i+1}}^2 = \tfrac{1}{n} \vy^T\mX_{\vgamma_{i+1}}(\mX_{\vgamma_{i+1}}^T\mX_{\vgamma_{i+1}})^{-1}\mX_{\vgamma_{i+1}}^T\vy$.
	The $R^2$ downdate can be obtained
	via
	$$
	R_{\vgamma_{i+1}}^2 = R_{\vgamma_{i}}^2 - \frac{\kappa s^2}{n}.
	$$
	
	
	%\item Let  $D = |\mC^T\mC|$ and $D_{\mbox{\tiny downdate}} = |\mX^T\mX|$ then
	%$D_{\mbox{\tiny downdate}} = cD$.
\end{enumerate}

\noindent Again, presuming relevant summary quantities have been precomputed
the updates for all of the above quantities costs $O(p_{\vgamma_{i}}^2 + n)$ time.



 
 
\section{Numerical results}
\label{sec:numerical}

We will now compare three different popular {\tt R} implementations of Bayesian model averaging on 
several small datasets. We compare the {\tt R} packages {\tt BAS} \citep{Clyde2017}, 
{\tt BayesVarSelect} \citep{Garcia-Donato2016}, and  {\tt BMS} \citep{Zeugner2015}. For each method
we assumed a uniform prior on the model space, i.e., $p(\vgamma)\propto 2^{-p}$. We used the
setting implied by the following commands for each of these methods:
\begin{itemize}
	\item {\tt BAS}: We used the command
\begin{verbatim}
bas.lm(y~X,prior=prior.val,modelprior=uniform(), initprobs="uniform")
\end{verbatim}

	where \verb|prior.val| takes one of the values \verb|"hyper-g"|, \verb|"hyper-g-laplace"| and\\ \verb|"hyper-g-n"| corresponding to a direct implementation of (\ref{eq:hyperGmarginal}),
	a Laplace approximation of (\ref{eq:hyperGmarginalIntegral}), and the 
	Laplace approximation of (\ref{eq:hyperGonNmarginalIntegral}) respectively. The
	hyperprior parameter $a=3$ is implicitly used.
	
	\item {\tt BayesVarSelect}: We used the command
\begin{verbatim}
Bvs(formula="y~.",fixed.cov=c("Intercept"),data=data.frame(y=y,X=X),
    prior.betas=prior.val,prior.models="Constant",time.test=FALSE,
    priorprobs=NULL,n.keep=50000)
\end{verbatim}

    
\noindent where \verb|prior.val| takes one of the values \verb|"Liangetal"| or \verb|"Robust"| 
corresponding to a direct implementation of (\ref{eq:hyperGmarginal}) with $a=3$, and a hybrid approach which
uses (\ref{eq:yGivenGammaRobust}) directly and numerical quadrature based on (\ref{eq:marginalLikelihoodRobust}) if this fails. The
hyperprior parameter $a=3$ is implicitly used.

\item {\tt BMS}: We used the command
\begin{verbatim}
bms(cbind(y,X),nmodel=50000,mcmc="enumerate",g="hyper=3",
    g.stats=TRUE,mprior="uniform")	
\end{verbatim}

\noindent which uses a direct implementation of (\ref{eq:hyperGmarginal}) for the hyper-$g$
prior with $a=3$.
\end{itemize}

We considered several small datasets to illustrate our methodology. These datasets
can be found in the {\tt R} packages {\tt MASS} \citep{Venables2002} and 
{\tt Ecdat} \citep{Croissant2016}. Table \ref{tab:datasets} summarizing the sizes and 
locations of various datasets. We chose {\tt USCrime} data because it is used in most papers in
the area and is small enough so that n\"aive implementations using special functions will
not lead to numerical issues. The {\tt Hitters} nad {\tt Kakadu} datasets are chosen to
be large enough to begin to strain the resources of a typical 2017 computer so that
relative differences in speeds between different packages becomes apparent. Finally, the
{\tt Kakadu} dataset will lead numerical instability in the direct evaluation of Bayes
factors for some of the priors on $g$ considered in this paper.

\begin{table}[h]
	\begin{center}
\begin{tabular}{l|r|r|l|l}
{\bf Dataset}	& $\vn$ & $\vp$ & Response & {\bf {\tt R} package} \\ 
	\hline 
UScrime 	& 47 & 15 & y & {\tt MASS} \\  
%Bodyfat	& 244  & 13 &  \\ 
%	\hline 
Hitters	& 263 & 19 & Salary & {\tt ISLR} \\ 
%	\hline 
%Wage	& 3000 & 17 &  {\tt ISLR}  \\
Kakadu	& 1827 & 22 & income & {\tt Ecdat}   \\  
VietNamI	& 27765 & 11 & lnhhexp & {\tt Ecdat}  \\ 
\end{tabular} 
	\end{center}
\caption{A summary of the datasets used in the paper and their respective {\tt R} packages.}
\label{tab:datasets}
\end{table}
 
For each of the datasets we first used the {\tt R} command {\tt na.omit()} to remove missing values. For {\tt USCrime} all variables except {\tt S} was log-transformed. For all datasets
the {\tt R} command {model.matrix()} was used to construct the design matrix using all 
variables as predictors.

\bigskip 

{\bf We need to think carefully about how to compare the methods in a concise  and fair way.}

\begin{table}[ht]
	\centering
{\small 
	\begin{tabular}{l|l|rrrrrr}
	&	Package & BAS       & BAS       & BVS        & BMS       & BLMA      & BLMA       \\
		\hline 
Package & Method  &  Direct    & Laplace   & Quad & ?         & Direct    & Safe      \\ 
\hline
BAS		& Direct  &            &           &      &           &           &           \\ 
BAS     & Laplace &            &           &      &           &           &           \\ 
BVS     & Quad    &            &           &      &           &           &           \\ 
BMS     & Direct  &            &           &      &           &           &           \\ 
BLMA    & Direct  &            &           &      &           &           &           \\ 
BLMA    & Safe    &            &           &      &           &           &           \\ 
	\end{tabular}
}
	\caption{The 2-norms between posterior inclusion probabilities for USCrime dataset 
	for the hyper-$g$ prior. Blank indicates an error occurred in one of the methods.}
	\label{tab:SummaryResults1}
\end{table}

 
 
\section{Conclusion}
\label{sec:conclusion}

We have reviewed the prior structures that lead to closed form expressions for Bayes factors for
linear models. We have described ways that each of these priors, except for the hyper-$g/n$ 
prior can be evaluated in a numerically stable manner and have implemented a package 
{\tt BayesianLinearModelAveraging} for performing full exact Bayesian model averaging using
this methodology. Our package is competitive with {\tt BAS} and {\tt BMS} in terms of computational speed,
is numerically more stable and accurate, and offers some different priors structures not
offered in {\tt BAS}. Our package is much faster than {\tt BayesVarSelect} and is also
numerically more stable and accurate.

We are currently working on several extensions to this work. Firstly, we are working on a
parallel implementation of the package which will allow for exact Bayesian inference for
problems roughly the size $p\approx 30$. Secondly, we are currently implementing  Markov
Chain Monte Carlo (MCMC) and population based MCMC methods for exploring the model space
when $p>30$. Lastly, we are deriving exact expressions for parameter posterior distributions
under some of the prior structures we have considered here.

\newpage 

\section*{Appendix A: Useful integrals}

The following integrals appear in \cite{Gradshteyn2007}.
\begin{itemize}
\item 
Equation 3.194 (iii):
$$
\int_0^\infty \frac{ x^{\mu - 1} }{(1 + \beta x)^\nu} dx = \beta^{-\mu} \mbox{Beta}(\mu,\nu - \mu) \quad   \mbox{(assuming $\mu,\nu>0$ and $\nu>\mu$).}
$$

 


%\item 
%Equation 3.383 (i):
%$$
%\int_{0}^u x^{\nu - 1} (u - x)^{\mu - 1}  e^{\beta x} dx = \mbox{Beta}(\nu,\mu) {}_1 %F_1(\nu;\mu+\nu;\beta u) \quad   \mbox{(assuming $\mbox{Re}(\mu)>0$ and %$\mbox{Re}(\nu)>0$).}
%$$


\item  Equation 3.197(5):
$$
\int_0^\infty x^{\lambda - 1}(1 + x)^\nu (1 + \alpha x)^\mu dx
=\mbox{Beta}(\lambda,-\mu-\nu-\lambda){}_2F_1(-\mu,\lambda;-\mu-\nu; 1 - \alpha)
$$

\noindent provided $-(\mu  + \nu) > \lambda > 0$.
 

\item Equation 7.621 (4) 
(the second line below is corrected from what appears on \cite{Gradshteyn2007} based on 
\cite{Erdelyi1953} Equation 269(5)):
is
$$
\begin{array}{rll}
\ds \int_0^\infty e^{-st} t^{b-1} {}_1F_1(a,c,kt) dt
& \ds = \Gamma(b)s^{-b} F(a,b,c,k s^{-1}) 
& \mbox{(if $|s| > |k|$)}
\\
& \ds = \Gamma(b)(s - k)^{-b} F\left(c - a, b; c; \frac{k}{k - s} \right)
& \mbox{(if $|s - k| > |k|$)}
\end{array} 
$$

\noindent assuming that $\mbox{Re}(b)>0$, $\mbox{Re}(s) > \max(0,\mbox{Re}(k))$.

\item Equation 3.385:
\begin{equation} \label{res:04}
\int_{0}^1 x^{\nu - 1} (1 - x)^{\lambda - 1}(1 - \beta x)^{\varrho} e^{-\mu x} dx 
= \mbox{Beta}(\nu,\lambda) {}_1 F_1(\nu,\varrho,\lambda+\nu,-\mu,\beta)
\end{equation}

\noindent provided $\mbox{Re}(\lambda)>0$, $\mbox{Re}(\nu)>0$ and $|\mbox{arg}(1-\beta)|<\pi$.





 

\end{itemize}

\newpage 
 
\section*{Appendix B: Raw results}


\begin{table}[ht]
	\centering
	{\small 
	\begin{tabular}{c|r|r|rrrrrr}
		Prior   & BIC    & ZE     & hyper-$g$ & hyper-$g$ & hyper-$g$  & hyper-$g$ & hyper-$g$ & hyper-$g$ \\
		\hline 
		Package & BLMA   & BLMA   & BAS       & BAS       & BVS        & BMS       & BLMA      & BLMA       \\
		\hline 
		Method  & Direct & Direct & Direct    & Laplace   & Quad & ?         & Direct    & Safe      \\ 
		\hline
		1 & 70.87 & 65.51 & 65.93 & 65.99 & 65.10 & 65.93 & 65.93 & 65.93 \\ 
		2 & 19.06 & 22.88 & 25.52 & 25.54 & 22.91 & 25.52 & 25.52 & 25.52 \\ 
		3 & 92.07 & 86.91 & 86.23 & 86.28 & 86.51 & 86.23 & 86.23 & 86.23 \\ 
		4 & 72.53 & 69.65 & 69.20 & 69.22 & 69.51 & 69.20 & 69.20 & 69.20 \\ 
		5 & 37.01 & 42.36 & 44.61 & 44.61 & 42.52 & 44.61 & 44.61 & 44.61 \\ 
		6 & 15.82 & 20.18 & 23.06 & 23.08 & 20.26 & 23.06 & 23.06 & 23.06 \\ 
		7 & 27.06 & 32.43 & 34.55 & 34.55 & 32.59 & 34.55 & 34.55 & 34.55 \\ 
		8 & 60.64 & 56.91 & 57.34 & 57.39 & 56.63 & 57.34 & 57.34 & 57.34 \\ 
		9 & 36.92 & 35.81 & 37.66 & 37.71 & 35.61 & 37.66 & 37.66 & 37.66 \\ 
		10 & 21.92 & 24.35 & 27.06 & 27.10 & 24.29 & 27.06 & 27.06 & 27.06 \\ 
		11 & 55.84 & 50.19 & 51.25 & 51.32 & 49.75 & 51.25 & 51.25 & 51.25 \\ 
		12 & 17.39 & 21.57 & 24.46 & 24.48 & 21.63 & 24.46 & 24.46 & 24.46 \\ 
		13 & 99.92 & 99.69 & 99.50 & 99.51 & 99.66 & 99.50 & 99.50 & 99.50 \\ 
		14 & 90.27 & 84.92 & 83.87 & 83.92 & 84.55 & 83.87 & 83.87 & 83.87 \\ 
		15 & 17.63 & 22.55 & 25.49 & 25.51 & 22.65 & 25.49 & 25.49 & 25.49  \\ 
		\hline
	\end{tabular}
}
	\caption{USCrime}
	\label{tab:USCrimeResults1}
\end{table}


\begin{table}[ht]
	\centering
		{\small 
	\begin{tabular}{c|rrr|rrrr}
		Prior   & hyper-$g/n$ & hyper-$g/n$ & hyper-$g/n$ & Robust &  Robust & Robust & Robust \\		
		\hline
		Package & BAS         & BLMA        & BLMA        & BVS & BLMA & BLMA & BLMA  \\
		\hline 
		Method  & Laplace     & appell      & Quad  & Hybrid & (\ref{eq:yGivenGammaRobust}) & (\ref{eq:yGivenGammaRobust2}) & Safe \\ 
		\hline
		1 & 65.14 & 65.10 & 65.10 & 64.74 & NaN & 64.74 & 64.74 \\ 
		2 & 22.93 & 22.91 & 22.91 & 24.51 & NaN & 24.51 & 24.51 \\ 
		3 & 86.54 & 86.51 & 86.51 & 85.59 & NaN & 85.59 & 85.59 \\ 
		4 & 69.52 & 69.51 & 69.51 & 69.02 & NaN & 69.02 & 69.02 \\ 
		5 & 42.53 & 42.52 & 42.52 & 44.08 & NaN & 44.08 & 44.08 \\ 
		6 & 20.27 & 20.26 & 20.26 & 22.04 & NaN & 22.04 & 22.04 \\ 
		7 & 32.59 & 32.59 & 32.59 & 34.08 & NaN & 34.08 & 34.08 \\ 
		8 & 56.66 & 56.63 & 56.63 & 56.47 & NaN & 56.47 & 56.47 \\ 
		9 & 35.64 & 35.61 & 35.61 & 36.35 & NaN & 36.35 & 36.35 \\ 
		10 & 24.31 & 24.29 & 24.29 & 25.78 &NaN  & 25.78 & 25.78 \\ 
		11 & 49.79 & 49.75 & 49.75 & 49.66 & NaN & 49.66 & 49.66 \\ 
		12 & 21.65 & 21.63 & 21.63 & 23.40 & NaN & 23.40 & 23.40 \\ 
		13 & 99.66 & 99.66 & 99.66 & 99.54 & NaN & 99.54 & 99.54 \\ 
		14 & 84.57 & 84.55 & 84.55 & 83.45 & NaN & 83.45 & 83.45 \\ 
		15 & 22.67 & 22.65 & 22.65 & 24.52 & NaN & 24.52 & 24.52 \\ 
		\hline
	\end{tabular}
}
	\caption{USCrime}
	\label{tab:USCrimeResults2}
\end{table} 






\begin{table}[ht]
	\centering
	{\small 	
	\begin{tabular}{c|r|r|rrrrrr}
		Prior   & BIC    & ZE     & hyper-$g$ & hyper-$g$ & hyper-$g$  & hyper-$g$ & hyper-$g$ & hyper-$g$ \\
		\hline 
		Package & BLMA   & BLMA   & BAS       & BAS       & BVS        & BMS       & BLMA      & BLMA       \\
		\hline 
		Method  & Direct & Direct & Direct    & Laplace   & Quad & ?         & Direct    & Safe      \\ 
		\hline
		1 & 73.48 & 85.14 & 89.24 & 89.26 & 89.45 & 89.24 & 89.24 & 89.24 \\ 
		2 & 82.17 & 90.11 & 92.85 & 92.86 & 93.02 & 92.85 & 92.85 & 92.85 \\ 
		3 & 6.90 & 12.77 & 17.40 & 17.42 & 16.63 & 17.40 & 17.40 & 17.40 \\ 
		4 & 11.90 & 17.39 & 22.22 & 22.24 & 21.45 & 22.22 & 22.22 & 22.22 \\ 
		5 & 7.54 & 13.06 & 17.51 & 17.53 & 16.74 & 17.51 & 17.51 & 17.51 \\ 
		6 & 86.09 & 92.64 & 94.55 & 94.56 & 94.79 & 94.55 & 94.55 & 94.55 \\ 
		7 & 11.84 & 19.71 & 24.75 & 24.77 & 24.06 & 24.75 & 24.75 & 24.75 \\ 
		8 & 45.33 & 49.94 & 53.58 & 53.59 & 53.00 & 53.58 & 53.58 & 53.58 \\ 
		9 & 43.72 & 44.14 & 45.66 & 45.66 & 44.91 & 45.66 & 45.66 & 45.66 \\ 
		10 & 42.19 & 47.96 & 50.05 & 50.05 & 49.78 & 50.05 & 50.05 & 50.05 \\ 
		11 & 45.30 & 62.34 & 68.96 & 69.00 & 69.07 & 68.96 & 68.96 & 68.96 \\ 
		12 & 54.61 & 56.19 & 58.57 & 58.59 & 58.45 & 58.57 & 58.57 & 58.57 \\ 
		13 & 42.74 & 63.34 & 71.11 & 71.16 & 71.32 & 71.11 & 71.11 & 71.11 \\ 
		14 & 9.16 & 17.27 & 22.98 & 23.01 & 22.22 & 22.98 & 22.98 & 22.98 \\ 
		15 & 88.71 & 92.14 & 93.15 & 93.16 & 93.42 & 93.15 & 93.15 & 93.15 \\ 
		16 & 96.64 & 98.16 & 98.56 & 98.56 & 98.67 & 98.56 & 98.56 & 98.56 \\ 
		17 & 12.86 & 25.71 & 33.81 & 33.85 & 33.15 & 33.81 & 33.81 & 33.81 \\ 
		18 & 6.46 & 13.01 & 18.14 & 18.16 & 17.35 & 18.14 & 18.14 & 18.14 \\ 
		19 & 7.11 & 13.74 & 18.72 & 18.74 & 17.97 & 18.72 & 18.72 & 18.72 \\ 
		\hline
	\end{tabular}
}
	\caption{Hitters}
	\label{tab:HittersResults1}
\end{table}

\begin{table}[ht]
	\centering
	{\small 
	\begin{tabular}{c|rrr|rrrr}
		Prior   & hyper-$g/n$ & hyper-$g/n$ & hyper-$g/n$ & Robust &  Robust & Robust & Robust \\		
		\hline
		Package & BAS         & BLMA        & BLMA        & BVS & BLMA & BLMA & BLMA  \\
		\hline 
		Method  & Laplace     & appell      & Quad  & Hybrid & (\ref{eq:yGivenGammaRobust}) & (\ref{eq:yGivenGammaRobust2}) & Safe \\ 
		\hline
		1 & 87.55 & NaN & 87.54 & 89.45 & NaN & 89.45 & 89.45 \\ 
		2 & 91.73 & NaN & 91.72 & 93.02 & NaN & 93.02 & 93.02 \\ 
		3 & 15.22 & NaN & 15.22 & 16.63 & NaN & 16.63 & 16.63 \\ 
		4 & 19.94 & NaN & 19.94 & 21.45 & NaN & 21.45 & 21.45 \\ 
		5 & 15.41 & NaN & 15.41 & 16.74 & NaN & 16.74 & 16.74 \\ 
		6 & 93.79 & NaN & 93.78 & 94.79 & NaN & 94.79 & 94.79 \\ 
		7 & 22.44 & NaN & 22.43 & 24.06 & NaN & 24.06 & 24.06 \\ 
		8 & 51.88 & NaN & 51.88 & 53.00 & NaN & 53.00 & 53.00 \\ 
		9 & 44.86 & NaN & 44.86 & 44.91 & NaN & 44.91 & 44.91 \\ 
		10 & 49.13 & NaN & 49.12 & 49.78 & NaN & 49.78 & 49.78 \\ 
		11 & 66.18 & NaN & 66.16 & 69.07 & NaN & 69.07 & 69.07 \\ 
		12 & 57.46 & NaN & 57.46 & 58.45 & NaN & 58.45 & 58.45 \\ 
		13 & 67.86 & NaN & 67.85 & 71.32 & NaN & 71.32 & 71.32 \\ 
		14 & 20.35 & NaN & 20.34 & 22.22 & NaN & 22.22 & 22.22 \\ 
		15 & 92.76 & NaN & 92.75 & 93.42 & NaN & 93.42 & 93.42 \\ 
		16 & 98.41 & NaN & 98.41 & 98.67 & NaN & 98.67 & 98.67 \\ 
		17 & 30.15 & NaN & 30.14 & 33.15 & NaN & 33.15 & 33.15 \\ 
		18 & 15.74 & NaN & 15.73 & 17.35 & NaN & 17.35 & 17.35 \\ 
		19 & 16.40 & NaN & 16.39 & 17.97 & NaN & 17.97 & 17.97 \\ 
		\hline
	\end{tabular}
}
	\caption{Hitters}
	\label{tab:HittersResults2}
\end{table}



\begin{table}[ht]
	\centering
	{\small 	
	\begin{tabular}{c|r|r|rrrrrr}
		Prior   & BIC    & ZE     & hyper-$g$ & hyper-$g$ & hyper-$g$  & hyper-$g$ & hyper-$g$ & hyper-$g$ \\
		\hline 
		Package & BLMA   & BLMA   & BAS       & BAS       & BVS        & BMS       & BLMA      & BLMA       \\
		\hline 
		Method  & Direct & Direct & Direct    & Laplace   & Quad & ?         & Direct    & Safe      \\ 
		\hline
		1 & 11.96 & 20.36 & 34.62 & 34.63 & NaN & 34.69 & 34.69 & 34.69 \\ 
		2 & 43.60 & 47.24 & 50.35 & 50.35 & NaN & 50.34 & 50.34 & 50.34 \\ 
		3 & 3.00 & 7.49 & 16.97 & 16.98 & NaN & 17.10 & 17.10 & 17.10 \\ 
		4 & 37.14 & 42.02 & 46.86 & 46.88 & NaN & 46.87 & 46.87 & 46.87 \\ 
		5 & 81.87 & 86.11 & 90.49 & 90.50 & NaN & 90.41 & 90.41 & 90.41 \\ 
		6 & 16.83 & 26.67 & 41.68 & 41.70 & NaN & 41.83 & 41.83 & 41.83 \\ 
		7 & 3.22 & 8.89 & 21.41 & 21.42 & NaN & 21.53 & 21.53 & 21.53 \\ 
		8 & 4.30 & 11.09 & 23.57 & 23.60 & NaN & 23.66 & 23.66 & 23.66 \\ 
		9 & 2.62 & 7.19 & 16.97 & 16.98 & NaN & 17.09 & 17.09 & 17.09 \\ 
		10 & 52.53 & 77.78 & 90.98 & 90.99 & NaN & 90.81 & 90.81 & 90.81 \\ 
		11 & 92.51 & 93.73 & 94.78 & 94.77 & NaN & 94.58 & 94.58 & 94.58 \\ 
		12 & 99.82 & 99.94 & 99.97 & 99.97 & NaN & 99.97 & 99.97 & 99.97 \\ 
		13 & 2.45 & 6.60 & 15.70 & 15.72 & NaN & 15.84 & 15.84 & 15.84 \\ 
		14 & 8.10 & 19.91 & 38.61 & 38.63 & NaN & 38.66 & 38.66 & 38.66 \\ 
		15 & 8.17 & 18.51 & 35.18 & 35.19 & NaN & 35.24 & 35.24 & 35.24 \\ 
		16 & 62.99 & 75.30 & 83.40 & 83.42 & NaN & 83.30 & 83.30 & 83.30 \\ 
		17 & 3.27 & 8.53 & 19.41 & 19.42 & NaN & 19.54 & 19.54 & 19.54 \\ 
		18 & 54.75 & 74.93 & 86.65 & 86.66 & NaN & 86.55 & 86.55 & 86.55 \\ 
		19 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & 100.00 & 100.00 & 100.00 \\ 
		20 & 26.63 & 44.11 & 62.58 & 62.61 & NaN & 62.56 & 62.56 & 62.56 \\ 
		21 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & 100.00 & 100.00 & 100.00 \\ 
		22 & 4.95 & 13.22 & 29.04 & 29.05 & NaN & 29.12 & 29.12 & 29.12 \\ 
		\hline
	\end{tabular}
}
	\caption{Kakadu}
	\label{tab:KakaduResults1}
\end{table}



\begin{table}[ht]
	\centering
	{\small 	
	\begin{tabular}{c|rrr|rrrr}
		Prior   & hyper-$g/n$ & hyper-$g/n$ & hyper-$g/n$ & Robust &  Robust & Robust & Robust \\		
		\hline
		Package & BAS         & BLMA        & BLMA        & BVS & BLMA & BLMA & BLMA  \\
		\hline 
		Method  & Laplace     & appell      & Quad  & Hybrid & (\ref{eq:yGivenGammaRobust}) & (\ref{eq:yGivenGammaRobust2}) & Safe \\ 
		\hline
		1 & 31.99 & NaN & 32.04 & 26.46 & NaN & 26.46 & 26.46 \\ 
		2 & 49.80 & NaN & 49.78 & 48.73 & NaN & 48.73 & 48.73 \\ 
		3 & 15.03 & NaN & 15.13 & 11.20 & NaN & 11.20 & 11.20 \\ 
		4 & 46.02 & NaN & 46.01 & 44.28 & NaN & 44.28 & 44.28 \\ 
		5 & 89.93 & NaN & 89.85 & 88.59 & NaN & 88.59 & 88.59 \\ 
		6 & 39.00 & NaN & 39.10 & 33.27 & NaN & 33.27 & 33.27 \\ 
		7 & 18.85 & NaN & 18.95 & 13.82 & NaN & 13.82 & 13.82 \\ 
		8 & 21.21 & NaN & 21.26 & 16.34 & NaN & 16.34 & 16.34 \\ 
		9 & 14.97 & NaN & 15.07 & 11.04 & NaN & 11.04 & 11.04 \\ 
		10 & 89.58 & NaN & 89.44 & 85.92 & NaN & 85.92 & 85.92 \\ 
		11 & 94.64 & NaN & 94.49 & 94.35 & NaN & 94.35 & 94.35 \\ 
		12 & 99.97 & NaN & 99.97 & 99.96 & NaN & 99.96 & 99.96 \\ 
		13 & 13.81 & NaN & 13.92 & 10.13 & NaN & 10.13 & 10.13 \\ 
		14 & 35.36 & NaN & 35.39 & 28.37 & NaN & 28.37 & 28.37 \\ 
		15 & 32.20 & NaN & 32.24 & 25.87 & NaN  & 25.87 & 25.87 \\ 
		16 & 82.40 & NaN & 82.29 & 79.98 & NaN & 79.98 & 79.98 \\ 
		17 & 17.21 & NaN & 17.31 & 12.85 & NaN & 12.85 & 12.85 \\ 
		18 & 85.31 & NaN & 85.22 & 81.95 & NaN & 81.95 & 81.95 \\ 
		19 & 100.00 & NaN & 100.00 & 100.00 & NaN & 100.00 & 100.00 \\ 
		20 & 59.87 & NaN & 59.83 & 53.58 & NaN & 53.58 & 53.58 \\ 
		21 & 100.00 & NaN & 100.00 & 100.00 & NaN  & 100.00 & 100.00 \\ 
		22 & 26.04 & NaN & 26.09 & 19.87 & NaN  & 19.87 & 19.87 \\ 
		\hline
	\end{tabular}
}
	\caption{Kakadu}
	\label{tab:KakaduResults2}
\end{table}



\begin{table}[ht]
	\centering
	\begin{tabular}{c|r|r|rrrrrr}
		Prior   & BIC    & ZE     & hyper-$g$ & hyper-$g$ & hyper-$g$  & hyper-$g$ & hyper-$g$ & hyper-$g$ \\
		\hline 
		Package & BLMA   & BLMA   & BAS       & BAS       & BVS        & BMS       & BLMA      & BLMA       \\
		\hline 
		Method  & Direct & Direct & Direct    & Laplace   & Quad & ?         & Direct    & Safe      \\ 
		\hline
		1 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN  & NaN & 100.00 \\ 
		2 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN & NaN  & 100.00 \\ 
		3 & 1.21   & 3.16   & 8.65   & 8.65   & NaN & NaN & NaN   & 8.64 \\ 
		4 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN & NaN & 100.00 \\ 
		5 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN & NaN & 100.00 \\ 
		6 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN & NaN & 100.00 \\ 
		7 & 0.62   & 1.72   & 5.33   & 5.33   & NaN & NaN & NaN & 5.32 \\ 
		8 & 96.07  & 98.32  & 99.35  & 99.35  & NaN & NaN & NaN & 99.35 \\ 
		9 & 3.28    & 8.16  & 20.69  & 20.69  & NaN & NaN & NaN & 20.66 \\ 
		10 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN & NaN & 100.00 \\ 
		11 & 100.00 & 100.00 & 100.00 & 100.00 & NaN & NaN & NaN & 100.00 \\ 
		\hline
	\end{tabular}
	\caption{VietNamI}
	\label{tab:VietNamIResults1}
\end{table}

\begin{table}[ht]
	\centering
	\centering
	\begin{tabular}{c|rrr|rrrr}
		Prior   & hyper-$g/n$ & hyper-$g/n$ & hyper-$g/n$ & Robust &  Robust & Robust & Robust \\		
		\hline
		Package & BAS         & BLMA        & BLMA        & BVS & BLMA & BLMA & BLMA  \\
		\hline 
		Method  & Laplace     & appell      & Quad  & Hybrid & (\ref{eq:yGivenGammaRobust}) & (\ref{eq:yGivenGammaRobust2}) & Safe \\ 
		\hline
		1 & 100.00 & NaN & NaN & NaN & NaN & 100.00 & 100.00 \\ 
		2 & 100.00 & NaN & NaN & NaN & NaN & 100.00 & 100.00 \\ 
		3 & 7.17 & NaN & NaN & NaN & NaN & 4.77 & 4.77 \\ 
		4 & 100.00 & NaN & NaN & NaN & NaN & 100.00 & 100.00 \\ 
		5 & 100.00 & NaN & NaN & NaN & NaN & 100.00 & 100.00 \\ 
		6 & 100.00 & NaN & NaN & NaN  & NaN  & 100.00 & 100.00 \\ 
		7 & 4.30 & NaN & NaN &NaN  & NaN & 2.70 & 2.70 \\ 
		8 & 99.21 & NaN & NaN & NaN & NaN  & 98.86 & 98.86 \\ 
		9 & 17.46 & NaN & NaN & NaN & NaN & 12.02 & 12.02 \\ 
		10 & 100.00 & NaN & NaN & NaN & NaN & 100.00 & 100.00 \\ 
		11 & 100.00 & NaN & NaN & NaN & NaN & 100.00 & 100.00 \\ 
		\hline
	\end{tabular}
	\caption{VietNamI}
	\label{tab:VietNamIResults2}
\end{table}

\section{Alternatives to putting a prior on $g$}
\label{sec:MarginalLikelihood}

The prior hyperparameter $g$ needs to be specified with exquisite care. 
In the following discussion let $\vgamma^*$ be the true model from which the data was generated. 
Selecting $g$ using a subjective elicitation of priors is usually criticized in the context of 
hypothesis testing so alternatives should be pursued. 

One may be tempted by selecting $g$ to be a large constant as to make the prior on
$\vbeta_\vgamma$ to be diffuse. This choice is problematic due to Bartlett's paradox. 
\begin{description}
	\item[Problem 1.] Bartlett's paradox: $p(\vzero|\vy) \to 1$ as $g\to\infty$.
\end{description}

\noindent This consequence is obvious if one considers the behaviour of 
(\ref{eq:yGivenG}) as $\to\infty$. In this case $\mbox{BF}(\vgamma_j)\to 0$  as $g\to\infty$ 
for all models except $\vgamma_j = \vzero$.
Bartlett's paradox is paradoxical in the sense that as the prior for $\vbeta_\vgamma$
is made increasingly uninformative the null model becomes increasingly preferred.

If $g$ cannot be made arbitrarily large one might consider setting $g$ to be some constant
value. The unit information prior of Kass \& Wasserman (1995), risk inflation criterion prior of 
Foster \& George (1994), and the benchmark prior of Fern\'andez et al. (2001) set $g$ to $g=n$, 
$g=p_\vgamma^2$ and $g = \max(n,p_\vgamma^2)$ respectively. 
However, these choices lead to what \cite{Liang2008} refer to as
the information paradox. 
\begin{description}
	\item[Problem 2.] Information paradox: $p(\vgamma^*|\vy) \not\to 1$ as $n\to\infty$.
\end{description}

\noindent  
\cite{Liang2008} showed that these choices
are also problematic since using any particular constant $g$ will lead to the information paradox.

A third approach might to be to select $g$ adaptively using an empirical Bayes procedure.
Selecting $g$ via an empirical Bayes procedure. 
\cite{Liang2008} considers two such procedures, a local and a global procedure.
The local empirical Bayes for a given $\vgamma$ global empirical Bayes choices
for $g$ are
$$
\ds \widehat{g}_{\mbox{\scriptsize EBL}}(\vgamma) = \argmax_{g>0} \big\{ \, p(\vy|g,\vgamma) \, \big\}
\qquad \mbox{and} \qquad 
\ds \widehat{g}_{\mbox{\scriptsize EBG}} = \argmax_{g>0} \big\{ \, p(\vy|g) = \sum_{\vgamma}  p(\vy,\vgamma|g)  \, \big\},
$$ 

\noindent respectively. \cite{Liang2008} show that setting
$g = \widehat{g}_{\mbox{\scriptsize EBL}}$ or 
$g = \widehat{g}_{\mbox{\scriptsize EBG}}$
are only partially model selection consistent in the following sense.
\begin{description}
	\item[Problem 3.] Partial model selection consistency: $p(\vgamma^*|\vy) \to 1$ as $n\to\infty$ except
	when $\vgamma^* = \vzero$.
\end{description}

\newpage 


\subsection{Zellner-Siow prior}


\cite{Liang2008} also considered
the Zellner-Siow prior structure \citep{Zellner1980} which is
equivalent to using $g\sim \mbox{IG}(1/2,n/2)$ inverse-gamma prior on $g$. 
Unfortunately, this choice of prior does not
lead to closed form expressions for $p(\vy|\vgamma)$ (and hence $\mbox{BF}(\vgamma)$).
\cite{Bayarri2007}
suggest univariate quadrature or Monte Calro integration to approximate $p(\vy|\vgamma)$. \cite{Liang2008} suggest Laplace approximation and show that
this approximation is model selection consistent. 

$$
\Phi_1(a,b,c;x,y) = \frac{\Gamma(c)} {\Gamma(a) \Gamma(c-a)} 
\int_0^1 t^{a-1} (1-t)^{c-a-1} (1-xt)^{-b} e^{yt} \,\mathrm{d}t, 
\quad \bR \,c > \bR \,a > 0 ~.
$$


$$
\begin{array}{rl}
\ds p(\vy|\vgamma)
\ds = K(n) \int \frac{\left(\tfrac{n}{2}\right)^{1/2}}{\Gamma(1/2)}
g^{-3/2}\exp\left( -\frac{n}{2g} \right)
(1 + g)^{(n - p_\vgamma - 1)/2}(1 + g (1 - R_\vgamma^2))^{-(n-1)/2}
dg
\\

\end{array} 
$$

We consider a variational Bayes approach conditional on $\vgamma$. Noting that it is possible
to integrate out $\alpha$, $\vbeta$, and $\sigma^2$ analytically, but not $g$ for this hyperprior
we consider a VB approximation corresponding to the factorization
$q(\alpha,\vbeta_\vgamma,\sigma^2,g) = q(\alpha,\vbeta_\vgamma,\sigma^2)q(g)$.
Then the optimal $q$-density for $q(g)$ is of the form
$$
%\begin{array}{rl}
\ds q(g) 
%& \ds \propto
%\exp\left[   
%- \left( \frac{1+p_\vgamma}{2} + 1 \right) \log(g) 
%- \bE_{-q(g)}\left( \frac{n}{2} + %\frac{\vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma}{2\sigma^2} \right) g^{-1} 
%\right]
%\\
%& \ds 
= f_{\mbox{\scriptsize IG}}\left(  g \,;\, \frac{1+p_\vgamma}{2}, \frac{n}{2} + \frac{Q}{2}  \right)
%\end{array}
$$

\noindent where $f_{\mbox{\scriptsize IG}}( x ; a, b) = b^a x^{-a-1}\exp(-b/x)$
is the inverse-gamma density function with shape $a$ and scale $b$, and 
$Q = \bE_{-q(g)}( \sigma^{-2} \vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma ).$
Next we consider
$$
\begin{array}{rl}
q(\alpha,\vbeta_\vgamma,\sigma^2)
& \ds \propto \exp\left[
\bE_{q(g)}\left\{
\log p(\vy,\alpha,\vbeta_\vgamma,\sigma^2|g)
\right\}
\right]
\\ [2ex]
%& \ds \propto \exp\left[
%- \tfrac{n}{2}\log(2\pi\sigma^2) 
%- \tfrac{1}{2\sigma^2}\|\vy - \vone \alpha- \mX_\vgamma\vbeta_\vgamma\|^2
%- \log(\sigma^2) 
%+ \tfrac{1}{2}\log|\mX_\vgamma^T\mX_\vgamma| 
%- \tfrac{p_\vgamma}{2}\log(2\pi\sigma^2)  
%- \bE_{q(g)} \left\{ 
%\frac{\vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma}{2g\sigma^2}
%\right\}
%\right]
%\\
& \ds \propto \exp\left[
- \left( \tfrac{n+p_\vgamma}{2} + 1\right)\log(\sigma^2) 
- \frac{n\alpha^2}{2\sigma^2} 
- \frac{\|\vy - \mX_\vgamma\vbeta_\vgamma\|^2}{2\sigma^2}
- \frac{\vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma}{2 \widehat{g}\sigma^2}
\right]
\end{array} 
$$

\noindent where $\widehat{g} = \bE_{q(g)}(g^{-1})^{-1}$. Hence, we can identify that
$q(\alpha|\sigma^2) = \phi(\alpha \, ; \, 0,\sigma^2/n)$
where $\phi(x;\mu,\sigma^2)$ is the Gaussian density with mean $\mu$
and variance $\sigma^2$.
Integrating out $\alpha$ from $q(\alpha,\vbeta_\vgamma,\sigma^2)$ we obtain
$$
\begin{array}{rl}
q(\vbeta_\vgamma,\sigma^2)
%& \ds = \int q(\alpha,\vbeta_\vgamma,\sigma^2) d\alpha 
%\\
%& \ds \propto 
%\exp\left[
%- \left( \tfrac{n+p_\vgamma}{2} + 1\right)\log(\sigma^2) 
%+ \tfrac{1}{2}\log(2\pi\sigma^2/n)
%- \frac{\|\vy - \mX_\vgamma\vbeta_\vgamma\|^2}{2\sigma^2}
%- \frac{\vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma}{2 %\widehat{g}\sigma^2}
%\right]
%\\
& \ds \propto 
\exp\left[
- \left( \tfrac{n+p_\vgamma-1}{2} + 1\right)\log(\sigma^2) 
- \frac{n}{2\sigma^2}
+ \frac{\vy^T\mX_\vgamma\vbeta_\vgamma}{\sigma^2}
- \frac{(1 + \widehat{g}^{-1})\vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma}{2\sigma^2}
\right]
\end{array} 
$$

\noindent from which we can identify that
$q(\vbeta_\vgamma|\sigma^2) = \phi( \vbeta_\vgamma \, ; \, \widehat{u} \widehat{\vbeta}_\vgamma,  \widehat{u} \sigma^2 (\mX_\vgamma^T\mX_\vgamma)^{-1})$
where $\widehat{u} = \widehat{g}/(1 + \widehat{g})$.
Integrating out $\vbeta_\vgamma$ from $q(\vbeta_\vgamma,\sigma^2)$
we obtain
%$$
%\begin{array}{rl}
%q(\sigma^2)
%& \ds \propto 
%\exp\left[
%- \left( \tfrac{n-1}{2} + 1\right)\log(\sigma^2) 
%- \tfrac{n}{2}\left( 1 - \widehat{u} R_\vgamma^2\right)\sigma^{-2}
%\right]
%\end{array} 
%$$
%$
%$\noindent which is a
$$
q(\sigma^2) = f_{\mbox{\scriptsize IG}}(\sigma^2 \, ; \, 
\tfrac{n-1}{2},
\tfrac{n}{2}( 1 - \widehat{u} R_\vgamma^2)).
$$

\noindent
Next we use the fact that if $\vx|z \sim N(\vmu,z\mV)$ and $z\sim \mbox{IG}(a,b)$ then
$\vx \sim t_{2a}( \vmu, (b/a)\mV)$. 
Hence,
$$
\begin{array}{rl} 
q(\alpha) 
& \ds = f_t\left[ \alpha \,;\, 0, \tfrac{n}{n-1}  \left( 1 - \widehat{u} R_\vgamma^2\right), n - 1\right],
\\ [1ex]
\ds \mbox{and} \qquad q(\vbeta) 
& \ds = f_t\left[ \vbeta \,;\,
\widehat{u} \widehat{\vbeta},
\tfrac{n}{n-1}  \widehat{u} \left( 1 - \widehat{u} R_\vgamma^2\right) (\mX_\vgamma^T\mX_\vgamma)^{-1}, n - 1
\right],
\end{array}
$$

\noindent where $f_t( \vx; \vmu, \mSigma, \nu)$ denotes the multivariate
$t$ density function with location $\vmu$, scale $\mSigma$, and degrees of 
freedom $\nu$.

Using the above results the quantity $Q$ is given by
$$
\begin{array}{rl} 
\ds Q
& \ds = \ds \bE_{q(\sigma^2)}\left[ \sigma^{-2}
\bE_{q(\vbeta_\vgamma|\sigma^2)}
\left\{ 
\vbeta_\vgamma^T\mX_\vgamma^T\mX_\vgamma\vbeta_\vgamma
\right\}
\right]
\\ [2ex]
& \ds = \ds \bE_{q(\sigma^2)}\left[
\sigma^{-2} \widehat{u}^2 \widehat{\vbeta}_\vgamma^T\mX_\vgamma^T\mX_\vgamma\widehat{\vbeta}_\vgamma
+   \widehat{u}  \mbox{tr}\left(  (\mX_\vgamma^T\mX_\vgamma)^{-1}  \mX_\vgamma^T\mX_\vgamma \right)
\right]
\\ [2ex]
& \ds = \ds \bE_{q(\sigma^2)}\left[ 
\sigma^{-2}\widehat{u}^2 n R_\vgamma^2
+  \widehat{u} p_\vgamma
\right]
\\ [2ex]
& \ds = \ds 
\frac{\widehat{u}^2 (n-1) R_\vgamma^2}{( 1 - \widehat{u} R_\vgamma^2)}
+  \widehat{u} p_\vgamma
\end{array}
$$

\noindent using $\vy^T\mX_\vgamma(\mX_\vgamma^T\mX_\vgamma)^{-1}\mX_\vgamma^T\vy  = n R_\gamma^2$.

Next
$$
\widehat{g} = \frac{n + Q}{1 + p_\vgamma}
$$

\noindent If $\widehat{u} = \widehat{g}/(1+\widehat{g})$ then
$\widehat{g} =  \widehat{u}/(1 - \widehat{u})$. Hence, we need to solve
$$
\frac{\widehat{u}}{1 - \widehat{u}} = \frac{n + \frac{\widehat{u}^2 (n-1) R_\vgamma^2}{( 1 - \widehat{u} R_\vgamma^2)}
	+  \widehat{u} p_\vgamma}{1 + p_\vgamma}
$$

\noindent for $\widehat{u}$.  
$$
\widehat{u}(1 + p_\vgamma)(1 - \widehat{u} R_\vgamma^2)
= n(1 - \widehat{u}) 
+ \widehat{u}^2 (n-1) R_\vgamma^2 
+  \widehat{u} p_\vgamma ( 1 - \widehat{u} R_\vgamma^2)
$$

\noindent Only keeping $O(n)$ terms
$$
0
= n(1 - \widehat{u}) 
+ \widehat{u}^2 (n-1) R_\vgamma^2 
$$

$$
(n-1) R_\vgamma^2 \widehat{u}^2 - n\widehat{u} + n = 0
$$

$$
\widehat{u} = \frac{n \pm n\sqrt{1 - 4 \times (n-1) R_\vgamma^2 /n}}{2(n-1) R_\vgamma^2}
$$




\section{$g/n$ Working}

Note that
$$
F_1(a,b_1,b_2,c; x,y) = \sum_{m=0}^\infty \frac{(a)_m(b)_m}{(c_m)}\frac{x^m}{m!} {}_2 F_1(a+m,b';c+m;y)
$$


However, when $x\approx 1$ we have
$$
\begin{array}{rl} 
F_1(a,b_1,b_2,c; 1,y) 
& \ds = \frac{\Gamma(c)} {\Gamma(a)\Gamma(c-a)} 
\int_0^1 t^{a-1} (1-t)^{c-b_1-a-1} (1-yt)^{-b_2} \, dt
\\ [2ex]
& \ds 
= \frac{\Gamma(c)} {\Gamma(c-a)} 
\frac{\Gamma(c - b_1 - a)}{\Gamma(c - b_1)}
\frac{}{} 
{}_2  F_1(b_2, a, c - b_1, y)
\end{array} 
$$

\noindent using the identity
$\Beta(b,c-b) \,_2F_1(a,b;c;z) = \int_0^1 t^{b-1} (1-t)^{c-b-1}(1-zt)^{-a} \, dt$ and properties
of the Beta function. This suggests the approximation
$$
\begin{array}{rl} 
\ds \mbox{BF}_{g/n}(\vgamma) 
& \ds \approx  
\frac{(a - 2)}{n(p_\vgamma + a - 2)} 
\frac{\Gamma\left( \tfrac{p_\vgamma + a}{2} \right)} {\Gamma\left( \tfrac{p_\vgamma + a}{2} - 1 \right)} 
\frac{\Gamma\left( \tfrac{p_\vgamma}{2}  - 1 \right)}{\Gamma\left(\frac{p_\vgamma }{2} \right)}
{}_2  F_1\left( \frac{n-1}{2}, 1, \frac{p_\vgamma }{2}, R_\vgamma^2 \right)
\\
& \ds  = \frac{(a - 2)}{n(p_\vgamma - 2)} 
{}_2  F_1\left( \frac{n-1}{2}, 1, \frac{p_\vgamma }{2}, R_\vgamma^2 \right)
\end{array} 
$$

\noindent We can then use
the identity (\ref{eq:logGuassHypergeometric}) 
to evaluate the above approximation
on the log scale, avoiding numerical overflow.


$$
\begin{array}{rl}
p_{g/n}(\vy|\vgamma) 
& \ds = K(n) \frac{a - 2}{2n}  \int_0^\infty 
\exp\left( -\tfrac{a}{2} \log\left(1 + \frac{g}{n}\right) \right)
(1 + g)^{(n-p_\vgamma-1)/2} \left[ 1 + g (1 - R_\vgamma^2) \right]^{-(n-1)/2}  dg
\\
& \ds \approx K(n) \frac{a - 2}{2n}  \int_0^\infty 
\exp\left( -\tfrac{a}{2} \log\left(1 + \frac{\widehat{g}}{n}\right) - \frac{a(g - \widehat{g})}{2(n + \widehat{g})} \right)
(1 + g)^{(n-p_\vgamma-1)/2} \left[ 1 + g (1 - R_\vgamma^2) \right]^{-(n-1)/2}  dg
\\
& \ds = K(n) \frac{a - 2}{2n} \left( 1 + \frac{\widehat{g}}{n} \right)^{-a/2} \exp\left( \frac{a\widehat{g}}{2(n + \widehat{g})} \right) \\
& \ds \qquad \times \int_0^\infty 
\exp\left(  - \frac{ag}{2(n + \widehat{g})} \right)
(1 + g)^{(n-p_\vgamma-1)/2} \left[ 1 + g (1 - R_\vgamma^2) \right]^{-(n-1)/2}  dg
\\
& \ds = K(n) \frac{a - 2}{2n}  \int_0^1 \exp\left[
- \tfrac{a}{2}\log\left(  1 - u \left(1  -  \tfrac{1}{n} \right) \right)
\right]
(1 - u)^{p/2 + a/2 - 2  }   \left(  1 - u R^2\right)^{-(n-1)/2} du
\\
& \ds \approx K(n) \frac{a - 2}{2n}  \int_0^1 \exp\left[
- \tfrac{a}{2}\log\left(  1 - \widehat{u} \left(1  -  \tfrac{1}{n} \right)  \right) + \tfrac{a}{2} \frac{\left(1  -  \tfrac{1}{n} \right)(u - \widehat{u})}{1 - \widehat{u} \left(1  -  \tfrac{1}{n} \right)}
\right]
(1 - u)^{p/2 + a/2 - 2  }   \left(  1 - u R^2\right)^{-(n-1)/2} du
\\
& \ds = K(n) \frac{a - 2}{2n} \left(  1 - \widehat{u} \left(1  -  \tfrac{1}{n} \right)  \right)^{-a/2} 
\exp\left[
-\frac{a}{2} \frac{\left(1  -  \tfrac{1}{n} \right)\widehat{u}}{1 - \widehat{u} \left(1  -  \tfrac{1}{n} \right)}
\right]\\
& \ds \qquad \times \int_0^1 \exp\left[
\frac{a}{2} \frac{\left(1  -  \tfrac{1}{n} \right)u}{1 - \widehat{u} \left(1  -  \tfrac{1}{n} \right)}
\right]
(1 - u)^{p/2 + a/2 - 2  }   \left(  1 - u R^2\right)^{-(n-1)/2} du
\end{array} 
$$

\begin{table}[h]
	\begin{center}
		\begin{tabular}{r|l|l|c|r|r}
			rank & $\vgamma$ (base: 1, 2, 6, 15, 16) & $p(\vgamma|\vy)$ & $R^2$ & $\widetilde{\text{BIC}}_\text{Best} - \widetilde{\text{BIC}}$ & $\text{BIC}_\text{Best} - \text{BIC}$ \\
			\hline
			1  &  +10, +11, +13&  2.56\%&  0.53&  0.00&  0.00\\
			2  &  +11, +12, +13&  2.29\%&  0.53&  0.22&  0.23\\
			3  &  +8, +11, +12, +13&  1.60\%&  0.53&  0.94&  2.17\\
			4  &  +8, +9, +10&  1.23\%&  0.53&  1.47&  1.53\\
			5  &  +10, +11, +13, +17&  1.10\%&  0.53&  1.69&  2.95\\
			6  &  +12&  1.10\%&  0.51&  1.70&  $-$0.30\\
			7  &  +7, +11, +12, +13,&  1.07\%&  0.53&  1.75&  3.02\\
			8  &  +8, +11, +12, +13, +17&  1.03\%&  0.54&  1.83&  4.40\\
			9  &  +9, +11, +12, +13&  0.67\%&  0.53&  2.68&  3.99\\
			10 &  +8, +9, +12&  0.64\%&  0.52&  2.78&  2.89\\
		\end{tabular}
		
	\end{center}
	
	\caption{Top 10 models selected for the Hitters data set ranked by posterior model probability, the difference
		in log probabilities $\log p(\vy|\vgamma)$ between the best model and this model and the difference
		between the Bayesian Information Criteria for this model and the best model}
	\label{tab:numerical_results_hitters}
\end{table}
\begin{table}[h]
	\begin{center}
		\begin{tabular}{r|l|r|c|r|r}
			rank & $\vgamma$ (base: 6, 13) & $p(\vgamma|\vy)$ & $R^2$ & $\widetilde{\text{BIC}}_\text{Best} - \widetilde{\text{BIC}}$ & $\text{BIC}_\text{Best} - \text{BIC}$ \\
			\hline
			1  &  +2&  17.64\%&  0.73&  0.00&  0.00\\
			2 &  +3&  5.85\%&  0.73&  2.21&  2.25\\
			3 &  +2, +11&  5.74\%&  0.74&  2.25&  1.98\\
			4 &  +2, +12&  3.56\%&  0.74&  3.20&  2.95\\
			5 &  +1, +3&  2.83\%&  0.74&  3.66&  3.42\\
			6 &  +1&  2.69\%&  0.73&  3.76&  3.84\\
			7 &  +2, +3&  2.07\%&  0.74&  4.28&  4.06\\
			8 &  +3, +5&  2.03\%&  0.74&  4.33&  4.11\\
			9 &  +2, +8&  1.96\%&  0.74&  4.40&  4.18\\
			10 &  +1, +2&  1.81\%&  0.74&  4.55&  4.33\\
		\end{tabular}
	\end{center}
	\label{tab:numerical_results_bodyfat}
	\caption{Top 10 models selected for the Body fat data set ranked by posterior model probability, the difference
		in log probabilities $\log p(\vy|\vgamma)$ between the best model and this model and the difference
		between the Bayesian Information Criteria for this model and the best model}
\end{table}

\begin{table}[h]
	
	\begin{center}
		\begin{tabular}{r|l|r|c|r|r}
			rank & $\vgamma$ (base: 13, 16, 17) & $p(\vgamma|\vy)$ & $R^2$ & $\widetilde{\text{BIC}}_\text{Best} - \widetilde{\text{BIC}}$ & $\text{BIC}_\text{Best} - \text{BIC}$ \\
			\hline
			1 &  &  67.57\%&  0.91&  0.00&  0.00\\
			2 &  +11&  5.57\%&  0.91&  4.99&  3.70\\
			3 &  +12&  3.99\%&  0.91&  5.66&  4.37\\
			4 &  +3&  3.88\%&  0.91&  5.71&  4.43\\
			5 &  +2&  2.86\%&  0.91&  6.32&  5.04\\
			6 &  +1&  2.18\%&  0.91&  6.86&  5.58\\
			7 &  +14&  1.56\%&  0.91&  7.54&  6.25\\
			8 &  +10&  0.92\%&  0.91&  8.59&  7.31\\
			9 &  +4&  0.91\%&  0.91&  8.61&  7.33\\
			10 &  +6&  0.85\%&  0.91&  8.75&  7.47\\
		\end{tabular}
	\end{center}
	\label{tab:numerical_results_wage}
	\caption{Top 10 models selected for the Wage data set ranked by posterior model probability, the difference
		in log probabilities $\log p(\vy|\vgamma)$ between the best model and this model and the difference
		between the Bayesian Information Criteria for this model and the best model}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{r|l|r|c|r|r}
			rank & $\vgamma$ (base: 2, 8, 9, 10, 16, 17) & $p(\vgamma|\vy)$ & $R^2$ & $\widetilde{\text{BIC}}_\text{Best} - \widetilde{\text{BIC}}$ & $\text{BIC}_\text{Best} - \text{BIC}$ \\
			\hline
			1 &  +6, +12&  7.62\%&  0.45&  0.00&  0.00\\
			2 &  +1, +6, +12&  6.80\%&  0.46&  0.23&  1.65\\
			3 &  +6&  4.88\%&  0.45&  0.89&  $-$0.40\\
			4 &  +1, +6&  4.46\%&  0.45&  1.07&  1.08\\
			5 &  +6, +7&  2.43\%&  0.45&  2.28&  2.31\\
			6 &  +6, +7, +12&  2.37\%&  0.45&  2.33&  3.79\\
			7 &  +5, +12&  2.29\%&  0.45&  2.41&  2.43\\
			8 &  +3, +6, +12&  1.56\%&  0.45&  3.17&  4.63\\
			9 &  +5, +6, +12&  1.52\%&  0.45&  3.22&  4.69\\
			10 &  +5&  1.32\%&  0.45&  3.50&  2.24\\
		\end{tabular}
	\end{center}
	\label{tab:numerical_results_gradrate}
	\caption{Top 10 models selected for the Graduation Rate data set ranked by posterior model probability, the
		difference in log probabilities $\log p(\vy|\vgamma)$ between the best model and this model and the difference between the Bayesian Information Criteria for this model and the best model}
\end{table}

\begin{table}
	\label{tab:numerical_results_uscrime}
	\caption{Top 10 models selected for the US Crime data set ranked by posterior model probability, the
		difference in log probabilities $\log p(\vy|\vgamma)$ between the best model and this model and the difference between the Bayesian Information Criteria for this model and the best model}
	\begin{center}
		\begin{tabular}{r|l|r|l|r|r}
			rank & $\vgamma$ (base: 1, 3, 4, 13)& $p(\vgamma|\vy)$ & $R^2$ & $\widetilde{\text{BIC}}_\text{Best} - \widetilde{\text{BIC}}$ & $\text{BIC}_\text{Best} - \text{BIC}$ \\
			\hline
			1 &  +11, +14&  2.48\%&  0.77&  0.00&  0.00\\
			2 & +14&  1.47\%&  0.74&  1.05&  1.45\\
			3 & +11, +12, +14&  0.99\%&  0.77&  1.85&  2.07\\
			4 & +10, +11, +14&  0.93\%&  0.77&  1.97&  2.22\\
			5 &  +8, +11, +14&  0.89\%&  0.77&  2.06&  2.33\\
			6 &  +11&  0.78\%&  0.73&  2.31&  2.91\\
			7 &  +12, +14&  0.71\%&  0.75&  2.51&  2.99\\
			8 &  +7, +14&  0.69\%&  0.75&  2.56&  3.04\\
			9 &  +7, +11, +14&  0.69\%&  0.77&  2.56&  2.94\\
			10 &  +7, +10, +11, +14&  0.69\%&  0.79&  2.56&  2.85\\
		\end{tabular}
	\end{center}
\end{table}
