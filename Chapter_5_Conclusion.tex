%! TEX root = thesis.tex
\chapter{Future Directions}
\label{sec:chapter_2_conclusion}
		
% TODO: This section recounts what we've done. I'm not sure that it
% communicates what we could do in the future very well.

We conclude by briefly summarising the content of the thesis and outline potential research directions we seek to pursue.


\section{Calculating Bayes factors for $g$-priors}

In Chapter 2 we reviewed the prior structures that lead to closed form expressions for
Bayes factors for linear models. We have described ways that each of these
priors, except for the hyper-$g/n$ prior can be evaluated in a numerically stable
manner and have implemented a package \texttt{blma} for performing full exact
Bayesian model averaging using this methodology. Our package is competitive
with \texttt{BAS} and \texttt{BMS} in terms of computational speed, is
numerically more stable and accurate, and offers some different priors
structures not offered in \texttt{BAS}. Our package is much faster than
\texttt{BayesVarSelect} and is also numerically more stable and accurate.

We are currently working on several extensions to this work. Firstly, we are
working on a parallel implementation of the package which will allow for exact
Bayesian inference for problems roughly the size $p\approx 30$. While a
prototype of this work has been developed which runs on Linux using
\texttt{OpenMP}, and displayed good performance as the number of cores used
increased, we found it difficult to get the parallel code to work reliably on
Macintosh and Windows computers.  As the majority of R users use these two
platforms, we feel this is an important technical issue to resolve. Our
algorithm could also be ported to GPUs.

Secondly, we are currently implementing  Markov Chain Monte Carlo (MCMC) and
population based MCMC methods for exploring the model space when $p>30$.  We
can also see several obvious paths to extend this work to Generalised Linear
Models - either using the approach described in \cite{Li2015} or by using
Laplace approximations. 

Thirdly, we are working on fast numerically stable quadrature based methods for
the hyper-$g/n$ and Zellner-Siow based priors. Further we are deriving exact 
expressions for parameter posterior distributions under some of the prior 
structures we have considered here. Many of these parameter posterior distributions
are expressed in terms of special functions whose numerical evaluation must
be handled with care.

% TODO: I still feel that you're missing something here. Look at Statistical
% Learning with Sparsity. John calls what we've already done the `building
% block' for a whole host of other approaches. To write this section, you need
% to read more of that book, understand what John's comment meant and then
% write out what you've understood.

\section{Particle Variational Approximation}

In Chapter 3 we developed PVA, a fast method for approximate Bayesian model averaging. There are several planned future extensions to this work. Firstly, we would
like to generalise the PVA approach to generalised linear models as well as
linear models, to be able to perform model selection for regression models
applicable to a wider range of types of data. The computational approach would
again either calculate Bayes factors based on the ideas of \cite{Li2015} or by using Laplace
approximations. Additional care needs to be exercised for these models as the likelihood can often become irregular for a
significant portion of models in the process of model selection.

Secondly, although the algorithm already runs in parallel on multicore CPUs
using \texttt{OpenMP}, we believe even greater gains in performance could be
achieved by porting the algorithm to run on GPUs, or by using distributed
computing such as \texttt{OpenMPI}.

Thirdly, and most excitingly, we could examine modifications to the PVA
algorithm itself. The way the algorithm currently ensures diversity amongst the
particles in the population is to reward increases in entropy, weighted by the
hyperparameter $\lambda$. The current version of the algorithm hard codes
$\lambda$ to $1$, but it would be interesting to alter $\lambda$ and as in the
Population EM algorithm of \cite{Rockova2017} and observe the effect on model
selection performance. The algorithm also currently maintains diversity in the
population by maintaining uniqueness of every particle within the population.
It would be interesting to relax this constraint and compare the effect on
model performance.



\section{Zero-inflated models via Gaussian Variational Approximation }


This thesis chapter presents the essential ideas necessary for a high performance
implementation for model fitting of ZIP regression models.
%, but the performance would be even better if our algorithm was re-implemented
%in a compiled language with good numeric libraries such as C++ with Eigen.
The majority of the performance improvements over existing approaches come from
avoiding unnecessary matrix inversion, which is a computationally expensive
and numerically unstable process taking $\BigO(p^3)$ flops, and  from
constructing and calculating with sparse matrices. The gains of these
approaches, particularly from sparse  matrix techniques, can be difficult to
fully realise in R without expert knowledge of the underlying implementation
and libraries.

Our application of these ideas to Andrew Gelman's data showed that the new
parameterisation very effectively speeds up fitting zero-inflated mixed models
to real world data with a large number of groups, while still maintaining
excellent accuracy versus an MCMC approach. This demonstrates the applicability
of the ideas presented within this chapter to real world data sets.


The first directions for future research stemming from this chapter would be
generalising the approximation to other zero-inflated models which handle
overdispersion in the data without the need for a random intercept, such as the
zero-inflated negative binomial model.

Furthermore, much more exploration could be done on alternative
parameterisations of the covariance matrix in the Gaussian Variational
Approximation (GVA). The specific parameterisation of the diagonal of the Cholesky
factor as a piecewise exponential/quadratic polynomial function was chosen
largely for convenience.

The current mean field update and GVA algorithms
use the entire sample. For large samples in the Big Data era, this may not be
computationally feasible. Other authors such as \cite{Tan2018} have used doubly
stochastic algorithms which both sub-sample the data and use noise to
approximate the integral expression for the expectation of the variational
lower bound. The sub-sampling in particular is very appealing in a Big Data
context. We wish to experiment with this class of algorithm, and compare the
performance and accuracy of this kind of doubly stochastic algorithm with the
more traditional mean field and GVA algorithms presented in
Chapter 4.
