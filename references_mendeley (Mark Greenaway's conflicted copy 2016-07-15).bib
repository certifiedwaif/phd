@article{Breiman1996,
abstract = {In model selection, usually a "best" predictor is chosen from a collection {\{}$\mu$̂({\textperiodcentered}, s){\}} of predictors where $\mu$̂({\textperiodcentered}, s) is the minimum least-squares predictor in a collection Us of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in Us. If L is the data used to derive the sequence {\{}$\mu$̂({\textperiodcentered}, s){\}}, the procedure is called unstable if a small change in L can cause large changes in {\{}$\mu$̂({\textperiodcentered}, s){\}}. With a crystal ball, one could pick the predictor in {\{}$\mu$̂({\textperiodcentered}, s){\}} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\{}$\mu$̂'({\textperiodcentered}, s){\}} and then averaging over many such predictor sequences.},
author = {Breiman, Leo},
doi = {10.1214/aos/1032181158},
file = {:home/markg/Dropbox/Downloads/euclid.aos.1032181158.pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {,Regression,cross-validation,prediction error,predictive loss,subset selection},
number = {6},
pages = {2350--2383},
title = {{Heuristics of instability in model selection}},
url = {http://projecteuclid.org/euclid.aos/1032181158},
volume = {24},
year = {1996}
}
@article{Piironen2016,
abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. Better and much less varying results are obtained by incorporating all the uncertainties into a full encompassing model and projecting this information onto the submodels. The reference model projection appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08650v1},
author = {Piironen, Juho and Vehtari, Aki},
doi = {10.1007/s11222-016-9649-y},
eprint = {arXiv:1503.08650v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piironen, Vehtari - 2016 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian model selection,Cross-validation,Projection,Reference model,Selection bias},
pages = {1--25},
title = {{Comparison of Bayesian predictive methods for model selection}},
year = {2016}
}
@article{Breiman1996a,
abstract = {In model selection, usually a "best" predictor is chosen from a collection {\{}$\mu$̂({\textperiodcentered}, s){\}} of predictors where $\mu$̂({\textperiodcentered}, s) is the minimum least-squares predictor in a collection Us of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in Us. If L is the data used to derive the sequence {\{}$\mu$̂({\textperiodcentered}, s){\}}, the procedure is called unstable if a small change in L can cause large changes in {\{}$\mu$̂({\textperiodcentered}, s){\}}. With a crystal ball, one could pick the predictor in {\{}$\mu$̂({\textperiodcentered}, s){\}} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\{}$\mu$̂'({\textperiodcentered}, s){\}} and then averaging over many such predictor sequences.},
author = {Breiman, Leo},
doi = {10.1214/aos/1032181158},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 1996 - Heuristics of instability in model selection(2).pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Regression,cross-validation,prediction error,predictive loss,subset selection},
number = {6},
pages = {2350--2383},
title = {{Heuristics of instability in model selection}},
url = {http://projecteuclid.org/euclid.aos/1032181158},
volume = {24},
year = {1996}
}
@article{Bhadra2016,
abstract = {Predictive performance in shrinkage regression suffers from two major difficulties: (i) the amount of relative shrinkage is monotone in the singular values of the design matrix and (ii) the amount of shrinkage does not depend on the response variables. Both of these factors can translate to a poor prediction performance, the risk of which can be explicitly quantified using Stein's unbiased risk estimate. We show that using a component-specific local shrinkage term that can be learned from the data under a suitable heavy-tailed prior, in combination with a global term providing shrinkage towards zero, can alleviate both these difficulties and consequently, can result in an improved risk for prediction. Demonstration of improved prediction performance over competing approaches in a simulation study and in a pharmacogenomics data set confirms the theoretical findings.},
archivePrefix = {arXiv},
arxivId = {1605.04796},
author = {Bhadra, Anindya and Datta, Jyotishka and Li, Yunfan and Polson, Nicholas G. and Willard, Brandon},
eprint = {1605.04796},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhadra et al. - 2016 - Prediction risk for global-local shrinkage regression.pdf:pdf},
number = {May},
title = {{Prediction risk for global-local shrinkage regression}},
url = {http://arxiv.org/abs/1605.04796},
year = {2016}
}
@article{George2000,
abstract = {For the problem of variable selection for the normal linear model, selection criteria such as AIC, Cp, BIC and RIC have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed-penalty criterion across a variety of orthogonal and nonorthogonal set-ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed.},
author = {George, Edward I and Foster, Dean P},
doi = {10.1093/biomet/87.4.731},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Foster - 2000 - Calibration and Empirical Bayes Variable Selection.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {731--747},
title = {{Calibration and Empirical Bayes Variable Selection}},
url = {papers2://publication/doi/10.2307/2673607?ref=no-x-route:d5bfd32a245ff2965e7b7fbdd5b4331b},
volume = {87},
year = {2000}
}
@article{Breiman1996b,
abstract = {In model selection, usually a "best" predictor is chosen from a collection {\{}$\mu$̂({\textperiodcentered}, s){\}} of predictors where $\mu$̂({\textperiodcentered}, s) is the minimum least-squares predictor in a collection Us of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in Us. If L is the data used to derive the sequence {\{}$\mu$̂({\textperiodcentered}, s){\}}, the procedure is called unstable if a small change in L can cause large changes in {\{}$\mu$̂({\textperiodcentered}, s){\}}. With a crystal ball, one could pick the predictor in {\{}$\mu$̂({\textperiodcentered}, s){\}} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\{}$\mu$̂'({\textperiodcentered}, s){\}} and then averaging over many such predictor sequences.},
author = {Breiman, Leo},
doi = {10.1214/aos/1032181158},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 1996 - Heuristics of instability in model selection(2).pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Regression,cross-validation,prediction error,predictive loss,subset selection},
number = {6},
pages = {2350--2383},
title = {{Heuristics of instability in model selection}},
url = {http://projecteuclid.org/euclid.aos/1032181158},
volume = {24},
year = {1996}
}
@article{Meinshausen2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0809.2932v2},
author = {Meinshausen, Nicolai},
eprint = {arXiv:0809.2932v2},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meinshausen - 2009 - Stability selection (Slides).pdf:pdf},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
pages = {1--30},
title = {{Stability selection (Slides)}},
year = {2009}
}
@article{DeLeeuw1992,
abstract = {The problem of estimating the dimensionality of a model occurs in various forms in applied statistics: estimating the number of factors in factor analysis, estimating the degree of a polynomial describing the data, selecting the variables to be introduced in a multiple regression equation, estimating the order of an AR or MA time series model, and so on.},
author = {DeLeeuw, J.},
doi = {10.1016/0049-3848(86)90167-2},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeLeeuw - 1992 - Introduction to Akaike (1973) Information Theory and an Extension of the Maximum Likelihood Principle.pdf:pdf},
isbn = {0172-7397},
issn = {00493848},
journal = {Breakthroughs in Statistics Volume I: Foundations and Basic Theory},
number = {May},
pages = {599--609},
pmid = {217},
title = {{Introduction to Akaike (1973) Information Theory and an Extension of the Maximum Likelihood Principle}},
year = {1992}
}
@article{Summation1993,
author = {Summation, Point},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Summation - 1993 - The accuracy.pdf:pdf},
keywords = {1,ams subject classifications,and all,floating point summation,inner products,introduction,means,norms,numbers are ubiquitous in,orderings,primary 65g05,puting,rounding error analysis,scientific com-,secondary 65b10,sums of floating point,they occur when evaluating,variances},
number = {4},
pages = {783--799},
title = {{The accuracy}},
volume = {14},
year = {1993}
}
@article{Miller1984,
abstract = {Computational algorithms for selecting subsets of regression variables are discussed. Only linear models and the least-squares criterion are considered. The use of planar- rotation algorithms, instead of Gauss-Jordan methods, is advocated. The advantages and disadvantages of a number of "cheap" search methods are described for use when it is not feasible to carry out an exhaustive search for the best-fitting subsets. Hypothesis testing for three purposes is considered, namely (i) testing for zero regression coefficients for remaining variables, (ii) comparing subsets and (iii) testing for any predictive value in a selected subset. Three small data sets are used to illustrate these tests. Spjotvoll's (1972a) test is discussed in detail, though an extension to this test appears desirable. Estimation problems have largely been overlooked in the past. Three types of bias are identified, namely that due to the omission of variables, that due to competition for selection and that due to the stopping rule. The emphasis here is on competition bias, which can be of the order of two or more standard errors when coefficients are estimated from the same data as were used to select the subset. Five possible ways of handling this bias are listed. This is the area most urgently requiring further research. Mean squared errors of prediction and stopping rules are briefly discussed. Com- petition bias invalidates the use of existing stopping rules as they are commonly applied to try to produce optimal prediction equations},
author = {Miller, By Alan J},
doi = {10.2307/2981576},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1984 - Selection of Subsets of Regression Variables(2).pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
keywords = {CONDITIONAL LIKELIHOOD,CRITERIA,LEAST SQUARES,MALLOWS' C AKAIKE'S INFORMATION,MEAN SQUARED ERRORS OF PREDICTION,MULTIPLE REGRESSION,PREDICTION,STEPWISE REGRESSION,SUBSET SELECTION,VARIABLE SELECTION},
number = {3},
pages = {389--425},
title = {{Selection of Subsets of Regression Variables}},
volume = {147},
year = {1984}
}
@article{??zaltin2011,
abstract = {The most widely used progress measure for branch-and-bound (B{\&}B) algorithms when solving mixed-integer programs (MIPs) is the MIP gap. We introduce a new progress measure that is often much smoother than the MIP gap. We propose a double exponential smoothing technique to predict the solution time of B{\&}B algorithms and evaluate the prediction method using three MIP solvers. Our computational experiments show that accurate predictions of the solution time are possible, even in the early stages of B{\&}B algorithms.},
author = {??zaltin, Osman Y. and Hunsaker, Brady and Schaefer, Andrew J.},
doi = {10.1287/ijoc.1100.0405},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/zaltin, Hunsaker, Schaefer - 2011 - Predicting the solution time of branch-and-bound algorithms for mixed-integer programs.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {INFORMS Journal on Computing},
keywords = {Branch-and-bound algorithm,Mixed-integer programming,Solution time prediction},
number = {3},
pages = {392--403},
title = {{Predicting the solution time of branch-and-bound algorithms for mixed-integer programs}},
volume = {23},
year = {2011}
}
@article{Bertsimas2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.03133v1},
author = {Bertsimas, Dimitris and King, Angela},
doi = {10.1214/15-AOS1388},
eprint = {arXiv:1507.03133v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsimas, King - 2014 - Best Subset Selection via a Modern Optimization Lens.pdf:pdf},
isbn = {0001415123},
pages = {1--63},
title = {{Best Subset Selection via a Modern Optimization Lens}},
year = {2014}
}
@article{Pearson2014,
abstract = {The two most commonly used hypergeometric functions are the confluent hypergeometric function and the Gauss hypergeometric function. We review the available techniques for accurate, fast, and reliable computation of these two hypergeometric functions in different parameter and variable regimes. The methods that we investigate include Taylor and asymptotic series computations, Gauss-Jacobi quadrature, numerical solution of differential equations, recurrence relations, and others. We discuss the results of numerical experiments used to determine the best methods, in practice, for each parameter and variable regime considered. We provide 'roadmaps' with our recommendation for which methods should be used in each situation.},
archivePrefix = {arXiv},
arxivId = {1407.7786},
author = {Pearson, John W. and Olver, Sheehan and Porter, Mason a.},
eprint = {1407.7786},
file = {:home/markg/Downloads/1407.7786v2.pdf:pdf},
keywords = {1,33c05,33c15,41a58,41a60,ams subject classifications,computation of special functions,confluent hypergeometric function,gauss hy-,introduction,methods for computing the,paper is to review,pergeometric function,primary,secondary,the aim of this},
pages = {41},
title = {{Numerical Methods for the Computation of the Confluent and Gauss Hypergeometric Functions}},
url = {http://arxiv.org/abs/1407.7786},
year = {2014}
}
@article{Masada2009,
abstract = {In this paper, we propose an acceleration of collapsed variational Bayesian (CVB) inference for latent Dirichlet allocation (LDA) by using Nvidia CUDA compatible devices. While LDA is an efficient Bayesian multi-topic document model, it requires complicated computations for parameter estimation in comparison with other simpler document models, e.g. probabilistic latent semantic indexing, etc. Therefore, we accelerate CVB inference, an efficient deterministic inference method for LDA, with Nvidia CUDA. In the evaluation experiments, we used a set of 50,000 documents and a set of 10,000 images. We could obtain inference results comparable to sequential CVB inference.},
author = {Masada, Tomonari and Hamada, Tsuyoshi and Shibata, Yuichiro and Oguri, Kiyoshi},
doi = {10.1007/978-3-642-02568-6_50},
file = {:home/markg/Downloads/CVM{\_}CUDA.pdf:pdf},
isbn = {3642025676},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {491--500},
title = {{Accelerating collapsed variational bayesian inference for latent dirichlet allocation with nvidia CUDA compatible devices}},
volume = {5579 LNAI},
year = {2009}
}
@article{Teh2006,
abstract = {Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational Bayesian inference for LDA.},
author = {Teh, Yee Whye and Newman, David and Welling, Max},
file = {:home/markg/Downloads/4535.pdf:pdf},
journal = {Neural Information Processing Systems},
pages = {1353----1360},
title = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
year = {2006}
}
@article{Million2007,
author = {Million, Elizabeth and Million, Elizabeth},
file = {:home/markg/Downloads/million-paper.pdf:pdf},
pages = {1--7},
title = {{The Hadamard Product}},
year = {2007}
}
@article{Wilson2010,
author = {Wilson, Melanie A},
file = {:home/markg/Downloads/melaniew.pdf:pdf},
journal = {Analysis},
keywords = {GWAS},
pages = {1--99},
title = {{Bayesian model uncertainty and prior choice with applications to genetic association studies}},
year = {2010}
}
@misc{Mainland2013,
abstract = {Stream fusion [6] is a powerful technique for automatically trans- forming high-level sequence-processing functions into efficient im- plementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all. In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together mul- tiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are im- plemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C. Categories},
author = {Mainland, Geoffrey and Leshchinskiy, Roman and {Peyton Jones}, Simon},
booktitle = {the 18th ACM SIGPLAN international conference},
doi = {10.1145/2500365.2500601},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mainland, Leshchinskiy, Peyton Jones - 2013 - Exploiting Vector Instructions with Generalized Stream Fusion.pdf:pdf},
isbn = {9781450323260},
issn = {15232867},
title = {{Exploiting Vector Instructions with Generalized Stream Fusion}},
year = {2013}
}
@article{Clyde2012,
abstract = {Monte Carlo algorithms are commonly used to identify a set of models for Bayesian model selection or model averaging. Because empirical frequencies of models are often zero or one in high-dimensional problems, posterior probabilities calculated from the observed marginal likelihoods, renormalized over the sampled models, are often employed. Such estimates are the only recourse in several newer stochastic search algorithms. In this paper, we prove that renormalization of posterior probabilities over the set of sampled models generally leads to bias that may dominate mean squared error. Viewing the model space as a finite population, we propose a new estimator based on a ratio of Horvitz–Thompson estimators that incorporates observed marginal likelihoods, but is approximately unbiased. This is shown to lead to a reduction in mean squared error compared to the empirical or renormalized estimators, with little increase in computational cost.},
author = {Clyde, Merlise A. and Ghosh, Joyee},
doi = {10.1093/biomet/ass040},
file = {:home/markg/Downloads/10-11.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Bayesian model averaging,Horvitz-Thompson estimator,Inclusion probability,Markov chain Monte Carlo,Median probability model,Model uncertainty,Variable selection},
number = {4},
pages = {981--988},
title = {{Finite population estimators in stochastic search variable selection}},
volume = {99},
year = {2012}
}
@article{Wand2012,
abstract = {The ag{\'{e}}d number theoretic concept of continued fractions can enhance certain Bayesian computations. The crux of this claim is due to continued fraction representations of numerically challenging special function ratios that arise in Bayesian computing. Continued fraction approximation via Lentz's Algorithm often leads to efficient and stable computation of such quantities. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
author = {Wand, Matt P. and Ormerod, John T.},
doi = {10.1002/sta4.4},
file = {:home/markg/Downloads/Wand{\_}et{\_}al-2012-Stat.pdf:pdf},
issn = {20491573},
journal = {Stat},
keywords = {Hypergeometric functions,Mean field variational Bayes,Parabolic cylinder functions,Special functions,Variable selection,Variational approximations},
number = {1},
pages = {31--41},
title = {{Continued fraction enhancement of Bayesian computing}},
volume = {1},
year = {2012}
}
@article{Nadarajah2015,
author = {Nadarajah, Saralees},
doi = {10.1080/00031305.2015.1028595},
file = {:home/markg/Downloads/on{\_}the{\_}computation{\_}of{\_}gauss{\_}hypergeometric{\_}functions.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {2},
pages = {146--148},
title = {{On the Computation of Gauss Hypergeometric Functions}},
url = {http://www.tandfonline.com/doi/full/10.1080/00031305.2015.1028595},
volume = {69},
year = {2015}
}
@article{Li2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06913v1},
author = {Li, Yingbo and Clyde, Merlise A},
eprint = {arXiv:1503.06913v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Clyde - 2015 - Mixtures of g -priors in Generalized Linear Models.pdf:pdf},
journal = {arXiv},
title = {{Mixtures of g -priors in Generalized Linear Models}},
volume = {1503.06913},
year = {2015}
}
@article{Bleier2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0412v1},
author = {Bleier, Arnim},
eprint = {arXiv:1312.0412v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bleier - 2013 - Practical Collapsed Stochastic Variational Inference for the HDP.pdf:pdf},
journal = {Proceedings of the NIPS workshop on topic models},
pages = {arXiv:1312.0412 [cs.LG]},
title = {{Practical Collapsed Stochastic Variational Inference for the HDP}},
url = {http://arxiv.org/abs/1312.0412},
year = {2013}
}
@article{Liang2008,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, F and Paulo, R and Molina, G and Clyde, M a and Berger, J O},
doi = {Doi 10.1198/016214507000001337},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2008 - Mixtures of g priors for Bayesian variable selection.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {aic,approximations,bayesian model averaging,bic,cauchy,criterion,empirical bayes,gaussian hypergeometric functions,linear-regression,matrix,model selection,multiple-regression,zellner-siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000254311500044$\backslash$nhttp://www.tandfonline.com/doi/abs/10.1198/016214507000001337$\backslash$nhttp://www.tandfonline.com/doi/pdf/10.1198/016214507000001337},
volume = {103},
year = {2008}
}
@article{OHara2009,
abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo {\&} Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Je{\AE}reys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their di{\AE}erent properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the di{\AE}erent methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
author = {O'Hara, R. B. and Sillanp????, M. J.},
doi = {10.1214/09-BA403},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hara, Sillanp - 2009 - A review of bayesian variable selection methods What, how and which.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {BUGS,MCMC,Variable selection},
number = {1},
pages = {85--118},
pmid = {273483200007},
title = {{A review of bayesian variable selection methods: What, how and which}},
volume = {4},
year = {2009}
}
@article{Fouskakis2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.2442v1},
author = {Fouskakis, D and Ntzoufras, Ioannis and Draper, David},
doi = {10.1214/14-BA887},
eprint = {arXiv:1307.2442v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fouskakis, Ntzoufras, Draper - 2015 - Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {athens,athens 15780 greece,bayes factors,bayesian variable selection,d,department of mathematics,email fouskakis,expected-posterior priors,fouskakis is with the,gaussian linear,gr,math,models,national technical university of,ntua,power-prior,pus,training samples,unit-information prior,zografou cam-},
number = {1},
pages = {75--107},
title = {{Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models}},
volume = {10},
year = {2015}
}
@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George andMcCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa- rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for pos- terior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probabil- ity. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I and Mcculloch, Robert E},
doi = {10.1.1.211.4871},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Mcculloch - 1997 - Approaches for bayesian variable selection.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,cal models,conjugate prior,gibbs sampling,gray code,hierarchi-,markov chain monte carlo,metropolis-hastings algorithms,mixtures,normal,normalization constant,regression,simulation},
pages = {339--373},
title = {{Approaches for bayesian variable selection}},
volume = {7},
year = {1997}
}
@article{Efron2013,
abstract = {Classical statistical theory ignores model selection in assessing estimation accuracy. Here we consider bootstrap methods for computing standard errors and confidence intervals that take model selection into account. The methodology involves bagging, also known as bootstrap smoothing, to tame the erratic discontinuities of selection-based estimators. A useful new formula for the accuracy of bagging then provides standard errors for the smoothed estimators. Two examples, nonparametric and parametric, are carried through in detail: a regression model where the choice of degree (linear, quadratic, cubic,. . . ) is determined by the Cp criterion, and a Lasso-based estimation problem.},
author = {Efron, Bradley},
doi = {10.1080/01621459.2013.823775},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 2013 - Estimation and Accuracy after Model Selection.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {abc intervals,bagging,bootstrap smoothing,c p,importance sam-,lasso,model averaging},
number = {October},
pages = {130725111823001},
pmid = {25346558},
title = {{Estimation and Accuracy after Model Selection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2013.823775},
volume = {1459},
year = {2013}
}
@article{Nan2014,
abstract = {Many exciting results have been obtained on model selection for high-dimensional data in both efficient algorithms and theoretical developments. The powerful penalized regression methods can give sparse representations of the data even when the number of predictors is much larger than the sample size. One important question then is: How do we know when a sparse pattern identified by such a method is reliable? In this work, besides investigating instability of model selection methods in terms of variable selection, we propose variable selection deviation measures that give one a proper sense on how many predictors in the selected set are likely trustworthy in certain aspects. Simulation and a real data example demonstrate the utility of these measures for application.},
author = {Nan, Ying and Yang, Yuhong},
doi = {10.1080/10618600.2013.829780},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nan, Yang - 2014 - Variable Selection Diagnostics Measures for High-Dimensional Regression.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Model selection diagnostics,Model selection instability,Variable selection deviation},
number = {3},
pages = {636--656},
title = {{Variable Selection Diagnostics Measures for High-Dimensional Regression}},
url = {http://www.tandfonline.com/doi/full/10.1080/10618600.2013.829780},
volume = {23},
year = {2014}
}
@article{Gordy1998,
abstract = {This paper introduces the "compound confluent hypergeometric" (CCH) distribution. The CCH unifies and generalizes three recently introduced generalizations of the beta distributions: the Gauss hypergeometric (GH) distribution of Armero and Bayarri (1994), the generalized beta (GB) distribution of McDonald and Xu (1995), and the confluent hypergeometric (CH) distribution of Gordy (forthcoming). In addition to greater flexibility in fitting data, the CCH offers two useful properties. Unlike the beta, GB and GH, the CCH allows for conditioning on explanatory variables in a natural and convenient way. The CCH family is conjugate for gamma distribution signals, and so may also prove useful in Bayesian analysis. Application of the CCH is demonstrated with two measures of household liquid assets. In each case, the CCH yields a statistically significant improvement in fit over the more restrictive alternatives.},
author = {Gordy, Michael B},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordy - 1998 - A generalization of generalized beta distributions.pdf:pdf},
number = {202},
pages = {1--28},
title = {{A generalization of generalized beta distributions}},
year = {1998}
}
@article{Maruyama2011,
abstract = {For the normal linear model variable selection problem, we propose selection criteria based on a fully Bayes formulation with a generalization of Zellner's g-prior which allows for p {\textgreater} n. A special case of the prior formulation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential interest.},
archivePrefix = {arXiv},
arxivId = {0801.4410},
author = {Maruyama, Yuzo and George, Edward I.},
doi = {10.1214/11-AOS917},
eprint = {0801.4410},
file = {:home/markg/Downloads/euclid.aos.1324563354.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayes factor,Model selection consistency,Ridge regression,Singular value decomposition,Variable selection},
number = {5},
pages = {2740--2765},
title = {{Fully bayes factors with a generalized g-prior}},
volume = {39},
year = {2011}
}
@article{Wang2015,
abstract = {Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.},
archivePrefix = {arXiv},
arxivId = {1506.02222},
author = {Wang, Xiangyu and Dunson, David and Leng, Chenlei},
eprint = {1506.02222},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Dunson, Leng - 2015 - No penalty no tears Least squares in high-dimensional linear models.pdf:pdf},
pages = {1--26},
title = {{No penalty no tears: Least squares in high-dimensional linear models}},
url = {http://arxiv.org/abs/1506.02222},
year = {2015}
}
@article{Wang2015a,
abstract = {Variable selection is a challenging issue in statistical applications when the number of predictors {\$}p{\$} far exceeds the number of observations {\$}n{\$}. In this ultra-high dimensional setting, the sure independence screening (SIS) procedure was introduced to significantly reduce the dimensionality by preserving the true model with overwhelming probability, before a refined second stage analysis. However, the aforementioned sure screening property strongly relies on the assumption that the important variables in the model have large marginal correlations with the response, which rarely holds in reality. To overcome this, we propose a novel and simple screening technique called the high-dimensional ordinary least-squares projection (HOLP). We show that HOLP possesses the sure screening property and gives consistent variable selection without the strong correlation assumption, and has a low computational complexity. A ridge type HOLP procedure is also discussed. Simulation study shows that HOLP performs competitively compared to many other marginal correlation based methods. An application to a mammalian eye disease data illustrates the attractiveness of HOLP.},
archivePrefix = {arXiv},
arxivId = {1506.01782},
author = {Wang, Xiangyu and Leng, Chenlei},
eprint = {1506.01782},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Leng - 2015 - High-dimensional Ordinary Least-squares Projection for Screening Variables.pdf:pdf},
keywords = {consistency,forward regression,generalized inverse,high dimensionality,lasso,marginal correlation,moore-penrose inverse,ordinary least squares,screening,sure independent,variable selection},
pages = {1--47},
title = {{High-dimensional Ordinary Least-squares Projection for Screening Variables}},
url = {http://arxiv.org/abs/1506.01782},
year = {2015}
}
@article{Eklund2007,
abstract = {Large scale Bayesian model averaging and variable selection exercises present, despite the great increase in desktop computing power, considerable computational challenges. Due to the large scale it is impossible to evaluate all possible models and estimates of posterior probabilities are instead obtained from stochastic (MCMC) schemes designed to converge on the posterior distribution over the model space. While this frees us from the requirement of evaluating all possible models the computational effort is still substantial and efficient implementation is vital. Efficient implementation is concerned with two issues: the efficiency of the MCMC algorithm itself and efficient computation of the quantities needed to obtain a draw from the MCMC algorithm. We evaluate several different MCMC algorithms and find that relatively simple algorithms with local moves perform competitively except possibly when the data is highly collinear. For the second aspect, efficient computation within the sampler, we focus on the important case of linear models where the computations essentially reduce to least squares calculations. Least squares solvers that update a previous model estimate are appealing when the MCMC algorithm makes local moves and we find that the Cholesky update is both fast and accurate.},
author = {Eklund, Jana and Karlsson, Sune},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eklund, Karlsson - 2007 - Computational Efficiency in Bayesian Model and Variable Selection.pdf:pdf},
keywords = {Bayesian Model Averaging,Cholesky decomposition,QR decomposition,Sweep operator,Swendsen-Wang algorithm},
number = {2007:4},
title = {{Computational Efficiency in Bayesian Model and Variable Selection}},
url = {http://econpapers.repec.org/RePEc:hhs:oruesi:2007{\_}004},
year = {2007}
}
@article{Zhu2004,
abstract = {Classification of patient samples is an important aspect of cancer diagnosis and treatment. The support vector machine (SVM) has been successfully applied to microarray cancer diagnosis problems. However, one weakness of the SVM is that given a tumor sample, it only predicts a cancer class label but does not provide any estimate of the underlying probability. We propose penalized logistic regression (PLR) as an alternative to the SVM for the microarray cancer diagnosis problem. We show that when using the same set of genes, PLR and the SVM perform similarly in cancer classification, but PLR has the advantage of additionally providing an estimate of the underlying probability. Often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classification, rather than class prediction. We consider two gene selection methods in this paper, univariate ranking (UR) and recursive feature elimination (RFE). Empirical results indicate that PLR combined with RFE tends to select fewer genes than other methods and also performs well in both cross-validation and test samples. A fast algorithm for solving PLR is also described.},
author = {Zhu, Ji and Hastie, Trevor},
doi = {10.1093/biostatistics/kxg046},
file = {:home/markg/Downloads/Zhu-Biostat04.pdf:pdf},
isbn = {1465-4644 (Print)$\backslash$r1465-4644 (Linking)},
issn = {14654644},
journal = {Biostatistics},
keywords = {Cancer diagnosis,Feature selection,Logistic regression,Microarray,Support vector machines},
number = {3},
pages = {427--443},
pmid = {15208204},
title = {{Classification of gene microarrays by penalized logistic regression}},
volume = {5},
year = {2004}
}
@book{Boyd2010,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:home/markg/Downloads/bv{\_}cvxbook.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
url = {https://web.stanford.edu/{~}boyd/cvxbook/bv{\_}cvxbook.pdf},
volume = {25},
year = {2010}
}
@article{Pregibon1981,
abstract = {A maximum likelihood fit of a logistic regression model (and other similar models) is extremely sensitive to outlying responses and extreme points in the design space. We develop diagnostic measures to aid the analyst in detecting such observations and in quantifying their effect on various aspects of the maximum likelihood fit. The elements of the fitting process which constitute the usual output (parameter estimates, standard errors, residuals, etc.) will be used for this purpose. With a properly designed computing package for fitting the usual maximum-likelihood model, the diagnostics are essentially "free for the asking." In particular, good data analysis for logistic regression models need not be expensive or time-consuming.},
author = {Pregibon, Daryl},
doi = {10.1214/aos/1176345513},
file = {:home/markg/Downloads/2240841.pdf:pdf},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {4},
pages = {705--724},
title = {{Logistic Regression Diagnostics}},
volume = {9},
year = {1981}
}
@article{Gatu2006,
abstract = {An efficient branch-and-bound algorithm for computing the best-subset regression models is proposed. The algorithm avoids the computation of the whole regression tree that generates all possible subset models. It is formally shown that if the branch-and-bound test holds, then the current subtree together with its right-hand side subtrees are cut. This reduces significantly the computational burden of the proposed algorithm when compared to an existing leaps-and-bounds method which generates two trees. Specifically, the proposed algorithm, which is based on orthogonal transformations, outperforms by O(n 3) the leaps-and-bounds strategy. The criteria used in identifying the best subsets are based on monotone functions of the residual sum of squares (RSS) such as R2, adjusted R2, mean square error of prediction, and Cp. Strategies and heuristics that improve the computational performance of the proposed algorithm are investigated. A computationally efficient heuristic version of the branch-and-bound strategy which decides to cut subtrees using a tolerance parameter is proposed. The heuristic algorithm derives models close to the best ones. However, it is shown analytically that the relative error of the RSS, and consequently the corresponding statistic, of the computed subsets is smaller than the value of the tolerance parameter which lies between zero and one. Computational results and experiments on random and real data are presented and analyzed. {\textcopyright} 2006 American Statistical Association.},
author = {Gatu, Cristian and Kontoghiorghes, Erricos John},
doi = {10.1198/106186006X100290},
file = {:home/markg/Downloads/BBASubset.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {least squares,qr decomposition,subset regression},
number = {1},
pages = {139--156},
title = {{Branch-and-Bound Algorithms for Computing the Best-Subset Regression Models}},
volume = {15},
year = {2006}
}
@article{Bursac2008,
author = {Bursac, Zoran and Gauss, C Heath and Williams, David Keith and Hosmer, David W},
doi = {10.1186/1751-0473-3-17},
file = {:home/markg/Downloads/1751-0473-3-17.pdf:pdf},
issn = {1751-0473},
journal = {Source Code for Biology and Medicine},
number = {1},
pages = {17},
title = {{Purposeful selection of variables in logistic regression}},
url = {http://www.scfbm.org/content/3/1/17},
volume = {3},
year = {2008}
}
@article{Zellner2004,
author = {Zellner, Dietmar and Zellner, Dietmar and Keller, Frieder and Keller, Frieder and Zellner, G{\"{u}}nter E. and Zellner, G{\"{u}}nter E.},
doi = {10.1081/SAC-200033363},
file = {:home/markg/Downloads/sac-200033363.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {3},
pages = {787--805},
title = {{Variable Selection in Logistic Regression Models}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1081/SAC-200033363{\&}magic=crossref||D404A21C5BB053405B1A640AFFD44AE3},
volume = {33},
year = {2004}
}
@article{Clausen1999,
abstract = {A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimiza- tion problems, are easy to state, and have a finite but usually very large number of feasible solutions. While some of these as e.g. the Shortest Path problem and the Minimum Spanning Tree problem have polynomial algo- ritms, the majority of the problems in addition share the property that no polynomial method for their solution is known. Examples here are vehicle routing, crew scheduling, and production planning. All of these problems are NP-hard. Branch and Bound (B{\&}B) is by far the most widely used tool for solv- ing large scale NP-hard combinatorial optimization problems. B{\&}B is, however, an algorithm paradigm, which has to be filled out for each spe- cific problem type, and numerous choices for each of the components ex- ist. Even then, principles for the design of efficient B{\&}B algorithms have emerged over the years. In this paper I review the main principles of B{\&}B and illustrate the method and the different design issues through three examples: the Sym- metric Travelling Salesman Problem, the Graph Partitioning problem, and the Quadratic Assignment problem.},
author = {Clausen, Jens},
doi = {10.1.1.5.7475},
file = {:home/markg/Downloads/b{\_}and{\_}b.pdf:pdf},
journal = {Department of Computer Science, University of {\ldots}},
pages = {1--30},
title = {{Branch and bound algorithms-principles and examples}},
url = {http://www.imada.sdu.dk/{~}jbj/heuristikker/TSPtext.pdf},
year = {1999}
}
@article{Hosmer1989,
abstract = {Selection of a subset of meaningful covariates for a statistical model is an important and often time-consuming task in model building. Lawless and Singhal (1978, Biometrics 34, 318-327) proposed a method for best subsets selection for nonnormal models. We develop a method for logistic regression that may be performed with any best subsets linear regression program. CR - Copyright {\&}{\#}169; 1989 International Biometric Society},
author = {Hosmer, David W and Jovanovic, Borko and Lemeshow, Stanley},
doi = {10.2307/2531779},
file = {:home/markg/Downloads/2531779.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
number = {4},
pages = {1265--1270},
title = {{Best Subsets Logistic Regression}},
url = {http://www.jstor.org/stable/2531779},
volume = {45},
year = {1989}
}
@article{Mcleod2010,
abstract = {The function bestglm selects the best subset of inputs for the glm family. The selection methods available include a variety of information criteria as well as cross-validation. Several examples are provided to show that this approach is sometimes more accurate than using the built-in R function step. In the Gaussian case the leaps-and-bounds algorithm in leaps is used provided that there are no factor variables with more than two levels. In the non-Gaussian glm case or when there are factor variables present with three or more levels, a simple exhaustive enumeration approach is used. This vignette also explains how the applications given in our article Xu and McLeod (2010) may easily be reproduced. A separate vignette is available to provide more details about the simulation results reported in Xu and McLeod (2010, Table 2) and to explain how the results may be reproduced.},
author = {Mcleod, a I},
file = {:home/markg/Downloads/bestglm.pdf:pdf},
journal = {Prostate The},
keywords = {aic,best subset glm,bic,cross validation,extended bic},
pages = {1--39},
title = {{bestglm : Best Subset GLM}},
url = {http://brieger.esalq.usp.br/CRAN/web/packages/bestglm/vignettes/bestglm.pdf},
year = {2010}
}
@article{Vandenberghe1996,
abstract = {In sernidefinite programming, one minimizes a linear function subject to the constraint that an affine combination ofsynunetric matrices is positive semidefinite. Such a constraint is nonlinear and nonsmooth, but convex, so semidefinite programs are convex optimization problems. Semidefinite programming unifies several standard problems (e.g., linear and quadratic programming) and finds many applications in engineering and combinatorial optimization. Although semidefinite programs are much more general than linear programs, they are not much harder to solve. Most interior-point methods for linear programming have been generalized to semidefinite programs. As in linear programming, these methods have polynomial worst-case complexity and perform very well in practice. This paper gives a survey of the theory and applications of semidefinite programs and an introduction to primal- dual interior-point methods for their solution.},
author = {Vandenberghe, Lieven and Boyd, Stephen},
doi = {10.1137/1038003},
file = {:home/markg/Downloads/semidef{\_}prog.pdf:pdf},
isbn = {10.1137/1038003},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {combinatorial optimization,convex optimization,eigenvalue optimization,interior-point methods,semidefinite programming,system and control theory},
number = {1},
pages = {49--95},
pmid = {18059682},
title = {{Semidefinite Programming}},
url = {http://epubs.siam.org/doi/abs/10.1137/1038003},
volume = {38},
year = {1996}
}
@article{S.2000,
author = {S. and Wood, N},
file = {:home/markg/Downloads/mspfinal.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society Series B},
keywords = {generalized additive models,generalized cross-validation,generalized ridge re-,gression,likelihood,model selection,multiple smoothing parameters,non-linear modelling,penalized,penalized regression splines},
number = {2},
pages = {413--428},
title = {{Modelling and smoothing parameter estimation with multiple quadratic penalties}},
volume = {62},
year = {2000}
}
@article{Petersen2012,
abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
doi = {10.1111/j.1365-294X.2006.03161.x},
file = {:home/markg/Downloads/matrixcookbook.pdf:pdf},
isbn = {0962-1083 (Print)$\backslash$r0962-1083 (Linking)},
issn = {09621083},
journal = {Citeseer},
keywords = {acknowledgements,and suggestions,bill baxter,christian rish{\o}j,contributions,derivative of,derivative of inverse matrix,determinant,di erentiate a matrix,douglas l,esben,matrix algebra,matrix identities,matrix relations,thank the following for,theobald,we would like to},
pages = {1--66},
pmid = {17284204},
title = {{The Matrix Cookbook}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+machine+learning+for+information+retrieval{\#}5},
year = {2012}
}
@article{Lee2015,
author = {Lee, Cathy Yuen Yi and Wand, Matt P.},
doi = {10.1002/sim.6737},
file = {:home/markg/Downloads/sim6737.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {10.1002/sim.6737 and Bayesian inference,Markov chain Monte Carlo,approximation,bayesian inference,carlo,group-specific curves,longitudinal and multilevel data,markov chain monte,mean field variational Bayes approximation,mean field variational bayes,semiparametric regression},
number = {August},
pages = {n/a--n/a},
title = {{Variational methods for fitting complex Bayesian mixed effects models to health data}},
url = {http://doi.wiley.com/10.1002/sim.6737},
year = {2015}
}
@article{Barber1998,
abstract = {Bayesian treatments of learning in neural networks$\backslash$nare typically based either on local Gaussian$\backslash$napproximations to a mode of the posterior weight$\backslash$ndistribution, or on Markov chain Monte Carlo$\backslash$nsimulations. A third approach, called ensemble$\backslash$nlearning, was introduced by Hinton and van Camp$\backslash$n(1993). It aims to approximate the posterior$\backslash$ndistribution by minimizing the Kullback-Leibler$\backslash$ndivergence between the true posterior and a$\backslash$nparametric approximating distribution. However, the$\backslash$nderivation of a deterministic algorithm relied on$\backslash$nthe use of a Gaussian approximating distribution$\backslash$nwith a diagonal covariance matrix and so was unable$\backslash$nto capture the posterior correlations between$\backslash$nparameters. In this paper, we show how the ensemble$\backslash$nlearning approach can be extended to$\backslash$nfull-covariance Gaussian distributions while$\backslash$nremaining computationally tractable. We also extend$\backslash$nthe framework to deal with hyperparameters, leading$\backslash$nto a simple re-estimation procedure. Initial$\backslash$nresults from a standard benchmark problem are$\backslash$nencouraging.},
author = {Barber, D and Bishop, C M},
file = {:home/markg/Downloads/1480-ensemble-learning-for-multi-layer-networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {395--401},
title = {{Ensemble learning for multi-layer networks}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=M55BL-GvQ8IC{\&}oi=fnd{\&}pg=PA395{\&}dq=Ensemble+learning+for+multi-layer+networks{\&}ots=Fsyx0wyKmU{\&}sig=WqiSbpJZBdFfuMuIEHrWqGqO9Sg},
year = {1998}
}
@article{Raiko2007,
abstract = {We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas au- tomatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords:},
author = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen, Juha},
file = {:home/markg/Downloads/raiko07a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian modelling,building blocks,graphical models,latent variable models,local computation,variational Bayesian learning},
pages = {155--201},
title = {{Building Blocks for Variational Bayesian Learning of Latent Variable Models}},
volume = {8},
year = {2007}
}
@article{Garay2015,
abstract = {In recent years, there has been considerable interest in regression models based on zero-inflated distribu-tions. These models are commonly encountered in many disciplines, such as medicine, public health, and environmental sciences, among others. The zero-inflated Poisson (ZIP) model has been typically consid-ered for these types of problems. However, the ZIP model can fail if the non-zero counts are overdispersed in relation to the Poisson distribution, hence the zero-inflated negative binomial (ZINB) model may be more appropriate. In this paper, we present a Bayesian approach for fitting the ZINB regression model. This model considers that an observed zero may come from a point mass distribution at zero or from the negative binomial model. The likelihood function is utilized to compute not only some Bayesian model selection measures, but also to develop Bayesian case-deletion influence diagnostics based on q-divergence measures. The approach can be easily implemented using standard Bayesian software, such as WinBUGS. The performance of the proposed method is evaluated with a simulation study. Further, a real data set is analyzed, where we show that ZINB regression models seems to fit the data better than the Poisson counterpart.},
author = {Garay, Aldo M and Lachos, Victor H and Bolfarine, Heleno and Paulo, S{\~{a}}o},
doi = {10.1080/02664763.2014.995610},
file = {:home/markg/Downloads/ZINB{\_}BayesianoRES.pdf:pdf},
issn = {0266-4763},
journal = {Journal of Applied Statistics},
keywords = {Bayesian inference,MCMC,binomial negative distribution,q-divergence measures,zero-inflated models},
number = {6},
pages = {1148--1165},
title = {{Bayesian estimation and case influence diagnostics for the zero-inflated negative binomial regression model}},
url = {http://www.tandfonline.com/loi/cjas20$\backslash$nhttp://dx.doi.org/10.1080/02664763.2014.995610$\backslash$nhttp://www.tandfonline.com/page/terms-and-conditions},
volume = {42},
year = {2015}
}
@article{Challis2011,
abstract = {Two popular approaches to forming principled bounds in approximate Bayesian inference are local variational methods and minimal Kullback-Leibler divergence methods. For a large class of models, we explicitly relate the two approaches, showing that the local variational method is equivalent to a weakened form of Kullback-Leibler Gaussian approximation. This gives a strong motivation to develop ecient methods for KL minimisation. An important and previously unproven property of the KL variational Gaussian bound is that it is a concave function in the parameters of the Gaussian for log concave sites. This observation, along with compact concave parameterisations of the covariance, enables us to develop fast scalable optimisation procedures to obtain lower bounds on the marginal likelihood in large scale Bayesian linear models.},
author = {Challis, Edward and Barber, D},
file = {:home/markg/Downloads/challis11a.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Fourteenth International {\ldots}},
number = {2009},
pages = {199--207},
title = {{Concave Gaussian variational approximations for inference in large-scale Bayesian linear models}},
url = {http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/challis{\_}barber{\_}AISTATS.pdf},
volume = {15},
year = {2011}
}
@article{Knowles2011,
abstract = {Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.},
author = {Knowles, David and Minka, Thomas P.},
file = {:home/markg/Downloads/KnoMin11.pdf:pdf},
isbn = {9781618395993},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {1--9},
title = {{Non-conjugate variational message passing for multinomial and binary regression}},
url = {http://eprints.pascal-network.org/archive/00008459/},
year = {2011}
}
@article{Chib1995,
author = {Chib, Siddhartha and Greenberg, Edward},
file = {:home/markg/Downloads/chib{\_}1995.pdf:pdf},
journal = {American Statistician},
number = {4},
pages = {327--335},
title = {{Understanding the metropolis-hastings algorithm}},
volume = {49},
year = {1995}
}
@article{Wand2014,
abstract = {Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very efficiently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit.},
author = {Wand, Matt P.},
file = {:home/markg/Downloads/wand14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {bayesian computing,field variational bayes,graphical models,matrix differential calculus,mean,variational approximation},
pages = {1351--1369},
title = {{Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing}},
url = {http://jmlr.org/papers/v15/wand14a.html},
volume = {15},
year = {2014}
}
@article{Opper2009,
abstract = {The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an Omicron(N)(2) number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually Omicron(N). The approach is applied to gaussian process regression with nongaussian likelihoods.},
author = {Opper, Manfred and Archambeau, C{\'{e}}dric},
doi = {10.1162/neco.2008.08-07-592},
file = {:home/markg/Downloads/neco{\_}mo09{\_}web.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
number = {3},
pages = {786--792},
pmid = {18785854},
title = {{The variational gaussian approximation revisited.}},
volume = {21},
year = {2009}
}
@article{Saul1996,
abstract = {We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition--the classification of handwritten digits.},
archivePrefix = {arXiv},
arxivId = {http://arxiv.org/pdf/cs/9603102.pdf},
author = {Saul, L K and Jaakkola, T and Jordan, M I},
doi = {10.1.1.54.4128},
eprint = {/arxiv.org/pdf/cs/9603102.pdf},
file = {:home/markg/Downloads/live-251-1520-jair.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
number = {1570},
pages = {61--76},
primaryClass = {http:},
title = {{Mean Field Theory for Sigmoid Belief Networks}},
url = {http://arxiv.org/abs/cs/9603102},
volume = {4},
year = {1996}
}
@article{Carlo1990,
author = {Carlo, Monte},
file = {:home/markg/Downloads/p5-hinton.pdf:pdf},
keywords = {gaussian quadrature,importance sampling,laplacian approximation,maximum likelihood estimation,nonlinear mixed effects models},
title = {{of the Weights P ( d ;}},
year = {1990}
}
@article{Nickisch2008,
abstract = {We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.},
author = {Nickisch, Hannes and Rasmussen, Carl Edward},
file = {:home/markg/Downloads/nickisch08a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with Statistics,Theory {\&} Algorithms},
pages = {2035--2078},
title = {{Approximations for Binary Gaussian Process Classification}},
url = {http://eprints.pascal-network.org/archive/00005312/},
volume = {9},
year = {2008}
}
@article{Seeger1999,
author = {Seeger, M},
file = {:home/markg/Downloads/1722-bayesian-model-selection-for-support-vector-machines-gaussian-processes-and-other-kernel-classifiers.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 12},
title = {{Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers}},
year = {1999}
}
@article{Wood2001,
abstract = {Objective functions that arise when fitting nonlinear models often contain local minima that are of little significance except for their propensity to trap minimization algorithms. The standard methods for attempting to deal with this problem treat the objective function as fixed and employ stochastic minimization approaches in the hope of randomly jumping out of local minima. This article suggests a simple trick for performing such minimizations that can be employed in conjunction with most conventional nonstochastic fitting methods. The trick is to stochastically perturb the objective function by bootstrapping the data to be fit. Each bootstrap objective shares the large-scale structure of the original objective but has different small-scale structure. Minimizations of bootstrap objective functions are alternated with minimizations of the original objective function starting from the parameter values with which minimization of the previous bootstrap objective terminated. An example is presented, fitting a nonlinear population dynamic model to population dynamic data and including a comparison of the suggested method with simulated annealing. Convergence diagnostics are discussed.},
author = {Wood, S N},
doi = {10.1111/j.0006-341X.2001.00240.x},
file = {:home/markg/Downloads/j.0006-341X.2001.00240.x.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
keywords = {ecological model,fitting,global optimization,nonlinear model fitting,population dynamic model,simulated annealing,stochastic optimization},
number = {1},
pages = {240--244},
pmid = {11252605},
title = {{Minimizing model fitting objectives that contain spurious local minima by bootstrap restarting.}},
volume = {57},
year = {2001}
}
@article{Wood2010,
abstract = {Generalized additive models (GAMs) have been popularized by the work of Hastie and Tibshirani (Generalized Additive Models (1990)) and the availability of user friendly gam software in Splus. However, whilst it is flexible and efficient, the gam framework based on backfitting with linear smoothers presents some difficulties when it comes to model selection and inference. On the other hand, the mathematically elegant work of Wahba (Spline Models for Observational Data (1990)) and co-workers on Generalized Spline Smoothing (GSS) provides a rigorous framework for model selection (SIAM J. Sci. Statist. Comput. 12 (1991) 383) and inference with GAMs constructed from smoothing splines: but unfortunately these models are computationally very expensive with operations counts that are of cubic order in the number of data. A `middle way' between these approaches is to construct GAMs using penalized regression splines (e.g. Marx and Eilers, Comput. Statist. Data Anal. (1998)). In this paper, we develop this idea further and show how GAMs constructed using penalized regression splines can be used to get most of the practical benefits of GSS models, including well founded model selection and multi-dimensional smooth terms, with the ease of use and low computational cost of backfit GAMs. Inference with the resulting methods also requires slightly fewer approximations than are employed in the GAM modelling software provided in Splus. This paper presents the basic mathematical and numerical approach to GAMs implemented in the package mgcv, and includes two environmental examples using the methods as implemented in the package.},
author = {Wood, S N and Augustin, N H},
doi = {10.1016/S0304-3800(02)00193-X},
file = {:home/markg/Downloads/wagam.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {gam,gcv,penalized regression spline},
number = {2-3},
pages = {157--177},
title = {{GAMs with integrated model selection using penalized regression splines and applications to environmental modelling}},
url = {http://opus.bath.ac.uk/7362/},
volume = {157},
year = {2010}
}
@article{Wood2004,
author = {Wood, Simon N},
doi = {10.1198/016214504000000980},
file = {:home/markg/Downloads/016214504000000980.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {generalized additive mixed model,generalized cross-validation,penalized quasi-likelihood,regularization,reml,ridge regression,smoothing spline analysis of,spline,stable computation,variance},
number = {467},
pages = {673--686},
title = {{Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000980},
volume = {99},
year = {2004}
}
@article{Wood2003,
abstract = {discuss the production of low rank smoothers for d greater than or equal to 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with 'knot placement' that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics},
author = {Wood, Simon N.},
doi = {10.1111/1467-9868.00374},
file = {:home/markg/Downloads/1467-9868.00374.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Generalized additive model,Regression spline,Thin plate spline},
pages = {95--114},
title = {{Thin plate regression splines}},
volume = {65},
year = {2003}
}
@article{Wood2008,
abstract = {Existing computationally efficient methods for penalized likelihood generalized additive model fitting employ iterative smoothness selection on working linear models (or working mixed models). Such schemes fail to converge for a non-negligible proportion of models, with failure being particularly frequent in the presence of concurvity. If smoothness selection is performed by optimizing ‘whole model' criteria these problems disappear, but until now attempts to do this have employed finite-difference-based optimization schemes which are computationally inefficient and can suffer from false convergence. The paper develops the first computationally efficient method for direct generalized additive model smoothness selection. It is highly stable, but by careful structuring achieves a computational efficiency that leads, in simulations, to lower mean computation times than the schemes that are based on working model smoothness selection. The method also offers a reliable way of fitting generalized additive mixed models.},
archivePrefix = {arXiv},
arxivId = {0709.3906},
author = {Wood, Simon N.},
doi = {10.1111/j.1467-9868.2007.00646.x},
eprint = {0709.3906},
file = {:home/markg/Downloads/j.1467-9868.2007.00646.x.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Akaike's information criterion,Generalized additive mixed models,Generalized additive models,Generalized approximate cross-validation,Generalized cross-validation,Penalized likelihood,Penalized regression splines,Stable computation},
number = {3},
pages = {495--518},
title = {{Fast stable direct fitting and smoothness selection for generalized additive models}},
volume = {70},
year = {2008}
}
@article{Clarke2006,
abstract = {The distribution and biomass of phytoplankton in the upper layers of the ocean are important indicators of productivity and carbon cycling. Large scale perturbations in phytoplankton are linked to global climate change, so accurate monitoring is increasingly important. The chlorophyll-a pigment concentration in the water is routinely measured as an index of algal biomass. Direct water sampling from ships and moorings provides accurate data, but woefully poor spatial and temporal coverage of the oceans. In contrast, multispectral sea surface reflectance data from orbiting satellite-borne sensors, which in principle can be used to derive pigment concentration, give the prospect of globally detailed spatial and temporal coverage. Unfortunately, there are some locally variable confounding factors, which the algorithms for converting reflectance data to ocean chlorophyll-a concentration do not take into account. Hence, statistical methods are needed to obtain accurate predictions of chlorophyll-a. concentration by using data from both these sources. We use penalized regression splines to model water sample data as a three-dimensional function of satellite measurements, seabed depth and time of year. The models are effectively complex calibrations of the satellite data against the bottle data. We compare the results by using thin plate regression splines and tensor product splines using generalized cross-validation to choose the relative amounts of smoothing for each of the covariates. Since the thin plate spline penalty functional is isotropic, this requires the introduction of two scaling parameters, which are also chosen by generalized cross-validation, to scale the covariates relatively to one another. The tensor product spline smooths each covariate appropriately by use of separate smoothing parameters for each covariate. The models are tested by application to data from the north-east Atlantic, first randomly subsampling the data to achieve even coverage over the entire region. Both approaches perform equally well, achieving R-2 approximate to 65{\%}, both for the data that are used to fit the model and for a validation data set. Of particular concern in this application is that monthly predictions from the models should be biologically plausible over the whole region, describing the broad regional features that are apparent in the satellite data and extrapolating sensibly where satellite data are not available. To achieve this, the satellite data must be one of the covariates in the model; spatiotemporal covariates alone are not sufficient to extrapolate sensibly into areas where no data are available.},
author = {Clarke, E. D. and Speirs, D. C. and Heath, M. R. and Wood, S. N. and Gurney, W. S. C. and Holmes, S. J.},
doi = {10.1111/j.1467-9876.2006.00540.x},
file = {:home/markg/Downloads/j.1467-9876.2006.00540.x.pdf:pdf},
issn = {0035-9254},
journal = {Journal of the Royal Statistical Society Series C-Applied Statistics},
pages = {331--353},
title = {{Calibrating remotely sensed chlorophyll-a data by using penalized regression splines}},
volume = {55},
year = {2006}
}
@article{Ood2006,
author = {Ood, S Imon N W},
doi = {10.1111/j.1467-842X.2006.00450.x},
file = {:home/markg/Downloads/j.1467-842X.2006.00450.x.pdf:pdf},
issn = {1369-1473},
keywords = {bayesian confidence interval,gam,gcv,generalized additive model,generalized cross,multiple smoothing parameters,penalized regression spline,validation},
number = {4},
pages = {445--464},
title = {{ON CONFIDENCE INTERVALS FOR GENERALIZED ADDITIVE MODELS BASED ON PENALIZED REGRESSION SPLINES University of Bath}},
volume = {48},
year = {2006}
}
@article{Wood2008a,
abstract = {Conventional smoothing methods sometimes perform badly when used to smooth data over complex domains, by smoothing inappropriately across boundary features, such as peninsulas. Solutions to this smoothing problem tend to be computationally complex, and not to provide model smooth functions which are appropriate for incorporating as components of other models, such as generalized additive models or mixed additive models. We propose a class of smoothers that are appropriate for smoothing over difficult regions of R2 which can be represented in terms of a low rank basis and one or two quadratic penalties. The key features of these smoothers are that they do not `smooth across' boundary features, that their representation in terms of a basis and penalties allows straightforward incorporation as components of generalized additive models, mixed models and other non-standard models, that smoothness selection for these model components is straightforward to accomplish in a computationally efficient manner via generalized cross-validation, Akaike's information criterion or restricted maximum likelihood, for example, and that their low rank means that their use is computationally efficient.},
author = {Wood, Simon N. and Bravington, Mark V. and Hedley, Sharon L.},
doi = {10.1111/j.1467-9868.2008.00665.x},
file = {:home/markg/Downloads/j.1467-9868.2008.00665.x.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Basis penalty smooth,Differential equation smoothing,FELSPLINE,Finite window smoothing,Known boundary smoothing,Spline},
number = {5},
pages = {931--955},
title = {{Soap film smoothing}},
volume = {70},
year = {2008}
}
@article{Wood2006,
abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
author = {Wood, Simon N.},
doi = {10.1111/j.1541-0420.2006.00574.x},
file = {:home/markg/Downloads/j.1541-0420.2006.00574.x.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
keywords = {Computationally efficient,Generalized additive mixed model (GAMM),Mixed effect variable coefficient model,Multiple penalties,Penalized regression,SS-ANOVA,Scale invariant,Smooth interaction,Smoothing penalty,Spline,Tensor product smooth},
number = {4},
pages = {392},
pmid = {17156276},
title = {{Low-rank scale-invariant tensor product smooths for generalized additive mixed models}},
volume = {62},
year = {2006}
}
@article{Demyanov2006,
abstract = {Population dynamic modelling often entails parameterizing quite sophisticated biological and ecological mechanisms. For models of moderate mechanistic complexity, this has traditionally been done in an ad hoc manner, with different parameters being estimated independently. The point estimates so obtained are then used for model simulation, perhaps with some further ad hoc adjustment based on comparison with any available data on population dynamics.Quantitative assessments of model adequacy and prediction uncertainty are not easily made using this approach. As an alternative, the paper investigates the practical feasibility of fitting a moderately complex population dynamic model directly and simultaneously to all the data available for parameterization of the model, and to all available data on the population dynamics of the target animal. This alternative approach allows us to combine all available quantitative information on the target species, to assess the viability of the model, the mutual consistency of model and different sources of data and to estimate the uncertainties that are associated with model-based predictions. The target organism in this study is the freshwater amphipod Gammarus pulex (L.), which we model using a stage-structured population dynamic model, implemented via a set of delay differential equations describing the basic demography of the population. Target data include population dynamic data from two sites, information on basic physiological relationships and environmental temperature data. Fitting is performed by using a non-linear least squares approach supplemented with a bootstrapping method for avoiding small scale local minima in the least squares objective function. Variance estimation is performed by further bootstrapping. Interest in Gammarus pulex population dynamics in this case is primarily related to likely population level responses to chemical stressors, and for this we examine predicted ‘recovery times' following exposure to a known toxicant.},
author = {Demyanov, V. and Wood, S. N. and Kedwards, T. J. and Dernyanov, V},
doi = {10.1111/j.1467-9876.2005.00527.x},
file = {:home/markg/Downloads/j.1467-9876.2005.00527.x.pdf:pdf},
issn = {0035-9254},
journal = {Applied Statistics},
keywords = {differential equation model,ecological prediction,ecological risk assessment,population dynamic model},
number = {1},
pages = {41--62},
title = {{Improving ecological impact assessment by statistical data synthesis using process-based models}},
url = {http://doi.wiley.com/10.1111/j.1467-9876.2005.00527.x},
volume = {55},
year = {2006}
}
@article{Amari1998a,
abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
author = {Amari, Shun-ichi},
doi = {10.1162/089976698300017746},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {251--276},
title = {{Natural Gradient Works Efficiently in Learning}},
volume = {10},
year = {1998}
}
@article{Amari1998,
abstract = {Gradient adaptation is a useful technique for adjusting a set of parameters to minimize a cost function. While often easy to implement, the convergence speed of gradient adaptation can be slow when the slope of the cost function varies widely for small changes in the parameters. In this paper, we outline an alternative technique, termed natural gradient adaptation, that overcomes the poor convergence properties of gradient adaptation in many cases. The natural gradient is based on differential geometry and employs knowledge of the Riemannian structure of the parameter space to adjust the gradient search direction. Unlike Newton's method, natural gradient adaptation does not assume a locally-quadratic cost function. Moreover, for maximum likelihood estimation tasks, natural gradient adaptation is asymptotically Fisher-efficient. A simple example illustrates the desirable properties of natural gradient adaptation.},
author = {Amari, S and Douglas, Sc},
doi = {10.1109/ICASSP.1998.675489},
isbn = {0-7803-4428-6},
issn = {1520-6149},
journal = {{\ldots} , 1998. Proceedings of the 1998 IEEE {\ldots}},
pages = {1213--1216},
title = {{Why natural gradient?}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=675489},
volume = {9},
year = {1998}
}
@article{Honkela2010,
abstract = {Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.},
author = {Honkela, Antti and Raiko, Tapani and Kuusela, Mikael and Tornio, Matti and Karhunen, Juha},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Honkela et al. - 2010 - Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {5},
pages = {3235--3268},
title = {{Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes}},
url = {http://eprints.pascal-network.org/archive/00007751/},
volume = {11},
year = {2010}
}
@article{Lambert1992,
abstract = {Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 - p, a Poisson(lambda) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(lambda) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects-lambda in the imperfect state-may depend on covariates. Sometimes p and lambda are unrelated; other times p is a simple function of lambda such as p = 1/(1 + lambda(tau)) for an unknown constant-tau. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.},
author = {Lambert, Diane},
file = {:home/markg/Downloads/1269547.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {EM ALGORITHM,NEGATIVE BINOMIAL,OVERDISPERSION,P},
number = {1},
pages = {1--14},
title = {{Zero-Inflated Poisson Regression, With an Application To Defects in Manufacturing}},
url = {File Attachments},
volume = {34},
year = {1992}
}
@article{Rohde2015,
author = {Rohde, D and Wand, M. P.},
file = {:home/markg/Downloads/RohdeWand.pdf:pdf},
keywords = {bayesian computing,conjugate variational message passing,fixed-form variational bayes,fixed-point iteration,non-,nonlinear conjugate gradient method},
number = {January},
pages = {1--41},
title = {{Semiparametric Mean Field Variational Bayes : General Principles and Numerical Issues Semiparametric Mean Field Variational Bayes :}},
year = {2015}
}
@article{Wand2002,
abstract = {Many statisitcal operations benefit from differential calculus. Examples include optimization of likelihood functions and calculation of information matrices. For multiparameter models differential calculuc suited to vector argument functions is usually the most efficient means of performing the required calculations. We present a primer on vector differential calculus and demonstrate its application to statistics through several worked examples.},
author = {Wand, M. P},
doi = {10.1198/000313002753631376},
file = {:home/markg/Downloads/Wand02.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {best linear prediction,generalized linear,generalized linear mixed model,information matrix,matrix differential calculus,maximum likelihood estimation,model,penalized quasi-likelihood,score equation},
number = {1},
pages = {55--62},
title = {{Vector Differential Calculus in Statistics}},
volume = {56},
year = {2002}
}
@article{Ormerod2012,
abstract = {Variational approximation methods have become a mainstay of contemporary machine learning methodology, but currently have little presence in statistics. We devise an effective variational approximation strategy for fitting generalized linear mixed models (GLMMs) appropriate for grouped data. It involves Gaussian approximation to the distributions of random effects vectors, conditional on the responses. We show that Gaussian variational approximation is a relatively simple and natural alternative to Laplace approximation for fast, non-Monte Carlo, GLMM analysis. Numerical studies show Gaussian variational approximation to be very accurate in grouped data GLMM contexts. Finally, we point to some recent theory on consistency of Gaussian variational approximation in this context. Supplemental materials are available online.},
author = {Ormerod, J. T. and Wand, M. P.},
doi = {10.1198/jcgs.2011.09118},
file = {:home/markg/Downloads/Ormerod11.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {best prediction,likelihood-based inference,longitudinal data analysis,machine learning,variance components},
number = {1},
pages = {2--17},
title = {{Gaussian Variational Approximate Inference for Generalized Linear Mixed Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.09118},
volume = {21},
year = {2012}
}
@article{Vatsa2014,
author = {Vatsa, Richa and Wilson, Simon},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vatsa, Wilson - 2014 - Variational Bayes Approximation for Inverse Non-Linear Regression.pdf:pdf},
issn = {2278-2273},
keywords = {1,2278-2273,2348-7909,e-issn,earch,inverse non-linear,journal of statistics,p-issn,reviews,special issue on recent,statistical methodologies and applications,variational bayes approximation for,vol},
number = {JANUARY 2014},
pages = {76--84},
title = {{Variational Bayes Approximation for Inverse Non-Linear Regression}},
year = {2014}
}
@book{Gelman2007,
abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/{\~{}}gelman/arm/},
author = {Gelman, Andrew and Hill, Jennifer},
booktitle = {Policy Analysis},
doi = {10.2277/0521867061},
isbn = {052168689X},
issn = {0022-0655},
pages = {625},
pmid = {14341096},
title = {{Data analysis using regression and multilevel/hierarchical models}},
url = {http://books.google.com/books?id=c9xLKzZWoZ4C},
volume = {625},
year = {2007}
}
@book{Agresti2002,
abstract = {Cap{\'{i}}tulo 4: Introducci{\'{o}}n a los GLM. La secci{\'{o}}n 4.3 est{\'{a}} dedicada a datos de conteo. Es una buena introducci{\'{o}}n, con un poco m{\'{a}}s de profundidad en cada tema. Todo el libro es muy bueno.},
author = {Agresti, Alan},
booktitle = {Statistical methodology in the pharmaceutical sciences},
doi = {10.1002/0471249688},
isbn = {0471360937},
issn = {0949-1775},
pages = {1--17},
pmid = {15003161},
title = {{Categorical Data Analysis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Categorical+Data+Analysis{\#}0$\backslash$nhttp://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Categorical+data+analysis{\#}0},
volume = {13},
year = {2002}
}
@article{Luts2013,
abstract = {A mean field variational Bayes approach to support vector machines (SVMs) using the latent variable representation on Polson {\&} Scott (2012) is presented. This representation allows circumvention of many of the shortcomings associated with classical SVMs including automatic penalty parameter selection, the ability to handle dependent samples, missing data and variable selection. We demonstrate on simulated and real datasets that our approach is easily extendable to non-standard situations and outperforms the classical SVM approach whilst remaining computationally efficient.},
archivePrefix = {arXiv},
arxivId = {1305.2667},
author = {Luts, Jan and Ormerod, John T.},
doi = {10.1016/j.csda.2013.10.030},
eprint = {1305.2667},
file = {:home/markg/Downloads/1305.2667v1.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {approximate bayesian inference},
number = {2000},
pages = {18},
title = {{Mean field variational Bayesian inference for support vector machine classification}},
url = {http://arxiv.org/abs/1305.2667},
volume = {73},
year = {2013}
}
@article{LeeWangScottYauMcLachlan2006,
abstract = {Count data with excess zeros relative to a Poisson distribution are common in many biomedical applications. A popular approach to the analysis of such data is to use a zero-inflated Poisson (ZIP) regression model. Often, because of the hierarchical study design or the data collection procedure, zero-inflation and lack of independence may occur simultaneously, which render the standard ZIP model inadequate. To account for the preponderance of zero counts and the inherent correlation of observations, a class of multi-level ZIP regression model with random effects is presented. Model fitting is facilitated using an expectation-maximization algorithm, whereas variance components are estimated via residual maximum likelihood estimating equations. A score test for zero-inflation is also presented. The multi-level ZIP model is then generalized to cope with a more complex correlation structure. Application to the analysis of correlated count data from a longitudinal infant feeding study illustrates the usefulness of the approach.},
annote = {Copyright - {\textcopyright} 2006 Arnold; Last updated - 2014-04-29},
author = {Lee, Andy H and Wang, Kui and Scott, Jane A and Yau, Kelvin K W and McLachlan, Geoffrey J},
file = {:home/markg/Downloads/lwsym{\_}smmr06.pdf:pdf},
isbn = {09622802},
journal = {Statistical methods in medical research},
keywords = {Medical Sciences; Infant; Breast Feeding -- statis,Newborn; Models,Statistical; Longitudinal Studies; Male; Female;},
number = {1},
pages = {47--61},
title = {{Multi-level zero-inflated Poisson regression modelling of correlated count data with excess zeros}},
url = {http://ezproxy.library.usyd.edu.au/login?url=http://search.proquest.com/docview/217677336?accountid=14757},
volume = {15},
year = {2006}
}
@article{BIMJ:BIMJ200390024,
author = {Yau, Kelvin K W and Wang, Kui and Lee, Andy H},
doi = {10.1002/bimj.200390024},
issn = {1521-4036},
journal = {Biometrical Journal},
keywords = {Count data,Generalised linear mixed models,Negative binomial,Poisson regression,Random effects,Zero-inflation},
number = {4},
pages = {437--452},
publisher = {WILEY-VCH Verlag},
title = {{Zero-Inflated Negative Binomial Mixed Regression Modeling of Over-Dispersed Count Data with Extra Zeros}},
url = {http://dx.doi.org/10.1002/bimj.200390024},
volume = {45},
year = {2003}
}
@book{Demmel1997,
author = {Demmel, James W},
publisher = {Siam},
title = {{Applied numerical linear algebra}},
year = {1997}
}
@book{Nocedal2006,
author = {Nocedal, Jorge and Wright, Stephen},
publisher = {Springer Science {\&} Business Media},
title = {{Numerical optimization}},
year = {2006}
}
@article{Wand2008,
author = {Wand, M P and Ormerod, J T},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wand, Ormerod - 2008 - ON SEMIPARAMETRIC REGRESSION WITH O'SULLIVAN PENALIZED SPLINES.pdf:pdf},
journal = {Australian {\&} New Zealand Journal of Statistics},
number = {2},
pages = {179--198},
publisher = {Wiley Online Library},
title = {{On semiparametric regression with O'Sullivan penalized splines}},
volume = {50},
year = {2008}
}
@article{broydon1970,
author = {Broydon, C G},
journal = {Journal of the Institute of Mathematics and its Applications},
title = {{The convergence of a class of double rank minimisation algorithms: 1. General considerations}},
volume = {6},
year = {1970}
}
@article{BIOM:BIOM1030,
author = {Hall, Daniel B},
doi = {10.1111/j.0006-341X.2000.01030.x},
issn = {1541-0420},
journal = {Biometrics},
keywords = {EM algorithm,Excess zeros,Generalized linear mixed model,Heterogeneity,Mixed effects,Overdispersion,Repeated measures},
number = {4},
pages = {1030--1039},
publisher = {Blackwell Publishing Ltd},
title = {{Zero-Inflated Poisson and Binomial Regression with Random Effects: A Case Study}},
url = {http://dx.doi.org/10.1111/j.0006-341X.2000.01030.x},
volume = {56},
year = {2000}
}
@article{JOFP:rethink,
author = {{Atkins David C.; Gallop}, Robert J},
doi = {10.1037/0893-3200.21.4.726},
journal = {Journal of Family Psychology},
keywords = {Poisson regression,count models,zero-inflated models},
number = {4},
pages = {726--735},
publisher = {American Psychological Association},
title = {{Rethinking How Family Researchers Model Infrequent Outcomes: A Tutorial on Count Regression and Zero-Inflated Models.}},
volume = {21},
year = {2007}
}
@article{Shankar1997829,
abstract = {This paper presents an empirical inquiry into the applicability of zero-altered counting processes to roadway section accident frequencies. The intent of such a counting process is to distinguish sections of roadway that are truly safe (near zero-accident likelihood) from those that are unsafe but happen to have zero accidents observed during the period of observation (e.g. one year). Traditional applications of Poisson and negative binomial accident frequency models do not account for this distinction and thus can produce biased coefficient estimates because of the preponderance of zero-accident observations. Zero-altered probability processes such as the zero-inflated Poisson (ZIP) and zero-inflated negative binomial (ZINB) distributions are examined and proposed for accident frequencies by roadway functional class and geographic location. The findings show that the {\{}ZIP{\}} structure models are promising and have great flexibility in uncovering processes affecting accident frequencies on roadway sections observed with zero accidents and those with observed accident occurrences. This flexibility allows highway engineers to better isolate design factors that contribute to accident occurrence and also provides additional insight into variables that determine the relative accident likelihoods of safe versus unsafe roadways. The generic nature of the models and the relatively good power of the Vuong specification test used in the non-nested hypotheses of model specifications offers roadway designers the potential to develop a global family of models for accident frequency prediction that can be embedded in a larger safety management system. },
author = {Shankar, V and Milton, J and Mannering, F},
doi = {http://dx.doi.org/10.1016/S0001-4575(97)00052-3},
issn = {0001-4575},
journal = {Accident Analysis and Prevention},
keywords = {Accident frequency,Poisson regression,Zero-inflated count models},
number = {6},
pages = {829--837},
title = {{Modeling accident frequencies as zero-altered probability processes: An empirical inquiry}},
url = {http://www.sciencedirect.com/science/article/pii/S0001457597000523},
volume = {29},
year = {1997}
}
@misc{stan-manual:2015,
title = {{No Title}}
}
@article{Liu1989,
author = {Liu, Dong C and Nocedal, Jorge},
journal = {Mathematical programming},
number = {1-3},
pages = {503--528},
publisher = {Springer},
title = {{On the limited memory BFGS method for large scale optimization}},
volume = {45},
year = {1989}
}
@misc{rstan-software:2015,
title = {{No Title}}
}
@article{Min01042005,
abstract = {For count responses, the situation of excess zeros (relative to what standard models
allow) often occurs in biomedical and sociological applications. Modeling repeated
measures of zero-inflated count data presents special challenges. This is because in
addition to the problem of extra zeros, the correlation between measurements upon
the same subject at different occasions needs to be taken into account. This article
discusses random effect models for repeated measurements on this type of response
variable. A useful model is the hurdle model with random effects, which separately
handles the zero observations and the positive counts. In maximum likelihood model
fitting, we consider both a normal distribution and a nonparametric approach for the
random effects. A special case of the hurdle model can be used to test for zero
inflation. Random effects can also be introduced in a zero-inflated Poisson or
negative binomial model, but such a model may encounter fitting problems if there is
zero deflation at any settings of the explanatory variables. A simple alternative
approach adapts the cumulative logit model with random effects, which has a single
set of parameters for describing effects. We illustrate the proposed methods with examples.},
author = {Min, Yongyi and Agresti, Alan},
doi = {10.1191/1471082X05st084oa},
journal = {Statistical Modelling},
number = {1},
pages = {1--19},
title = {{Random effect models for repeated measures of zero-inflated count data}},
url = {http://smj.sagepub.com/content/5/1/1.abstract},
volume = {5},
year = {2005}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Jaakola{\_}Jordan{\_}2000.ps:ps},
title = {{Jaakola{\_}Jordan{\_}2000}}
}
@article{Ormerod2010,
abstract = {The American Statistician, Vol.64, No.2, 2010, 140-153},
author = {Ormerod, J T and Wand, M P},
doi = {10.1198/tast.2010.09058},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ormerod, Wand - 2010 - Explaining Variational Approximations.pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bayesian inference,Bayesian networks,Directed ac,back,bayesian inference,bayesian networks,di-,generalized linear mixed models,kull-,leibler divergence,linear mixed models,rected acyclic graphs},
number = {2},
pages = {140--153},
title = {{Explaining Variational Approximations}},
volume = {64},
year = {2010}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - WandOrmerod08.rs:rs},
title = {{WandOrmerod08}}
}
@article{Welham2007,
abstract = {Three types of polynomial mixed model splines have been proposed: smoothing splines, P-splines and penalized splines using a truncated power function basis. The close connections between these models are demonstrated, showing that the default cubic form of the splines differs only in the penalty used. A general definition of the mixed model spline is given that includes general constraints and can be used to produce natural or periodic splines. The impact of different penalties is demonstrated by evaluation across a set of functions with specific features, and shows that the best penalty in terms of mean squared error of prediction depends on both the form of the underlying function and the signal:noise ratio.},
author = {Welham, Sue J and Cullis, Brian R and Kenward, Michael G and Thompson, Robin},
doi = {10.1111/j.1467-842X.2006.00454.x},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Welham et al. - 2007 - A COMPARISON OF MIXED MODEL SPLINES FOR CURVE FITTING.pdf:pdf},
journal = {Aust. N. Z. J. Stat},
keywords = {P-splines,best linear unbiased prediction,mixed models,penalized splines,residual maximum likelihood,smoothing splines},
number = {1},
pages = {1--23},
title = {{A COMPARISON OF MIXED MODEL SPLINES FOR CURVE FITTING}},
volume = {49},
year = {2007}
}
@article{Zhao2006,
abstract = {Linear mixed models are able to handle an extraordinary range of complications in regression-type analyses. Their most common use is to account for within-subject correlation in longitudinal data analysis. They are also the standard vehicle for smoothing spatial count data. However, when treated in full generality, mixed models can also handle spline-type smoothing and closely approximate kriging. This allows for nonparametric regression models (e.g., additive models and varying coefficient models) to be handled within the mixed model framework. The key is to allow the ran-dom effects design matrix to have general structure; hence our label general design. For continuous response data, particularly when Gaussianity of the response is reasonably assumed, computation is now quite mature and sup-ported by the R, SAS and S-PLUS packages. Such is not the case for bi-nary and count responses, where generalized linear mixed models (GLMMs) are required, but are hindered by the presence of intractable multivariate in-tegrals. Software known to us supports special cases of the GLMM (e.g., PROC NLMIXED in SAS or glmmML in R) or relies on the sometimes crude Laplace-type approximation of integrals (e.g., the SAS macro glimmix or glmmPQL in R). This paper describes the fitting of general design general-ized linear mixed models. A Bayesian approach is taken and Markov chain Monte Carlo (MCMC) is used for estimation and inference. In this gener-alized setting, MCMC requires sampling from nonstandard distributions. In this article, we demonstrate that the MCMC package WinBUGS facilitates sound fitting of general design Bayesian generalized linear mixed models in practice.},
author = {Zhao, Y and Staudenmayer, J and Coull, B A and Wand, M P},
doi = {10.1214/088342306000000015},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2006 - General Design Bayesian Generalized Linear Mixed Models.pdf:pdf},
journal = {Statistical Science},
keywords = {Generalized additive models,Markov chain Monte Carlo,WinBUGS,and phrases,hierarchical center-ing,kriging,nonparametric regression,penalized splines,spatial count data},
number = {1},
pages = {35--51},
title = {{General Design Bayesian Generalized Linear Mixed Models}},
volume = {21},
year = {2006}
}
@article{Pham,
abstract = {A fast mean field variational Bayes (MFVB) approach to nonparametric regression when the predictors are subject to classical measurement error is investigated. It is shown that the use of such technology to the measurement error setting achieves reasonable accuracy. In tandem with the methodological development, a customized Markov chain Monte Carlo method is developed to facilitate the evaluation of accuracy of the MFVB method.},
author = {Pham, Tung H and Ormerod, John T and Wand, M P},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham, Ormerod, Wand - Unknown - Mean Field Variational Bayesian Inference for Nonparametric Regression with Measurement Error.pdf:pdf},
keywords = {Markov chain Monte Carlo,Penalized splines,classical measurement error,variational approximations},
title = {{Mean Field Variational Bayesian Inference for Nonparametric Regression with Measurement Error}}
}
@article{Cibulskis2013,
abstract = {Detection of somatic point substitutions is a key step in characterizing the cancer genome. However, existing methods typically miss low-allelic-fraction mutations that occur in only a subset of the sequenced cells owing to either tumor heterogeneity or contamination by normal cells. Here we present MuTect, a method that applies a Bayesian classifier to detect somatic mutations with very low allele fractions, requiring only a few supporting reads, followed by carefully tuned filters that ensure high specificity. We also describe benchmarking approaches that use real, rather than simulated, sequencing data to evaluate the sensitivity and specificity as a function of sequencing depth, base quality and allelic fraction. Compared with other methods, MuTect has higher sensitivity with similar specificity, especially for mutations with allelic fractions as low as 0.1 and below, making MuTect particularly useful for studying cancer subclones and their evolution in standard exome and genome sequencing data. Somatic single-nucleotide substitutions are an important and common mechanism for altering gene function in cancer. Yet they are difficult to identify. First, they occur at a very low frequency in the genome, ranging from 0.1 to 100 mutations per megabase (Mb), depending on tumor type 1–7 . Second, the alterations may be present only in a small fraction of the DNA molecules originating from the specific genomic locus for reasons including contaminating normal cells in the ana-lyzed sample, local copy-number variation in the cancer genome and presence of a mutation only in a subpopulation of the tumor cells 8–11 ('subclonality'). The fraction of DNA molecules harboring an altera-tion ('allelic fraction') has been reported to be as low as 0.05 for highly impure tumors 8 . The study of the subclonal structure of tumors is not only critical to understanding tumor evolution both in disease pro-gression and response to treatment 12 but also for developing reliable clinical diagnostic tools for personalized cancer therapy 13 . Recent reports on subclonal events in cancer have used three dif-ferent nonstandard experimental strategies: (i) analysis of clonal mutations present in several, but not all, of the metastases from the same patient, which suggested that these mutations were subclonal in the primary tumor 14 ; (ii) detection of subclonal mutations by ultra-deep sequencing 11 ; or (iii) sequencing of very small numbers of single cells 15–17 . In contrast, tens of thousands of tumors are being sequenced at standard depths of 100–150× for exomes and 30–60× for whole genomes as part of large-scale cancer genome projects, such as The Cancer Genome Atlas 1,2,7 and the International Cancer Genome Consortium 18 . To detect clonal and subclonal mutations present in these samples, one needs a highly sensitive and specific mutation-calling method. Although specificity can be controlled through subsequent experimental validation, this is an expensive and time-consuming step that is impractical for general application. The sensitivity and specificity of any somatic mutation–calling method varies along the genome and depends on several fac-tors, including the depth of sequence coverage in the tumor and a patient-matched normal sample, the local sequencing error rate, the allelic fraction of the mutation and the evidence thresholds used to declare a mutation. Characterizing how sensitivity and specificity depend on these factors is necessary for designing experiments with adequate power to detect mutations at a given allelic fraction, as well as for inferring the mutation frequency along the genome, which is a key parameter for understanding mutational processes and significance analysis 19,20 . To meet these critical needs of high sensitivity and specificity, which are not adequately addressed by available methods 21–23 , we developed a caller of somatic point mutations, MuTect. During its development, MuTect was used in many collaborative studies 1–4,7,19,24–35 . Here we describe the publicly available version of MuTect, including the ration-ale behind its different components. We also estimate its perform-ance as a function of the aforementioned factors using benchmarking approaches that, to our knowledge, have not been described before. The performance of the method is also supported by independent experi-mental validation in previous studies 3,4,7,19,24–30 as well as by its appli-cation to data sets analyzed in other publications 36,37 . We demonstrate that our method is several times more sensitive than other methods for low-allelic-fraction events while remaining highly specific, allowing for deeper exploration of the mutational landscape of highly impure tumor samples and of the subclonal evolution of tumors. MuTect is freely available for noncommercial use at http://www. broadinstitute.org/cancer/cga/mutect (Supplementary Data).},
author = {Cibulskis, Kristian and Lawrence, Michael S and Carter, Scott L and Sivachenko, Andrey and Jaffe, David and Sougnez, Carrie and Gabriel, Stacey and Meyerson, Matthew and Lander, Eric S and Getz, Gad},
doi = {10.1038/nbt.2514},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cibulskis et al. - 2013 - Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples.pdf:pdf},
journal = {Nature Biotechnology},
title = {{Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples}},
volume = {31},
year = {2013}
}
@article{Albert1993,
author = {Albert, James H and Chib, Siddhartha},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Albert, Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data.pdf:pdf},
journal = {Source Journal of the American Statistical Association},
number = {422},
pages = {669--679},
publisher = {American Statistical Association},
title = {{Bayesian Analysis of Binary and Polychotomous Response Data}},
url = {http://www.jstor.org/stable/2290350 http://www.jstor.org/ http://www.jstor.org/action/showPublisher?publisherCode=astata.},
volume = {88},
year = {1993}
}
@article{Lawrence2013,
abstract = {Major international projects are underway that are aimed at creating a comprehensive catalogue of all the genes responsible for the ini-tiation and progression of cancer 1–9 . These studies involve the sequencing of matched tumour–normal samples followed by math-ematical analysis to identify those genes in which mutations occur more frequently than expected by random chance. Here we describe a fundamental problem with cancer genome studies: as the sample size increases, the list of putatively significant genes produced by current analytical methods burgeons into the hundreds. The list includes many implausible genes (such as those encoding olfactory receptors and the muscle protein titin), suggesting extensive false-positive findings that overshadow true driver events. We show that this problem stems largely from mutational heterogeneity and provide a novel analytical methodology, MutSigCV, for resolving the problem. We apply MutSigCV to exome sequences from 3,083 tumour–normal pairs and discover extraordinary variation in mutation frequency and spectrum within cancer types, which sheds light on mutational processes and disease aetiology, and in mutation frequency across the genome, which is strongly correlated with DNA replication timing and also with transcriptional activity. By incorporating mutational heterogeneity into the analyses, MutSigCV is able to eliminate most of the apparent artefactual findings and enable the identification of genes truly associated with cancer.},
author = {Lawrence, Michael S and Stojanov, Petar and Polak, Paz and Kryukov, Gregory V and Cibulskis, Kristian and Sivachenko, Andrey and Carter, Scott L and Stewart, Chip and Mermel, Craig H and Roberts, Steven A and Kiezun, Adam and Hammerman, Peter S and Mckenna, Aaron and Drier, Yotam and Zou, Lihua and Ramos, Alex H and Pugh, Trevor J and Stransky, Nicolas and Helman, Elena and Kim, Jaegil and Sougnez, Carrie and Ambrogio, Lauren and Nickerson, Elizabeth and Shefler, Erica and Cort{\'{e}}s, Maria L and Auclair, Daniel and Saksena, Gordon and Voet, Douglas and Noble, Michael and Dicara, Daniel and Lin, Pei and Lichtenstein, Lee and Heiman, David I and Fennell, Timothy and Imielinski, Marcin and Hernandez, Bryan and Hodis, Eran and Baca, Sylvan and Dulak, Austin M and Lohr, Jens and Landau, Dan-Avi and Wu, Catherine J and Melendez-Zajgla, Jorge and Hidalgo-Miranda, Alfredo and Koren, Amnon and Mccarroll, Steven A and Mora, Jaume and Lee, Ryan S and Crompton, Brian and Onofrio, Robert and Parkin, Melissa and Winckler, Wendy and Ardlie, Kristin and Gabriel, Stacey B and Roberts, Charles W M and Biegel, Jaclyn A and Stegmaier, Kimberly and Bass, Adam J and Garraway, Levi A and Meyerson, Matthew and Golub, Todd R and Gordenin, Dmitry A and Sunyaev, Shamil and Lander, Eric S and Getz, Gad},
doi = {10.1038/nature12213},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lawrence et al. - 2013 - Mutational heterogeneity in cancer and the search for new cancer-associated genes.pdf:pdf},
journal = {Nature},
title = {{Mutational heterogeneity in cancer and the search for new cancer-associated genes}},
volume = {499},
year = {2013}
}
@article{Braun2008,
abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate infer-ence possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demon-strate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
author = {Braun, Michael and Mcauliffe, Jon},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braun, Mcauliffe - 2008 - Variational inference for large-scale models of discrete choice.2526:2526},
title = {{Variational inference for large-scale models of discrete choice}},
year = {2008}
}
@article{Hodis2012,
abstract = {SUMMARY Despite recent insights into melanoma genetics, systematic surveys for driver mutations are chal-lenged by an abundance of passenger mutations caused by carcinogenic UV light exposure. We devel-oped a permutation-based framework to address this challenge, employing mutation data from in-tronic sequences to control for passenger mutational load on a per gene basis. Analysis of large-scale melanoma exome data by this approach discovered six novel melanoma genes (PPP6C, RAC1, SNX31, TACC1, STK19, and ARID2), three of which—RAC1, PPP6C, and STK19—harbored recurrent and poten-tially targetable mutations. Integration with chromo-somal copy number data contextualized the land-scape of driver mutations, providing oncogenic insights in BRAF-and NRAS-driven melanoma as well as those without known NRAS/BRAF mutations. The landscape also clarified a mutational basis for RB and p53 pathway deregulation in this malignancy. Finally, the spectrum of driver mutations provided unequivocal genomic evidence for a direct muta-genic role of UV light in melanoma pathogenesis.},
author = {Hodis, Eran and Watson, Ian R and Kryukov, Gregory V and Arold, Stefan T and Imielinski, Marcin and Theurillat, Jean-Philippe and Nickerson, Elizabeth and Auclair, Daniel and Li, Liren and Place, Chelsea and Dicara, Daniel and Ramos, Alex H and Lawrence, Michael S and Cibulskis, Kristian and Sivachenko, Andrey and Voet, Douglas and Saksena, Gordon and Stransky, Nicolas and Onofrio, Robert C and Winckler, Wendy and Ardlie, Kristin and Wagle, Nikhil and Wargo, Jennifer and Chong, Kelly and Morton, Donald L and Stemke-Hale, Katherine and Chen, Guo and Noble, Michael and Meyerson, Matthew and Ladbury, John E and Davies, Michael A and Gershenwald, Jeffrey E and Wagner, Stephan N and Hoon, Dave S B and Schadendorf, Dirk and Lander, Eric S and Gabriel, Stacey B and Getz, Gad and Garraway, Levi A},
doi = {10.1016/j.cell.2012.06.024},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodis et al. - 2012 - A Landscape of Driver Mutations in Melanoma.pdf:pdf},
journal = {Cell},
pages = {251--263},
title = {{A Landscape of Driver Mutations in Melanoma}},
url = {http://dx.doi.org/10.1016/j.cell.2012.06.024},
volume = {150},
year = {2012}
}
@article{Challis2013,
abstract = {We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the fol-lowing novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate in-ference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.},
author = {Challis, Edward and {Barber DBARBER}, David},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Challis, Barber DBARBER - 2013 - Gaussian Kullback-Leibler Approximate Inference.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Gaussian processes,active learning,experimental design,generalised linear models,large scale inference,latent linear models,sparse learning,variational approximate inference},
pages = {2239--2286},
title = {{Gaussian Kullback-Leibler Approximate Inference}},
volume = {14},
year = {2013}
}
@article{Ghosh2006,
abstract = {In modeling defect counts collected from an established manufacturing processes, there are usually a relatively large number of zeros (non-defects). The commonly used models such as Poisson or Geometric distributions can underestimate the zero-defect probability and hence make it difficult to identify significant covariate effects to improve production quality. This article introduces a flexible class of zero inflated models which includes other familiar models such as the Zero Inflated Poisson (ZIP) models, as special cases. A Bayesian estimation method is developed as an alternative to tra-ditionally used maximum likelihood based methods to analyze such data. Simulation studies show that the proposed method has better finite sample performance than the classical method with tighter interval estimates and better coverage probabilities. A real-life data set is analyzed to illustrate the practicability of the proposed method easily implemented using WinBUGS.},
author = {Ghosh, Sujit K and Mukhopadhyay, Pabak and Lu, Jye-Chyi},
doi = {10.1016/j.jspi.2004.10.008},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosh, Mukhopadhyay, Lu - 2006 - Bayesian analysis of zero-inflated regression models.pdf:pdf},
journal = {Journal of Statistical Planning and Inference},
keywords = {62E10,62F15,62P30 Keywords,Bayesian inference,Data augmentation,Gibbs sampling,MSC,Markov chain Monte Carlo,WinBUGS,Zero-inflated power series models},
pages = {1360--1375},
title = {{Bayesian analysis of zero-inflated regression models}},
url = {www.elsevier.com/locate/jspi},
volume = {136},
year = {2006}
}
@article{Ruli2013,
author = {Ruli, Erlis and Ventura, Laura},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruli, Ventura - 2013 - Modern Bayesian Inference in Zero-Inflated Poisson Models.pdf:pdf},
keywords = {asymptotic expansions,count data,likelihood,matching prior,modified profile,nuisance parameter,tail area probability,zip regression},
title = {{Modern Bayesian Inference in Zero-Inflated Poisson Models}},
year = {2013}
}
