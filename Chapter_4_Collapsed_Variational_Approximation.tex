\documentclass{amsart}[12pt]
% \documentclass{usydthesis}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{color}

\title{Collapsed Variational Approximation}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}

\newcommand{\mgc}[1]{{\color{blue}#1}}
\newcommand{\joc}[1]{{\color{red}#1}}

\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Abstract}

Bayesian model selection is computationally expensive, and prone to getting stuck in local minima if the
posterior likelihood is multi-modal. This issue is particularly acute if the spike-and-slab prior,
particularly popular for Bayesian model selection, is used. We seek to address both problems by proposing a
population non-parametric Variational Bayes approximation algorithm - a population-based optimisation
strategy. Maintaining a population allows the posterior distribution to be explored more thoroughly, finding
multiple maxima. The variational approximation's lower bound includes an entropy term which ensures diversity
in the population by penalising similarity (the particles repel each other). This ensures the high probability
regions of the posterior distribution is thoroughly explored. This better reflects model selection
uncertainty.

\section{Introduction}

Bayesian model selection is a powerful set of techniques for model selection. These techniques are especially
useful in problems of high-dimension, such as bioinformatics problems where the model space is complex and
the optimal model is difficult for statisticians to manually specify.

In this article, we focus on the important case of model selection for normal linear models
\[
	p(\vy | \valpha, \vbeta) = \N_n(\vone^\top \alpha + \mX \vbeta, \mI_n)
\]
% Justify choice of prior

We use the Cake variant of the spike and slab prior for model selection, as it avoids the Lindley and Bartlett
paradoxes. Citation.

Many computational schemes for Bayesian model selection exist, using Monte Carlo Markov Chains techniques for 
computing the posterior distributions of $\vgamma$.
However, these schemes are both computationally intensive and can become trapped in local maxima of
the posterior distribution if the distribution is high-dimensional and multi-modal, as is the case with
popular choices of prior for Bayesian model selection problems, such as spike-and-slab priors. The difficulty
of becoming trapped in local maxima can be partially mitigated by using population-based MCMC schemes such
as Jasra et al. 2007, Bottolo and Richardson 2010, Hans et al 2007, Liang and Wong 2000. However, this
increases the computational cost of sampling from the posterior distributions still further, especially in
high-dimensional problems.

\cite{Mitchell1988} initially proposed the spike-and-slab prior distribution on regression co-efficients not
currently included in the model -- which places a mixture of a point mass 'spike' at $0$ and a diffuse uniform
distribution 'slab' elsewhere. The random error component is assigned a normal distribution with mean $0$ and
standard deviation $\sigma$. The approach was further developed by \cite{Madigan1994} to incorporate an
alternative Bayesian approach that takes full account of the true model uncertainty by averaging over a small
subset of models, and an efficient search algorithm for finding these models. \cite{George1997} investigated
computational methods for posterior evaluation and exploration in this setting, and using Gray Code sequencing
and Markov Chain Monte Carlo to explore the model space in moderate and large-sized problems respectively.
More recently, \cite{Ishwaran2005} developed a rescaled spike-and-slab model improves effective variable
selection in terms of risk misclassification by using selective shrinkage.

Existing approaches to the problem of model selection focus upon finding a single best model quickly (\cite{You2014},
\cite{Rockova2014}). Exploring the model space using only one model at a time will provide a misleading view
of the uncertainty in the posterior, as it is typically highly multimodal.

Our main contributions are:

i) Search over the binary strings $\vgamma$ directly, as the estimates of $\vbeta$ are available in closed
form once $\vgamma$ is known.

ii) We make use of a population--based optimisation scheme to search the model space. We take advantage of the
population of solutions by incorporating a penalty for lack of entropy.

iii) The entire trajectory of particles gives far more information about variable selection than a single
snapshot of the final best decision.

iv) Our model incorporates the Cake prior, which avoids Lindley's and Bartlett's paradoxes by selecting the
prior based on sample size.

\cite{Rockova2016}
We propose a non-parametric Variational Bayes approximation. Rather than searching for a single optimal model,
we instead maintain a population of models (particles). This allows us to explore more of the posterior model
space, gaining a better estimate of the variation of the model space than an approximation involving only a
single model. It also allows the particles to ``interact'', searching for the essential posterior modes
together. In our variational approximation, this is done by incorporating an ``entropy term'' in the
variational lower bound, which ensures diversity amongst the models in the population, preventing all
particles from simply seeking the global posterior modes. This algorithm is determininistic, and can be
executed efficiently using rank one updates and downdates.

Difficulty of implementation -- practical model selection
\cite{Chipman2014}

% Collapsed Variational Bayes
% Collapsed Variational Bayes approaches have proved useful in Bayesian nonparametric settings such as Latent
% Dirichlet Allocation \cite{Teh2007}
Our variational approximation is a fixed form parametric approximation which places a weight on each covariate
\[
	q(\vgamma) = \sum_{i=1}^p w_i \I(\vgamma_i)
\]

% Population-based MCMC approaches

This article is organised as follows. In Section 2, we detail the derivation of our approximation and fitting
algorithms. In Section 3, we discuss computational issues with implementing our algorithm efficiently. In
Section 4, we present the results of our numerical experiments. In Section 5, we present our conclusions and
discuss our results.

\section{Method}

Base on John's CVB for Mark document.

The main body of the algorithm is a two--stage process. In the first stage, we iterate through the population of
bitstrings, using a greedy search strategy to attempt to alter each bit in the model bitstring to increase the log likelihood. If the log likelihood for the new bitstring is no higher than the previous bitstring, then the
alteration is rejected and the next alteration tried. The alterations are also rejected if the new bitstring
already exists within the population, ensuring that the constraint that all models in the population are
unique is maintained.

In the second stage, we re--calculate the weights for each individual in the population, based on the
likelihood of that model relative to the data $p(\vy; \vbeta_\vgamma)$ and the use this to re--calculate the
probability--based weights $w_i$ for each bitstring in the population. This is then used to re--calculate the
lower bound
\[
	\log \underline{p}(\vy; \vw, \Gamma) = \sum_{k=1}^K \vw_k \log p(\vy; \vbeta_{\vgamma_k}) - \vw_k \log \vw_k
\]

which is the sum of the weighted log-likelihood of the population and the entropy of the probability weights.
These two stages repeat until the lower bound converges.

\section{Results}



\section{Implementation techniques}
To ensure uniqueness of the $K$ models in the population, before a new candidate model with a covariate added
or removed is considered, the population of existing models is checked to see if it already exists in the
population. If so, the addition or removal of the covariate is skipped and the next candidate model considered.
In our implementation, this check is made computationally efficient by maintaining a hash table of the models,
allowing the check as to wether the model is already in the population to be performed in $\BigO(1)$ time.

\bibliographystyle{elsarticle-harv}
\bibliography{references_mendeley}

\end{document}
