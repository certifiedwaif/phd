\documentclass{amsart}[12pt]
% \documentclass{usydthesis}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{color}

\title{Preliminaries}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}

\newcommand{\mgc}[1]{{\color{blue}#1}}
\newcommand{\joc}[1]{{\color{red}#1}}

\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Introduction}

Start off with a summary of the Particle Em for Variable Selection paper by Veronika Rockova

Introduction
EM vulnerable to local entrapment when likelihood is multi-modal, especially spike-and-slab posterior
distributions for model selection
non-parametric Variational Bayes, repulsive particles. These particles are geared towards unchartered areas of
the posterior, providing a more comprehensive summary of its' topography than simple parallel EM deployments
Report of a single model will be a misleading reflection of the model uncertainty in a highly multimodal
posterior. Identify a collection of representative models
What's our choice of prior?
Hard to explore model space and escape local entrapment, especially in high dimensions
Population-based methods, allow trajectories to ``communicate''

Main contributions
As a precursor to Particle Em, we propose the Reversed EMVS algorithm, a discrete optimisation approach for
spike-and-slab variable selection. Treat $\vgamma$ as parameters of interest, and $\vbeta$ as missing data.
Closed form updates, targeting the discrete model space.

Population-based optimisation approach that exhibits both individual and social behaviour. Particles share a
common goal (finding essential posterior modes) and realise it by exploring the posterior environment while
mutually interacting. Social behaviour is mediated through entropy, which serves as a diversifying penalty.

Entire trajectory of evolving posteriors is far more informative for variable selection

By forging connections between Particle EM and sequential Monte Carlo (SMC) procedures, we propose a
stochastic Particle EM variant

Visualisation

Approximate Bayesian inference
Variational Approximation
Variational Bayes
Collapsed Variational Bayes

My writing

\section{Abstract}

Bayesian model selection is computationally expensive, and prone to getting stuck in local minima if the
posterior likelihood is multi-modal. This issue is particularly acute if the spike-and-slab prior,
particularly popular for Bayesian model selection, is used. We seek to address both problems by proposing a
population non-parametric Variational Bayes approximation algorithm - a population-based optimisation
strategy. Maintaining a population allows the posterior distribution to be explored more thoroughly, finding
multiple maxima. The variational approximation's lower bound includes an entropy term which ensures diversity
in the population by penalising similarity (the particles repel each other). This ensures the high probability
regions of the posterior distribution is thoroughly explored. This better reflects model selection
uncertainty.

\section{Introduction}

Bayesian model selection is a powerful set of techniques for model selection. These techniques are especially
useful in problems of high-dimension, such as bioinformatics problems where the model space is complex and
the optimal model is difficult for statisticians to manually specify.

In this article, we focus on the important case of model selection for normal linear models
\[
	p(\vy | \valpha, \vbeta) = \N_n(\vone^\top \alpha + \mX \vbeta, \mI_n)
\]
% Justify choice of prior
We use a g-prior for model selection (\cite{Goel1986}), as it avoids the Lindley and Bartlett paradoxes, as 
described in \cite{Liang2008}.

Many computational schemes for Bayesian model selection exist, using Monte Carlo Markov Chains techniques for 
computing the posterior distributions of $\vgamma$.
However, these schemes are both computationally intensive and can become trapped in local maxima of
the posterior distribution if the distribution is high-dimensional and multi-modal, as is the case with
popular choices of prior for Bayesian model selection problems, such as spike-and-slab priors. The difficulty
of becoming trapped in local maxima can be partially mitigated by using population-based MCMC schemes such
as Jasra et al. 2007, Bottolo and Richardson 2010, Hans et al 2007, Liang and Wong 2000. However, this
increases the computational cost of sampling from the posterior distributions still further, especially in
high-dimensional problems.

Existing approaches to the problem focus upon finding a single best model quickly (\cite{You2014},
\cite{Rockova2014}). Exploring the model space using only one model at a time will provide a misleading view
of the uncertainty in the posterior, as it is typically highly multimodal.

\cite{Rockova2016}
We propose a non-parametric Variational Bayes approximation. Rather than searching for a single optimal model,
we instead maintain a population of models (particles). This allows us to explore more of the posterior model
space, gaining a better estimate of the variation of the model space than an approximation involving only a
single model. It also allows the particles to ``interact'', searching for the essential posterior modes
together. In our variational approximation, this is done by incorporating an ``entropy term'' in the
variational lower bound, which ensures diversity amongst the models in the population, preventing all
particles from simply seeking the global posterior modes. This algorithm is determininistic, and can be
executed efficiently using rank one updates and downdates.

Difficulty of implementation -- practical model selection
\cite{Chipman2014}

% Collapsed Variational Bayes
% Collapsed Variational Bayes approaches have proved useful in Bayesian nonparametric settings such as Latent
% Dirichlet Allocation \cite{Teh2007}
Our variational approximation is a fixed form parametric approximation which places a weight on each covariate
\[
	q(\vgamma) = \sum_{i=1}^p w_i \I(\vgamma_i)
\]

% Population-based MCMC approaches

This article is organised as follows.

\section{Results}


\section{Implementation techniques}
To ensure uniqueness of the $K$ models in the population, before a new candidate model with a covariate added
or removed is considered, the population of existing models is checked to see if it already exists in the
population. If so, the addition or removal of the covariate is skipped and the next candidate model considered.
In our implementation, this check is made computationally efficient by maintaining a hash table of the models,
allowing the check as to wether the model is already in the population to be performed in $\BigO(1)$ time.

\bibliographystyle{elsarticle-harv}
\bibliography{references_mendeley}

\end{document}
