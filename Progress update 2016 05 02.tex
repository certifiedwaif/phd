\documentclass{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{tikz}
\input{include.tex}
\input{Definitions.tex}

\usefonttheme{serif}

\title{Progress update}
\author{Mark Greenaway}

\mode<presentation>
{ \usetheme{boxes} }

\begin{document}
\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Mean field updates and variational lower bound \ldots nearly done}
\begin{itemize}
\item The mean field updates are done
\item These turn out to be really nice, everything is in terms of $\tau_g = \E [g^{-1}]$
			and fixed constants $n$, $\|\vy\|^2$ and $R^2$
\item We already have parallel code	to calculate $R^2$
\item This allows us to do Variational Bayes by optimising one variational parameter. So we can do them all 
			at once!
\item Variational lower bound is pretty much done
\item I'm writing, \textbf{then} coding -- I have a thesis to produce
\item John is admonishing me on my writing style, which is probably for the best
\item He also complimented me on reading some papers of my own volition, so that was nice
\item Structured Variational Bayes
\end{itemize}
\end{frame}

\begin{frame}{Model selection algorithms. Maybe brute force isn't so bad after all}
\begin{itemize}
\item We're performing all $2^p$ model fits, so our algorithm is $\BigO(2^p n p^2)$ - which is to say,
			exponential time complexity
\item That sounded terrible to me. So what are the alternatives?
\item Branch and bound, as implemented in \texttt{leaps}. Worst case is still $\BigO(2^p n p^2)$. Pushing it
			to do $p = 30$
\item Mixed Integer Optimisation - still NP hard
\item Step selection uses a greedy algorithm -- but this doesn't select optimal models because there is no
			guaranteed ``safe move''. So you get stuck in local optima
\item No silver bullet -- to my knowledge there is \textbf{no} polynomial time algorithm known which
			will perform this task
\item So I might think about this more later
\item Whatever algorithm we use, we're going to need a lot of computational resources for large problems
\end{itemize}
\end{frame}

\begin{frame}{The End of Moore's Law}
\begin{itemize}
\item For a long time, computers have been getting faster and faster, the famous
			Moore's Law -- ``the number of components per integrated circuit doubled roughly every 18 months'' -- 
			has held from 1965 to just about now
\item This used to be great. If something was too slow, you'd just wait for the next fastest CPU to come out
			and just re-run it
\item This is now over, as the physical limits of silicon are being reached -- particularly the power wall. 
			The semiconductor industry has abandoned Moore's Law as a target
\item So now what?!? No, really, I'm asking \textbf{you}
\item Multi-core, GPUs and distributed computing are partial answers. These are great for
			linear algebra/vectorisable algorithms
\item But many algorithms aren't like that e.g. iterative algorithms where every step depends on the 
			result of the previous step
\end{itemize}
\end{frame}

\begin{frame}{Parallelism in R -- not there yet}
\begin{itemize}
\item R was developed in a single-core world, with a single-core design
\item In particular, R manages memory using a reference counting garbage collector. This is inherently
			not thread safe
\item This garbage collection design is in nearly every R package. To change this, they would all have to
			be rewritten
\item R's support for parallelism are based on hacks -- \texttt{mclapply} and \texttt{\%dopar\%}. No true 
			shared memory concurrency
\item Much of this	support is experimental, and as Dario has found some of it doesn't work at all
\item The bad news is you might be waiting a long time for parallelism to be supported fully in R
\item I'm not waiting. I'm using languages that support parallelism and can be called from R
\item For most of us, the future belongs to something like Julia. Maybe a version of R that incorporates
			those ideas?
\end{itemize}
\end{frame}

\begin{frame}{Research servers and HPC}
\begin{itemize}
\item About a quarter of our research servers are currently down due to air conditioner failure.
			Paul Szabo and Dr	Mathas are aware of this
\item HPC typically has 15 to 20 nodes free at any one time -- 24 cores each, so I'll just use that
\item GPU nodes are also available -- no need to wait for hardware to be installed. It's ready now
\item OpenACC is already supported in GCC. Only minor changes to my code would be needed to support this
\item OpenMP supported on GPUs in Clang already, and in will be in GCC in 6.x. Even less code changes
\end{itemize}
\end{frame}

\end{document}