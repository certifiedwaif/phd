\documentclass{amsart}
\newcommand{\Bernoulli}[1]{\text{Bernoulli} \left( #1 \right)}
\newcommand{\half}{\frac{1}{2}}
\usepackage{amsmath}
\usepackage{amssymb}
\begin{document}
\section{Variational approximations}
Variational approximations to Bayesian models are an extension of the EM algorithm,
where each parameter in the model is treated as latent and unobserved in turn, and is
estimated conditional on all of the other parameters.

\section{\S 21 Variational inference}

From Machine Learning: A Probabilistic Perspective,

For some simple two node graphical models, of the form $x \to \mathcal{D}$, we can compute
the exact posterior $p\vec{x}|\mathcal{D})$ in closed form, provided the prior $p(\vec{x})$ 
is conjugate to the likelihood, $p(\mathcal{D}|\vec{x})$ (which means the likelihood must
be in the exponential family). See Chapter 5 for some examples of this.

In more general settings, we must use approixmate inference methods. In \S 8.4.1, we
discussed the Gaussian approximation, which is useful for inference in two node models of
the form $\vec{x} \to \mathcal{D}$, where the prior is not conjugate. (For example, 
\S 8.4.3 applied to the method of logistic regression).

The Gaussian approximation is simple. However, some posteriors are not naturally modelled as
Gaussians. For example, when inferring multinomial parameters, a Dirichlet
distribution is a better choice, and when inferring states in a discrete graphical model,
a categorical distribution is a better choice.

In this chapter, we will study a more general class of deterministic approximate inference
algorithms based on \emph{variational inference} (Jordan et al. 1998; Jaakda and Jordan 
2000; Jaakkola 2001; Wainwright and Jordan 2008). The basic idea is to pick an approximation
as close as possible to the true posterior,
$p^{*}(\vec{x}) \text{def} = p(\vec{x}|\mathcal{D})$. This reduces inference to an 
optimisation problem. By relaxing the constraints and/or approximating the objective, we can
trade accuracy for speed. The bottom line is that variational inference often gives us the
speed benefirts of MAP estimation but the statistical benefits of the Bayesian approach.

\subsection{\S 21.2 Variational inference}

Suppose $p^{8}(\vec{x})$ is our true but intractable distribution, and $q(\vec{x})$ is some 
approximation, chosen from some tractable family, such as a multivariate Gaussian or a
factored distributuion. We assume $q$ has some free parameters which we want to optimise,
so as to make q ``similiar to'' p.

An obvious cost function to try is the KL divergence:
\[
	KL(p^{*}||q) = \sum_x p^{*}(x) \log{\frac{p^{*}(x)}{q(x)}}
\]

However, this is hard to compute, since taking expectations with respect to $p^{*}$ is
presumed to be intractable. A natural alternative is the reverse KL divergence:
\[
	KL(q||p^{*}) = \sum_x p^{*}(x) \log{\frac{q(x)}{p^{*}(x)}}
\]

The main advantage of this objective is that computing expectations wrt $p^{*}$ is assumed
to be intractable. We discuss the statistical differences between these two objects in
\S 21.2.2. % which I should summarise

Unfortunately, Equation 21.2 is still no tractable as written, since even evaluating
$p^{*}(\vec{x}) = p(\vec{x} | \mathcal{D})$ pointwise is hard, since it requires evaluating
the intractable normalisation constant $Z=p(\mathcal{D})$. However, usually the unnormalised
distribution $\tilde{p}(\vec{x}) def = p(\vec{x}, \mathcal{D}) = p^{*}(\vec{x} Z$ is
tracable to compule. We thereore define our new objective function as follows:
\[
J(q) def = KL(q||\tilde{p})
\]

where we are slightly abusing notation, since $\tilde{p}$ is not a normalised distribution.
Plugging in the definition of KL, we get
\begin{align*}
J(q) &= \sum_x q(x) \log{\frac{q(x)}{\tilde{p}(x)}} \\
&= \sum_x q(x) \log{\frac{q(x)}{Z p^{*}(x)}} \\
&= \sum_x q(x) \log{\frac{q(x)}{p^{*}(x)}} - \log{Z} \\
&= KL(q || p^{*}) - \log{Z}
\end{align*}

Since $Z$ is a constant, by minimising $J(q)$, we will force $q$ to become close to
$p^{*}$.

Since KL divergence is always non-negative, we see that $J(q)$ is an upper bound on the
negative log likelihood:
\[
J(q) = KL(q||p^{*}) - \log{Z} \geq -\log{Z} = -\log{p(\mathcal{D})}
\]

Alternately, we can try to \emph{maximise} the following quantity (in Koller and Friedman
2009, this is referred to as the \emph{energy functional}), which is the lower bound on
the log likelihood of the data:
\[
L(q) def = -J(q) = -KL(q||p^{*}) + \log{Z} \leq \log{Z} = \log{p(\mathcal{D})}
\]

Since this bound is tight (what does this mean?!? - Mark) when $q=p^{*}$, we see that the
variational inference is closely related to EM (see \S 11.4.7).

% This seems so obvious now. If you take the expectation of a variable in the factored
% approximation with respect to every other variable, the only terms involved in that
% expectation will be the ones involving the variable you're currently maximising.

% But it's important that you work through the details of this.
\subsection{\S 21.3 The mean field method}

\subsection{Univariate Gaussian}
% Approximate as y~N(\mu, \lambda \sigma^2|\lambda), \lambda~Gamma(\alpha, \beta)
% Factorise q(\mu, \sigma) = q(\mu)q(sigma), with \lambda as a prior
% Write out the full likelihood, then figure out how to maximise q(\mu) and q(\sigma) seperately
% For q(\mu), completing the square will give you the correct form with the expectation of
% \sigma as a term.
% For q(\sigma), I think from memory you can get the term easily enough as Gamma is a conjugate
% prior for a normal distribution. Again, the expectation of \mu is involved.
% These calculations turn out to be very like calculating full conditionals for Gibbs sampling,
% but with expectations used in place of the current value of the paramteres. But this is the
% sense in which the structure of each algorithm is the same.

\subsubsection{Lower bound}
% I still haven't really figured out a good technique for calculating these.

\subsection{Linear mixed model}
% The structure is a little more complicated, but the thinking is not.
% Factorise the approximation q(\mu, \Sigma) into q(\mu, Sigma) q(u_1, u_2, \ldots, u_n)
% The process we follow from here is much like that for the Univariate Gaussian case.

\subsubsection{Lower bound}

\subsection{Probit regression}
% This was the first model where I tried to do some of the calculations for the
% likelihood, mean field updates and lower bound myself. I didn't get them all
% right, but I got close.
Let $Y_i|\beta_0, \beta_1, \ldots, \beta_k \sim \Bernoulli{\Psi(\beta_0 + \beta_1 x_{1i} + \ldots + \beta_k x_{ki})}$, and $\beta \sim N(\mu_\beta, \Sigma_\beta)$.

Letting $X = [1 x_{1i} \ldots x_{ki}]$, the likelihood can be written compactly as
\[
p(\vec{y}|\vec{\beta}) = \Psi(X \beta)^{\vec{y}} \Psi(1 - \Psi(X \beta))^{\vec{1} - \vec{y}}
\]]

Question: Why don't we just optimise $\beta$ directly in this case?

Introduce a vector of auxillary variables, $\vec{a} = (a_1, \ldots, a_n)$, where
\[
a_i | \vec{\beta} \sim N((X\beta)_i, 1)
\]

Then we can write
\[
p(y_i|a_i) = I(a_i \geq 0)^y_i I(a_i < 0)^{1-y_i}
\]

$\beta -> y$

$\beta -> a -> y$

So then we can write the likelihood for each individual datum as
\begin{align*}
	p(y_i, \sigma) &= p(y_i|a_i) p(a_i|\vec{\beta}) p(\vec{\beta}) \\
	&= \Psi(a_i \geq 0)^{y_i} \Psi(a_i < 0)^{1 - y_i} \frac{1}{\sqrt{2 \pi}} \exp{- \frac{(a_i - X \beta)^2}{2}} \left( 2 \pi \right)^{-\frac{k}{2}} |\Sigma|^{-\frac{1}{2}} \exp{-\frac{1}{2} (\beta - \mu_\beta)^T \Sigma^{-1} (\beta - \mu_\beta)}
\end{align*}

To maximise this function, we need only maximise the terms of the log likelihood
involving $a_i$ or $\beta$. That is, we maximise

\begin{align*}
\log{p(y_i, a_i, \vec{\beta})} =& y_i \log{(\Psi(a_i \geq X \beta))} + (1 - y_i) \log{(\Psi(a_i < X \beta))} + \\
&- \frac{(a_i - X \beta)^2}{2} - \frac{(\beta - \mu_\beta)^T \Sigma^{-1} (\beta - \mu_\beta)}{2}
\end{align*}

Adopting the factored approximation
\[
q(\theta) = q_a(\vec{a}) q_{\vec{\beta}}(\vec{\beta})
\]

we find the distribution of $q_a$ and $q_\beta$.

%\begin{align*}
%q_a^*(a) \propto& \exp{E_\beta \log{p(y, a, \beta)}} \\
%=& \exp{\sum_{i=1}^n E_\beta \left( y_i \log{\Psi{a_i \geq 0}} + (1 - y_i) \log{\Psi{a_i < %0}} \right) - \frac{1}{2}(a_i - X \mu_{q(\beta)})^2 + \\ &\frac{1}{2}(\mu_{q(\beta)} - %\mu_{\beta})^T \Sigma^{-1} (\mu_{q(\beta)} - \mu_\beta)  }
%\end{align*}

We then calculate the mean field update equations.

\[
q_a^*(a) \propto \exp{E_\beta \log{p(y, a, \beta)}}
\]

TODO: Truncated normal distribution for $a_i$

TODO: Normal distribution for $q_\beta$

Meeting with John - 16/09/2013

John stressed that we must always identify the distributions first

The trick for finding the distribution for $q_a^*(a)$ is to consider the cases where
$y_i=1$ and $y_i=0$ seperately.

\begin{align*}
p(y_i=1, a_i, \beta) &\propto \exp{E_{-a_i} \left[\frac{-(a_i - X \beta)^2}{2}\right]} \\
&\propto \exp{E_{-a_i}\left[\frac{-(a_i^2 - a_i x_i^T \beta)^2}{2}\right]}, a_i \geq 0 \\
&= TN_+(\mu_{q(a_i)}, 1)
\end{align*}

When $y_i=0$, $p(y_i=0|a_i, \beta)$, $q(a_i) = TN_{-} (\mu_{q(a_i)}, 1)$.

\[
q(\beta) \propto \exp{\left( \frac{-\beta^T[X^T X + \Sigma_{\beta}^{-1}]\beta}{2} + (X^T \mu_{q(a)} + \Sigma_{\beta}^{-1} \mu_{\beta} )^T \beta \right)}
\]

If $q(\beta) \propto \exp{\frac{1}{2}\beta^T A \beta + \beta^T \beta}$ then $q(\beta) = N(A^{-1}b, A^{-1})$

So the above equals $N([X^T + \Sigma_{\beta}^{-1}]^{-1} [X^T \mu_{q(a)} + \Sigma_{\beta}^{-1}]^{-1})$

Expectation of a truncated normal distribution.

\begin{align*}
q(a_i) &= TN_{+}(x_i^T \mu_{q(\beta)}, 1), \text{ when } y_i=1 \\
&=TN_{-}(x_i^T \mu_{q(\beta)}, 1), \text{ when } y_i=0 
\end{align*}

More zeta functions
\[
\zeta_k(x) = \frac{\partial^k \log{\Psi(x)}}{\partial x^k}
\]

so

\[
\zeta_1 = \frac{\psi(x)}{\Psi(x)}
\]

The $sn$ library calculates this in a numerically stable manner, using continued fractions.
John has better code, which is written in C and calculates the function to a numerical
threshold.

\section{Finite normal mixture model}
Let $X_1, X_2, \ldots, X_n$ be a univariate sample that is modelled as a random sample from a
mixture of K normal density functions with parameters $(\mu_k, \sigma_k^2)$,
$1 \leq k \leq K$. The joint density function of the sample is
\begin{equation}\label{eq:gaussian_mixture}
p(x_1, \ldots, x_n) = \Pi_{i=1}^n \left [ \sum_{k=1}^K w_k \frac{1}{\sqrt{2 \pi}} \exp{ \left( -\frac{1}{2}(x_i - \mu_k)^2/\sigma_k^2 \right)} \right ]
\end{equation}
where the weights $w_k$, $1 \leq k \leq K$, are nonnegative and sum to unity. Let 
$(w_1, \ldots, w_k) \sim \text{Dirichlet($\alpha, \ldots, \alpha$)}$, $\alpha > 0$.

% TODO: Start working through the details.

As before, we introduce an auxillary node
$a_1, \ldots, a_n|w_1, \ldots, w_k \sim \text{Multinomial}(1, \vec{\alpha})$. Then

\[
p(y_i|a_i) = \Pi_{i=1}^n \Pi_{k=1}^K \left [ \frac{1}{\sqrt{2 \pi}} \exp{ \left ( \frac{-(x_i - \mu_k)^2}{2 \sigma_k^2} \right )} \right ] ^{a_i}.
\]

\subsection{The likelihood and log-likelihood}
The joint likelihood can be specified as the product of conditional likelihoods

\[
p(x;q) = \Pi_{i=1}^n \Pi_{k=1}^K p(x_i|a_{i1}, \ldots, a_{iK}) p(a_{i1}, \ldots, a_{iK} | w_1, \ldots, w_K) p(w_1, \ldots, w_K) p(\mu_k) p(\sigma_k^2)
\]

It follows that the joint likelihood is

\[
\sum_{i=1}^n \sum_{k=1}^K \log{p(x_i|\vec{a_i})} + \log{p(\vec{a_i}|\vec{w})} + \log{p(\vec{w})} + \log{p(\mu_k)} + \log{p(\sigma_k^2)}
\]

where

\begin{align*}
\sum_{i=1}^n \sum_{k=1}^K \log{p(x_i|\vec{a_i})} &= \sum_{i=1}^n \sum_{k=1}^K - \half \log{2 \pi} - \half \log{\sigma_k^2} - \half a_{ik} \frac{(x_i - \mu_k)^2}{\sigma_k^2} \\
&= - \half Kn \log{2 \pi} - \half \sum_{k=1}^K n \log{\sigma_k^2} - \half \sum_{i=1}^n \sum_{k=1}^K a_{ik} \frac{(x_i - \mu_k)^2}{\sigma_k^2}
\end{align*}

\begin{align*}
\sum_{i=1}^n \sum_{k=1}^K \log{p(\vec{a_i}|\vec{w})} &= \sum_{i=1}^n \sum_{k=1}^K \log{(\Pi w_k^{a_{ik}})} \\
&= \sum_{i=1}^n \sum_{k=1}^K a_{ik} \log{w_k}
\end{align*}

\begin{align*}
\log{p(\vec{w})} = \log{\left( \frac{\Gamma(\sum_{k=1}^K \alpha)}{\Pi_{k=1}^K \Gamma(\alpha)} \Pi_{k=1}^K w_k^{\alpha-1} \right)}
\end{align*}

\begin{align*}
\log{p(\mu_k)} &= \log{ \frac{1}{\sqrt{2 \pi \sigma_{\mu_k}^2}} \exp{\left( -\frac{(\mu_k - \mu_{\mu_k})^2}{2 \sigma_{\mu_{\mu_k}^2}} \right)}} \\
&= \half \log{2 \pi} - \half \frac{(\mu_k - \mu_{\mu_k})^2}{2 \sigma_{\mu_{\mu_k}^2}}
\end{align*}

and

\begin{align*}
\log{p(\sigma_k^2)} &= \log{\left [ \frac{B_k^{A_k}}{\Gamma(A_k)} \sigma_k^{2 (-(A_k - 1))} \exp{-\frac{B_k}{\sigma_k^2}} \right ]}
\end{align*}

\subsection{Mean field update equations}
$\log{E_{-a_{ik}} \log{p(x_{ij}, \theta)}} = $

\subsection{Lower bound}


\end{document}
