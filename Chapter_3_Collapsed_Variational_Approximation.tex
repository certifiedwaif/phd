\documentclass{amsart}[12pt]
% \documentclass{usydthesis}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{color}

\title{Collapsed Variational Approximation}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}

\newcommand{\mgc}[1]{{\color{blue}#1}}
\newcommand{\joc}[1]{{\color{red}#1}}

\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Abstract}

Bayesian model selection is computationally expensive, and prone to getting stuck in local minima if the
posterior likelihood is multi-modal. This issue is particularly acute if the spike-and-slab prior,
particularly popular for Bayesian model selection, is used. We seek to address both problems by proposing a
population non-parametric Variational Bayes approximation algorithm - a population-based optimisation
strategy. Maintaining a population allows the posterior distribution to be explored more thoroughly, finding
multiple maxima. The variational approximation's lower bound includes an entropy term which ensures diversity
in the population by penalising similarity (the particles repel each other). This ensures the high probability
regions of the posterior distribution is thoroughly explored. This better reflects model selection
uncertainty.

\section{Introduction}

Bayesian model selection is a powerful set of techniques for model selection. These techniques are especially
useful in problems of high-dimension, such as bioinformatics problems where the model space is complex and
the optimal model is difficult for statisticians to manually specify.

In this article, we focus on the important case of model selection for normal linear models
\[
	p(\vy | \valpha, \vbeta) = \N_n(\vone^\top \alpha + \mX \vbeta, \mI_n)
\]
% Justify choice of prior

\cite{Mitchell1988} initially proposed the spike-and-slab prior distribution on regression co-efficients not
currently included in the model -- which places a mixture of a point mass 'spike' at $0$ and a diffuse uniform
distribution 'slab' elsewhere. The random error component is assigned a normal distribution with mean $0$ and
standard deviation $\sigma$. The approach was further developed by \cite{Madigan1994} to incorporate an
alternative Bayesian approach that takes full account of the true model uncertainty by averaging over a small
subset of models, and an efficient search algorithm for finding these models. \cite{George1997} investigated
computational methods for posterior evaluation and exploration in this setting, and using Gray Code sequencing
and Markov Chain Monte Carlo to explore the model space in moderate and large-sized problems respectively.
More recently, \cite{Ishwaran2005} developed a rescaled spike-and-slab model which improves effective variable
selection in terms of risk misclassification by using selective shrinkage. We further extend this approach, by
using the Cake variant of the spike-and-slab prior for model selection, as it avoids the Lindley and Bartlett
paradoxes. Citation.

Existing approaches to the problem of model selection focus upon finding a single best model quickly
(\cite{You2014}, \cite{Rockova2014}). Exploring the model space using only one model at a time will provide a
misleading view of the uncertainty in the posterior, as it is typically highly multimodal.

Many computational schemes for Bayesian model selection exist, using Monte Carlo Markov Chains techniques for 
computing the posterior distributions of $\vgamma$.
However, these schemes are both computationally intensive and can become trapped in local maxima of
the posterior distribution if the distribution is high-dimensional and multi-modal, as is the case with
popular choices of prior for Bayesian model selection problems, such as spike-and-slab priors. The difficulty
of becoming trapped in local maxima can be partially mitigated by using population-based MCMC schemes such
as Jasra et al. 2007, Bottolo and Richardson 2010, Hans et al 2007, Liang and Wong 2000. However, this
increases the computational cost of sampling from the posterior distributions still further, especially in
high-dimensional problems.

\cite{Rockova2016} introduced the notion of Particle EM. Rather than searching for a single optimal model,
Particle EM instead maintains a population of models (particles). This allows the algorithm to explore more of
the posterior model space, gaining a better estimate of the variation of the model space than an algorithm
involving only a single model. It also allows the particles to ``interact'', searching for the essential
posterior modes together. In Particle EM, this is done by incorporating an ``entropy term'' in the variational
lower bound, which ensures diversity amongst the models in the population, preventing all particles from
simply seeking the global posterior modes. The algorithm is determininistic.

We build upon this work by proposing a fixed-form parametric Variational Bayes approximation of $\vgamma$.
We adopt a prior structure incorporating the Cake prior for variable selection, which avoids the Lindley
and Bartlett's paradoxes.
Our fitting algorithm can be executed efficiently using rank-one updates and downdates.

Difficulty of implementation -- practical model selection
\cite{Chipman2014}

% Collapsed Variational Bayes
% Collapsed Variational Bayes approaches have proved useful in Bayesian nonparametric settings such as Latent
% Dirichlet Allocation \cite{Teh2007}
Our variational approximation is a fixed form parametric approximation which places a weight on each covariate
\[
	q(\vgamma) = \sum_{k=1}^K w_k \I(\vgamma_k)
\]

% Population-based MCMC approaches

Our main contributions are:

i) Our algorithm searches over the binary strings $\vgamma$ directly, as the estimates of $\vbeta$ are available 
in closed form once $\vgamma$ is known.

ii) We make use of a population--based optimisation scheme to search the model space. We take advantage of the
population of solutions by incorporating a penalty for lack of entropy, which ensures diversity in the
population of solutions.

iii) The entire trajectory of particles gives far more information about variable selection than a single
snapshot of the final best decision.

iv) Our model incorporates the Cake prior, which avoids Lindley's and Bartlett's paradoxes by selecting the
prior based on sample size.

This article is organised as follows. In Section 2, we detail the derivation of our approximation and fitting
algorithms. In Section 3, we discuss computational issues with implementing our algorithm efficiently. In
Section 4, we present the results of our numerical experiments. In Section 5, we present our conclusions and
discuss our results.

\section{Method}

\subsection{Posterior distributions of $g$-priors in terms of special functions}
% Things we derived


\subsubsection{Calculating $p(\vy | \vgamma)$}
We want to calculate $p(\vy | \vgamma)$ so that we can calculate $p(\vgamma | \vy)$ using Bayes' Rule and rank models against	one another. \\
To do this, we need to calculate
$$p(\vy | \vgamma) = \int p(\vy | \alpha, \vbeta, \sigma^2, \vgamma) p(\alpha) p(\vbeta | g, \vgamma) p(\sigma^2) p(g) d \alpha d \vbeta d \sigma^2 d g.$$
We choose our priors so that this integral is tractable, with help from \cite{Gradshteyn1988}.
The marginal likelihoods behave like BIC. \\
We found the following closed form expressions.
Include derivations in the appendices.
\small
\begin{itemize}
	\item Liang et al. 2008 hyper-$g$ prior \cite{Liang2008}
		$p(\vy | \vgamma) = \frac{K(n) (a - 2)}{p_\vgamma + a  - 2} {}_2 F_1 \left( \frac{n-1}{2}, 1; \frac{p_\vgamma + a}{2}; R_\vgamma^2 \right)$
	\item Liang et al. 2008 hyper-$g/n$ prior \cite{Liang2008}
		$p(\vy | \vgamma) = \frac{K(n) (a - 2)}{n (p_\vgamma + a  - 2)} F_1 \left( 1, \frac{a}{2}, \frac{n-1}{2}; \frac{p_\vgamma + a}{2}; 1 - \frac{1}{n}, R_\vgamma^2 \right)$
\end{itemize}
These expressions are numerically stable to evaluate.

We want to be able to calculate the marginal likelihood of the data given $\vgamma$, so that we can
calculate the posterior probability of $\vgamma$ and rank models against one another. To do this, we
need to calculate the following integral.
We choose our priors so that this integral is tractable, with help from the book Table of Integrals and
Series by Gradshteyn. The marginal likelihoods behave like BIC.
We found closed form expressions for the marginal likelihood for the hyper-$g$ and hyper-$g/n$ priors.

\subsubsection{Calculating $p(\vy | \vgamma)$ for Bayarri's robust prior}
\begin{itemize}
	\item Bayarri calculated the marginal likelihood given $\vgamma$ for the Robust Bayarri prior
	 	in \cite{Bayarri2012} 
		\tiny
		$p(\vy | \vgamma) = K(n) \left(\frac{n + 1}{1 + p_\vgamma}\right)^{-p_\vgamma/2} \frac{(1 - R^2_\vgamma)^{-(n-1)/2}}{p_\vgamma + 1} {}_2 F_1 \left[ \frac{n-1}{2}, \frac{p_\vgamma+1}{2}; \frac{p_\vgamma + 3}{2}; \frac{[1 - 1/(1 - R^2_\vgamma)](p_\vgamma + 1)}{1 + n} \right]$.
	\small
	\item But this expression is not numerically well-behaved, because the second argument of the
				${}_2 F_1$ function is greater than $1$.
	\item Using a trick that's available when ${}_2 F_1(., 1; .; .)$ ... is numerically
				well-behaved, we derived the new expression
		\tiny
		$p(\vy | \vgamma) = K(n) \left(\frac{n + 1}{1 + p_\vgamma}\right)^{(n-p_\vgamma-1)/2} \frac{[1 + L (1 - R^2_\vgamma)]^{-(n-1)/2}}{p_\vgamma + 1} {}_2 F_1 \left[ \frac{n-1}{2}, 1; \frac{p_\vgamma + 3}{2}; 
		\frac{R^2_\vgamma}{1 + L(1 - R^2_\vgamma)} \right]$ \\
		\small
		where $L = (1 + n)/(1 + p_\vgamma) - 1$.
\end{itemize}

Bayarri calculated the marginal likelihood given $\vgamma$ for the Robust Bayarri prior.
But this expression is not numerically well-behaved.
We used Euler's transformation and properties of the hypergeometric function to derive a new,
numerically stable function.


\subsection{Collapsed Variational Approximation}

Suppose that
\[
	q(\vgamma) = \sum_{i=1}^K w_k I(\vgamma = \vgamma_k)
\]
where $0 < w_k < 1$, $\vone^\top \vw = 1$ and $\mGamma = [\vgamma_1, \ldots, \vgamma_K]$ are a population of
models. A variational lower bound for $\log p(\vy; \vw, \mGamma)$ is given by
\begin{align*}
	\log \underline{p}(\vy; \vw, \mGamma) &= \int \exp\left[ \bE_q \log \left\{ \frac{p(\vy, \vtheta, \vgamma)}{q(\vgamma)} \right\} \right] d \vtheta dg \\
	&= \sum_{k=1}^K w_k \log p(\vy, \vgamma_k) - w_k \log w_k
\end{align*}

under the assumption that each of the $\vgamma_1, \ldots, \vgamma_K$ are unique where
\[
	p(\vy, \vgamma_k; h) = \int p(\vy, \vtheta, \vgamma_k) d \vtheta.
\]

Since $\log \underline{p}(\vy; \vw, \mGamma)$ is a lower bound we can maximise this bound with respect to
$\vw$ and $\mGamma$ to make the bound as tight as possible. The optimal value of the $w_k$ satisfy
\begin{align*}
	w_k = \frac{p(\vy, \vgamma_k)}{\sum_{j=1}^K p(\vy, \vgamma_j)} = \frac{p(\vgamma_k) \text{BF}_{\vgamma_k}}{\sum_{j=1}^K p(\vgamma_j) \text{BF}_{\vgamma_j}}.
\end{align*}

Let $\vgamma_{jk}^{i} = (\vgamma_{1k, }, \ldots, \vgamma_{j-1, k}, i, \vgamma_{j+1, k}, \ldots, \vgamma_{pk})^\top$. The algorithm to fit the approximation is given below.
% Re-type this using algorithmic environment
\begin{enumerate}
	\item Loop until convergence.
	\item For $k = 1, \ldots, K$ \\
	For $j = 1, \ldots, p$
	$\vgamma_{jk} = \begin{cases}
	1 &\text{ if } p(\vy, \vgamma_{jk}^{(1)}) > p(\vy, \vgamma_{jk}^{(0)}) \\
	0 & \text{ otherwise}.
	\end{cases}$ \\
	$w_k = p(\vy, \vgamma_k) / \sum_{j=1}^K p(\vy, \vgamma_j)$
\end{enumerate}
In order to make the updates efficiently we need to be able to perform rank-one updates and rank-one downdates for the calculation of $\hat{\sigma}_\vgamma^2$.

The main body of the algorithm is a two--stage process. In the first stage, we iterate through the population of
bitstrings, using a greedy search strategy to attempt to alter each bit in the model bitstring to increase the log likelihood. If the log likelihood for the new bitstring is no higher than the previous bitstring, then the
alteration is rejected and the next alteration tried. The alterations are also rejected if the new bitstring
already exists within the population, ensuring that the constraint that all models in the population are
unique is maintained.

In the second stage, we re--calculate the weights for each individual in the population, based on the
likelihood of that model relative to the data $p(\vy; \vbeta_\vgamma)$ and the use this to re--calculate the
probability--based weights $w_i$ for each bitstring in the population. This is then used to re--calculate the
lower bound
\[
	\log \underline{p}(\vy; \vw, \Gamma) = \sum_{k=1}^K \vw_k \log p(\vy; \vbeta_{\vgamma_k}) - \vw_k \log \vw_k
\]

which is the sum of the weighted log-likelihood of the population and the entropy of the probability weights.
These two stages repeat until the lower bound converges.

\section{Results}


Our first numerical experiment is designed to show that our algorithm successfully finds the posterior models
of high probability, overcoming the difficulties of optimising over the multi-modal spike-and-slab posterior.
This example is taken from \citep{Rockova2016}. 
We consider a random sample of $n = 50$ observations on $p = 12$ predictors. $\mX_i \sim \N_p(\vzero, \mSigma)$
for $i = 1, \ldots, n$ where
$\mSigma = \text{bdiag}(\mSigma_1, \mSigma_1, \mSigma_1, \mSigma_1)$ with
$\mSigma_1 = (\sigma_{ij})_{i, j = 1}^{3, 3}$ where $\sigma_{ij} = 0.9$ for $i \ne j$ and $\sigma_{ii} = 1$.
The true model is $\vbeta_0 = (1.3, 0, 0, 1.3, 0, 0, 1.3, 0, 0, 1.3, 0, 0)^\top$.
The responses are then generated from $\vy = \mX \vbeta_0 + \vepsilon$, where
$\vepsilon \sim \N_n(\vzero, \mI_n)$.

\subsection{$n < p$, High dimensional example, Particle EM}

\begin{figure}\label{fig:highdim_warm_start_covariates}
\caption{$F_1$ scores for CVA on the simulated high dimensional data with $\Gamma$ initialised from the SCAD models, with models with more covariates preferred. $K=20, 50, 100$ respectively}
% \includegraphics[scale=0.33]{code/QLT/results/20_generate_data_high_dimensional_warm_start_covariates_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/50_generate_data_high_dimensional_warm_start_covariates_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/100_generate_data_high_dimensional_warm_start_covariates_log_prob1.pdf}
\includegraphics[scale=0.5]{highdimensional_covariates_logprob1.pdf}
\end{figure}

\begin{figure}\label{fig:highdim_warm_start_likelihood}
\caption{$F_1$ scores for CVA on the simulated high dimensional data with $\Gamma$ initialised from the SCAD models, with models with higher likelihood preferred. $K=20, 50, 100$ respectively}
% \includegraphics[scale=0.33]{code/QLT/results/20_generate_data_high_dimensional_warm_start_likelihood_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/50_generate_data_high_dimensional_warm_start_likelihood_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/100_generate_data_high_dimensional_warm_start_likelihood_log_prob1.pdf}
\includegraphics[scale=0.5]{highdimensional_likelihood_logprob1.pdf}
\end{figure}

\begin{figure}\label{fig:highdim_cold_start}
\caption{$F_1$ scores for CVA on the simulated high-dimensional data with $\Gamma$ initialised randomly.
					K=20, 50 and 100 respectively}
% \includegraphics[scale=0.33]{code/QLT/results/20_generate_data_high_dimensional_cold_start_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/50_generate_data_high_dimensional_cold_start_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/100_generate_data_high_dimensional_cold_start_log_prob1.pdf}
\includegraphics[scale=0.5]{highdimensional_cold_logprob1.pdf}
\end{figure}

\subsection{Exploration of the posterior model space}

If the covariates in a model selection problem are highly collinear then the posterior distribution will be
highly multi-modal when a spike-and-slab prior structure is used. This can make seeking the optimal model very
challenging, due to the many local optima. In this section, we present a series of numerical experiments which
demonstrate the capability of our algorithm to successfully find the models with high posterior probability in
such situations.

We first present an example where $n > p$ and $p$ is relatively small ($p = 12$), to allow for the full 
enumeration of the model space. Later, we show an example for the important $p > n$ case, and compare our results 
against the xxx algorithm.

The results were generated by repeatedly simulating the data set $1,000$ times and fitting the CVA model.
The median posterior model probability was calculated by taking the median across all of the runs of the
total posterior probability of all of the models discovered by the CVA algorithm during that run. The global
percentage was the proportion of the runs where the CVA algorithm successfully found the global mode.

\subsubsection{$p < n$}
Figure \ref{fig:cva_posterior_models} shows all $4,096$ posterior model probabilities ordered by the model's bit
strings. We can clearly see a few peaks in the full posterior distribution. Our experiment aims to show that
most of these posterior peaks are successfully identified by our algorithm.

We begin by using the setting $\lambda = 1$, allowing particle repulsion between each of the models within the
population. Our population of bit strings $\mGamma^{(0)} = [\vgamma_1^{(0)}, \ldots, \vgamma_K^{(0)}]$ with
$K = 20$ particles was randomly initialised from a sequencial of independent Bernoulli trials with probability
of success $1/2$.

\begin{figure}	
	\caption{Posterior model probabilities when $p = 12$. Red triangles denote models visited by the CVA
						algorithm, while black triangles are models that were not visited.}
	\label{fig:cva_posterior_models}
	\includegraphics[scale=0.5]{code/correlation/cva_low_dimensional.pdf}
\end{figure}

\subsection{$p > n$}

\begin{table}
	\caption{Simulation results including the average number of modes located, the average percentage of
	posterior coverage achieved, and the percentage of times that the global mode was located, for various
	population sizes $(K=20, 50, 100)$ and choices of repulsion $\lambda=0, 1, 2, 3$}
	\label{tab:result2}
	\begin{tabular}{l|llll|llll|llll|llll}
	\hline
	 					& \multicolumn{4}{c}{K=20} 	& \multicolumn{4}{c}{K=50} & \multicolumn{4}{c}{K=100} \\
	$\lambda$ & 0 & 1 & 2 & 3 & 0 & 1 & 2 & 3 & 0 & 1 & 2 & 3 & 0 & 1 & 2 & 3 \\
	\hline
	\# Modes & \\
	\% Posterior & 83.1 & 84.7 & 83.7 & 82.3 & 84.7 & 83.2 & 85.4 & 84.8 & 81.9 & 82.9 & 83.9 & 84.6 & 83.9 & 81.5 & 82.0 & 85.2 \\
	\% Global Mode & 91 & 90 & 87 & 82 & 86 & 85 & 80 & 90 & 90 & 90 & 86 & 91 & 88 & 91 & 76 & 88 \\
	\hline
	\end{tabular}

\end{table}

% Show posterior probabilities on the log scale
As Figure \ref{fig:cva_posterior_models} shows, 
in the plots of the log posterior probabilities of the models, the particles can be seen clustering at the
highest probability models first, then spreading through the medium and low probability models. From these
plots we can see that once $K$ is high enough, there is a good variety of high, medium and low posterior
probability models in the population of particles. The coverage of the posterior probability distribution
by the population of particles is high, as the particles tend to cluster towards the higher posterior
probability models as CVA's greedy search algorithms proceeds.

\subsection{Comparison of CVA against other model selection methods on simulated data sets}

	% TP <- sum(vgamma.hat[pos])
	% TN <- sum(1 - vgamma.hat[neg])
	% FP <- sum(vgamma.hat[neg])
	% FN <- sum(1 - vgamma.hat[pos])
	
	% sensitivity <- TP/length(pos)
	% specificity <- TN/length(neg)
	% precision <- TP/sum(vgamma.hat)
	% recall <- TP/(TP + FN)
	% accuracy <- (TP + TN)/(TP + TN + FP + FN)
		
	% F1 <- 2*precision*recall/(precision+recall)		

The method used to assess the quality of the variable selection was to generate data from a known true model
$\vgamma$,
and then compare this against the model $\hat{\vgamma}$ found by each of the model selection methods that we 
compared.
We then calculated the $F_1$ score for $\hat{\vgamma}$,
where
$F_1 = 2 * \text{precision} * \text{recall} / (\text{precision} + \text{recall})$ with
$\text{precision} = \text{true positives} / (\text{true positives} + \text{false positives})$ and
$\text{recall} = \text{true positives} / (\text{true positives} + \text{false negatives})$.

When running CVA,
three methods of initialising $\Gamma$ were tried.
A cold start where each $\gamma_{ij}$ in $\gamma_i$, $1 \leq i \leq K$ was initialised randomly from a
$\Bernoulli(p)$ distribution, with $p = 10 / |\vgamma|$.
Two methods of warm start were also tried, to explore whether the CVA algorithm could do better by being
initialised from another method than by being initialised randomly: 
a warm start from SCAD where models with more covariates were preferred, and
a warm start from SCAD where models with higher likelihood were preferred.

The experiments were repeated with Maruyama's prior, Liang's hyper-g prior and Bayarri's robust prior.
The results of the algorithm were found to be insensitive to the choice of prior.
For each combination of population size, data set, and prior the experiment was repeated 50 times.

\subsubsection{QTL example}
The QTL example is taken from \cite{Xu2007}. A BC population of $n=600$ was simulated for a single large
chromosome of $1800$ cM. This chromosome was covered by 121 evenly spaced markers. Nine of the markers
overlapped with QTL of the main effects and 13 out of the $\binom{121} 2 = 7,260$ possible marker pairs had
interaction effects.

consistently achieving higher $F_1$ scores than the competing methods that we examined
When initialising $\Gamma$ with a warm start from SCAD preferring models with more covariates or with higher
likelihood, CVA performed poorly with a lower number of particles (K=$20$). Performance improved as the number
of particles in the population was increased, as can be seen in Figures
\ref{fig:highdim_warm_start_covariates} and \ref{fig:highdim_warm_start_likelihood}. Performance was better
when $\Gamma$ was initialised randomly, for all population sizes $K=20$, $50$ or $100$. This indicates that
the CVA algorithm is sensitive to its' initialisation, and that as SCAD does poorly on these problems, CVA
also does poorly comparatively with a warm start from SCAD, although still better than SCAD alone.

\begin{figure}\label{fig:QLT_warm_start_covariates}
\caption{$F_1$ scores for CVA on the simulated QLT data with $\Gamma$ initialised from the SCAD models, with
models with more covariates preferred. $K=20, 50, 100$ respectively}
\includegraphics[scale=0.5]{QTL_covariates_maruyama.pdf}

% \includegraphics[scale=0.33]{code/QLT/results/20_generate_data_QTL_warm_start_covariates_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/50_generate_data_QTL_warm_start_covariates_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/100_generate_data_QTL_warm_start_covariates_log_prob1.pdf}
\end{figure}

\begin{figure}\label{fig:QLT_warm_start_covariates}
\caption{$F_1$ scores for CVA on the simulated QLT data with $\Gamma$ initialised from the SCAD models, with
models with higher likelihood preferred. $K=20, 50, 100$ respectively}
\includegraphics[scale=0.5]{QTL_likelihood_maruyama.pdf}
\end{figure}


Initialising $\Gamma$ randomly, CVA performed poorly by comparison with either of the warm start methods
above, as can be seen from \ref{fig:QLT_cold_start}.
\begin{figure}\label{fig:QLT_cold_start}
\caption{$F_1$ scores for CVA on the simulated QLT data with $\Gamma$ initialised randomly. $K=20, 50, 100$ respectively}
\includegraphics[scale=0.5]{QTL_cold_maruyama.pdf}

% \includegraphics[scale=0.33]{code/QLT/results/20_generate_data_QTL_cold_start_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/50_generate_data_QTL_cold_start_log_prob1.pdf}
% \includegraphics[scale=0.33]{code/QLT/results/100_generate_data_QTL_cold_start_log_prob1.pdf}
\end{figure}


\subsubsection{High-dimensional example $p > n$}
When initialising $\Gamma$ with a warm start from SCAD preferring models with higher likelihood,
CVA performed less well on the QTL example than SCAD preferring models with more covariates, but still
better than other competing methods except for MCP,
as can be seen from Figure \ref{fig:QLT_warm_start_likelihood}

\section{Variable inclusion for small data sets}

We compared variable selection using CVA against exact variable selection on five small data sets,
Hitters, Bodyfat, Wage, College and US Crime. 
The variable inclusion probabilities were estimated by taking the sum of the columns of the population
of models selected $\mGamma$
weighted by marginal likelihood of each model.
The exact variable inclusion probabilities were calculated by summing the columns of the matrix of all possible
models $\mGamma$ weighted by marginal likelihood of each model.
The mean relative error of the variable inclusion probabilities estimated by CVA was calculated,
and the results of these comparisons are presented in Table \ref{tab:variable_inclusion_rel_error}.
The number of particles in the population $K$ affected the
variable inclusion probability in the variables selected by CVA, while the marginal probability
$p(\vgamma | \vy)$ used to weight models in $\Gamma$ seemed to have only a very minor impact.
When the robust Bayarri prior is chosen to rank models in CVA,
the marginal probability $p(\vgamma | \vy)$ changes a lot as opposed to ranking models with other priors.
From the previous section, we see that changing the CVA model ranking
marginal had a large impact on the $F_1$ score obtained.
Variables with low posterior probability are truncated to 0, as CVA seeks higher posterior probability models,
ignoring the lower posterior probability models.

\begin{table}\label{tab:variable_inclusion_rel_error}
\caption{Relative error of the variable inclusion probability estimated by CVA to the
					exact variable inclusion probability, partitioned by exact probability under or equal to $0.5$ and
					over $0.5$}
\begin{tabular}{|ll|rrr|rrr|}
	\hline
	Dataset & Prior & & $<=0.5$ & & & $>0.5$ &\\
	& & $K = 20$ & $K = 50$ & $K = 100$ & $K = 20$ & $K = 50$ & $K = 100$ \\
	\hline
	Bodyfat&BIC&0.63&0.48&0.37&0.07&0.01&0.02\\
	&g.safe&0.66&0.52&0.42&0.07&0.01&0.02\\
	&Robust&0.65&0.52&0.4&0.07&0.01&0.02\\
	&ZE&0.65&0.51&0.39&0.06&0.01&0.02\\
	College&BIC&0.7&0.58&0.49&0.03&0.02&0.03\\
	&g.safe&0.9&0.78&0.64&0.06&0.06&0.06\\
	&Robust&0.88&0.78&0.63&0.06&0.06&0.06\\
	&ZE&0.82&0.66&0.57&0.03&0.06&0.06\\
	Hitters&BIC&0.74&0.64&0.5&0.12&0.07&0.06\\
	&g.safe&NA&0.81&0.83&NA&0.17&0.07\\
	&Robust&0.84&0.81&0.75&0.29&0.17&0.07\\
	&ZE&0.79&0.72&0.67&0.27&0.13&0.05\\
	USCrime&BIC&0.82&0.7&0.64&0.47&0.15&0.12\\
	&g.safe&0.76&0.71&0.64&0.45&0.16&0.12\\
	&Robust&0.79&0.7&0.61&0.35&0.14&0.08\\
	&ZE&0.76&0.7&0.64&0.45&0.16&0.13\\
	Wage&BIC&0.67&0.49&0.35&0&0&0\\
	&g.safe&0.69&0.47&0.33&0&0&0\\
	&Robust&0.69&0.47&0.32&0&0&0\\
	&ZE&0.69&0.47&0.32&0&0&0\\
	\hline
\end{tabular}
\end{table}

\section{Implementation techniques}
Our model updates are made efficient by using rank-one updates to update the inverse of $(\mX^\top \mX)^{-1}$.

To ensure uniqueness of the $K$ models in the population, before a new candidate model with a covariate added
or removed is considered, the population of existing models is checked to see if it already exists in the
population. If so, the addition or removal of the covariate is skipped and the next candidate model considered.
In our implementation, this check is made computationally efficient by maintaining a hash table of the models,
allowing the check as to whether the model is already in the population to be performed in $\BigO(1)$ time.

\bibliographystyle{elsarticle-harv}
\bibliography{references_mendeley}

\end{document}
