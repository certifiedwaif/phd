\documentclass{article}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\title{Preliminaries}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Variational approximations}

\subsection{Definition}

Consider a model $p(\vtheta|\vy)$ which is of interest to us but computationally difficult or intractable to 
fit. We may be able to gain much of the same insight from a given data set by fitting an accurate approximation 
of the model, allowing us to summarise the data and perform statistical inference.

Call the approximating model $q(\theta)$. Then we seek a $q(\theta)$ which approximates $p(\vtheta|\vy)$
as closely as possible in some sense. A useful measure of distance between probability distributions is the
Kullback-Leibler divergence, which is defined as

$$
\KL(q || p) \equiv \int q(\vtheta) \log{\frac{q(\theta)}{p(\theta)}} d \vtheta.
$$

We may seek a best approximating distribution $q^*(\vtheta)$ to a given posterior distribution
$p(\vtheta|\vy)$ from a parameteric family of approximating distributions $Q$ by finding

$$
q^*(\vtheta) = \argmin_{q \in Q} \KL \{ {q(\vtheta) || p(\vtheta|\vy)} \}.
$$

% Mean field update

\subsection{Laplace's Method of approximation}

\subsubsection{Definition}

Consider the second order Taylor expansion of $\log f(\vtheta)$ around the mode $\vtheta_m$

$$
\log f(\vtheta) \approx f(\vtheta_m) + (\vtheta - \vtheta_m) \nabla \log f(\vtheta) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\log f''').
$$

Assuming that $\vtheta$ is a stationary point of $\log f$, then $\nabla f(\vtheta) = \vzero$ and so

$$
\log f(\vtheta) \approx f(\vtheta_m) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\| \vtheta - \vtheta_m \|^3)
$$

at such a point. The quadratic form in $\vtheta$ in the approximate expresion for the log likelihood above gives 
rise to a Gaussian approximation for the likelihood $\N(\vtheta_m, \mH_{\log f}(\vtheta_m)^{-1})$.

The approximation is crude but can be quite accurate if the likelihood is symmetric and unimodal.

\subsection{Newton-Raphson}

\subsubsection{Definition}

Consider the second order Taylor expansion of $f(\vtheta)$ around $\vtheta_{n}$

$$
f(\vtheta) \approx f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta x^\top \mH_f(\vtheta_{n}) \Delta x + \sO(\| \Delta \vtheta_{n} \|^3).
$$

This expression attains its' maxima with respect to $\Delta x$ when

$$
\vzero = \mD_{\Delta \vtheta} [f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta \vtheta^\top \mH_f(\vtheta_{n}) \Delta \vtheta] = \nabla f(\vtheta_n) + \mH_f(\vtheta_n) \Delta \vtheta.
$$

Thus taking $\Delta \vtheta = -\nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$, we hope that choosing
$\vtheta_{n + 1} = \vtheta_{n} - \nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$ will be closer to the stationary 
point $\vtheta^*$ of $f$. This choice of $\vtheta_{n+1}$ establishes an iterative algorithm for finding the 
maxima/minima of $f$.

Convergence is achieved when the step length $\| \vtheta_{n+1} - \vtheta_{n} \| < \epsilon$, for some
suitably small choice of $\epsilon$, such as $\epsilon = 10^{-8}$.

\subsubsection{Time and memory complexity}

Execution of this algorithm requires the calculation of a vector addition, dot product and solution of a
linear system for each iteration. The vector addition and dot product can both be performed in $p$ flops,
where $p$ is the dimension of the problem. The expensive part of the computation is the solution of the linear
system $\mH_f(\vtheta_n) \Delta \vtheta$, which takes $\sO(p^3)$ flops. Even on modern computer systems, this
can quickly become computationally intractable for problems of modest size. This motivates the development
of Quasi-Newton algorithms, such as BFGS, L-BFGS and L-BFGS-B, discussed in Section \ref{sec:quasi_newton}.

\subsubsection{Radius of convergence and convergence to local solutions}

\subsection{Quasi-Newton Rapson algorithms}
\label{sec:quasi_newton}
L-BFGS-B

\subsubsection{Definition}

% Wolfe conditions
The problem of interest is to find

$$
\min_\vx f(\vx)
$$

for some smooth $f: \R^n -> \R$. Each step involves approximately solving the subproblem

$$
\min_\alpha f(\vx_k + \alpha \vp_k)
$$

where $\vx_k$ is the current best guess, $\vp_k \in \R^n$ is a search direction and $\alpha$ is the
step length.

Denote a univariate function $\phi$ restricted to the direction $\vp_k$ as
$\phi(\alpha) = f(\vx_k + \alpha \vp_k)$. A step length $\alpha_k$ is said to satisfy the \emph{Wolfe conditions}
is the following two inequalities hold:

\begin{enumerate}
\item[i] $f(\vx_k + \alpha_k \vp_k) \leq f(\vx_k) + c_1 \alpha_k \vp_k^\top \grad f(\vx_k)$ \text{(Armijo rule)}
\item[ii] $\vp_k^\top \grad f(\vx_k + \alpha_k \vp_k) \geq c_2 \vp_k^\top \grad f(\vx_k)$ \text{(curvature condition)}
\end{enumerate}

The first condition ensures that the step length $\alpha_k$ decreases $f$ sufficiently, while the second condition
ensures that the slope had been sufficiently reduced.

The rationale for imposing the Wolfe conditions in an optimisation algorithm where
$\vx_{k+1} = x_k + \alpha \vp_k$ is to ensure the convergence of the gradient to zero. In particular, if the
cosine of the angle between $\vp_k$ and the gradient,

$$
\cos \theta_k = \frac{\nabla f(\vx_k)^\top \vp_k}{\|\nabla f(\vx_k)\| \|\vp_k\|}
$$

is bounded away from zero and the Wolfe conditions hold, then $\nabla f(\vx_k) \to 0$.

An additional moptivation, in the case of a quasi-Newton method is that if $\vp_k = - \mB_k^{-1} \nabla f(\vx_k)$,
where the matrix $\mB_k$ is updated by the BFGS or DFP formula, then if $\mB_k$ is positive definite condition ii
implies $\mB_{k+1}$ is also positive definite.

\subsubsection{Time and memory complexity}

\section{Numerical matters}

\subsection{Floating point numbers}
\subsection{Numerical differentiation and integration}
\subsection{Numerical linear algebra}
\subsection{Difficulty of inverting linear systems}

\section{Positive semi-definite matrices}
A positive semidefinite matrix $\mLambda$ is a square matrix of size $n \times n$ such that for any vector
$\vx \in \R^n$, $\vx^\top \mLambda \vx \geq 0$. Equivalently, all eigenvalues of $\mLambda$ are greater than or 
equal to 0.

\section{Matrix calculus}

Matrix calculus is a very useful and compact notation for the derivatives of functions of vectors and matrices.
A useful introduction is \cite{wand02}, while a more comprehensive treatment can be found in
\cite{MagnusNeudecker99}.

\subsection{Notation}

$\mD_\mLambda$ the matrix derivative with respect to $\mLambda$
$\mH_\mLambda$ the Hessian with respect to $\mLambda$

\section{B-Splines}

% This is taken from the Wikipedia page on the subject
A B-Spline is a piecewise polynomial function of degree $< n$ in a variable $x$. It is defined over a
domain $t_0 \leq x \leq t_m, m=n$. The points where $x = t_j$ are known as knots or break-points. The
number of internal knots is equal to the degree of the polynomial if there are no knot multiplicities.
The knots must be in ascending order. The number of knots is the minimum for the degree of the B-spline,
which has a non-zero value in the range between the first and last knot. Each piece of the function is a
polynomial of degree $< n$ between and including adjacent knots. A B-Spline is a continuous function at the
knots. When all internal knots are distinct its derivatives are also continuous up to the derivative of degree
$n - 1$. If internal knots are coincident at a given value of x, the continuity of derivative order is reduced
by 1 for each additional knot.

For any given set of knots, the B-spline is unique, hence the name, B being short for Basis. The usefulness
of B-splines lies in the fact that any spline function of order $n$ on a given set of knots can be expressed
as a linear combination of B-spline:

% B-Splines
$$
S_{n, t}(x) = \sum_i \alpha_i B_{i, n}(x)
$$

where

\begin{align*}
B_{i, 1}(x) &:= \begin{cases}
1 & \text{if } t_i \leq x < t_{i+1} \\
0 & \text{otherwise}
\end{cases} \\
B_{i, k}(x) &:= \frac{x - t_i}{t_{i + k - 1} - t_i} B_{i, k-1} (x) + \frac{t_{i + k} - x}{t_{i + k} - t_{i + 1}} B_{i, k-1} (x)
\end{align*}

% Divided difference notation?
% Lagrange's interpolating polynomials?

\bibliographystyle{elsarticle-harv}
\bibliography{Chapter_0_preliminary}

\end{document}
