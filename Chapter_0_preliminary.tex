\documentclass{article}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\title{Preliminaries}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Variational approximations}

\subsection{Definition}

Consider a model $p(\vtheta|\vy)$ which is of interest to us but computationally difficult or intractable to fit. 
We may be able to gain much of the same insight from a given data set by fitting an accurate approximation of
the model, allowing us to summarise the data and perform statistical inference.

Call the approximating model $q(\theta)$. Then we seek a $q(\theta)$ which approximates $p(\vtheta|\vy)$
as closely as possible in some sense. A useful measure of distance between probability distributions is the
Kullback-Leibler divergence, which is defined as

$$
\KL(q || p) \equiv \int q(\vtheta) \log{\frac{q(\theta)}{p(\theta)}} d \vtheta.
$$

We may seek a best approximating distribution $q^*(\vtheta)$ to a given posterior distribution
$p(\vtheta|\vy)$ from a parameteric family of approximating distributions $Q$ by finding

$$
q^*(\vtheta) = \argmin_{q \in Q} \KL \{ {q(\vtheta) || p(\vtheta|\vy)} \}.
$$

% Mean field update

\subsection{Laplace's Method of approximation}

\subsubsection{Definition}

Consider the second order Taylor expansion of $\log f(\vtheta)$ around the mode $\vtheta_m$

$$
\log f(\vtheta) \approx f(\vtheta_m) + (\vtheta - \vtheta_m) \nabla \log f(\vtheta) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\log f''')
$$

Assuming that $\vtheta$ is a stationary point of $\log f$, then $\nabla f(\vtheta) = \vzero$ and so

$$
\log f(\vtheta) \approx f(\vtheta_m) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(f''')
$$

at such a point. The quadratic form in $\vtheta$ in the approximate expresion for the log likelihood above gives 
rise to a Gaussian approximation for the likelihood $\N(\vtheta_m, \mH_{\log f}(\vtheta_m)^{-1})$.

The approximation is crude but can be quite accurate if the likelihood is symmetric and unimodal.

\subsection{Newton-Raphson}

\subsubsection{Definition}

Consider the second order Taylor expansion of $f(\vtheta)$ around $\vtheta_{n}$

$$
f(\vtheta) \approx f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta x^\top \mH_f(\vtheta_{n}) \Delta x + \sO(f''')
$$

This expression attains its' maxima with respect to $\Delta x$ when

$$
\vzero = \mD_{\Delta \vtheta} [f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta \vtheta^\top \mH_f(\vtheta_{n}) \Delta \vtheta] = \nabla f(\vtheta_n) + \mH_f(\vtheta_n) \Delta \vtheta.
$$

Thus taking $\Delta \vtheta = -\nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$, we hope that choosing
$\vtheta_{n + 1} = \vtheta_{n} - \nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$ will be closer to the stationary 
point $\vtheta^*$ of $f$. This choice of $\vtheta_{n+1}$ establishes an iterative algorithm for finding the 
maxima/minima of $f$.

Convergence is achieved when the step length $\| \vtheta_{n+1} - \vtheta_{n} \| < \epsilon$, for some
suitably small choice of $\epsilon$, such as $\epsilon = 10^{-8}$.

\subsubsection{Time and memory complexity}

Execution of this algorithm requires the calculation of a vector addition, dot product and solution of a
linear system for each iteration. The vector addition and dot product can both be performed in $p$ flops,
where $p$ is the dimension of the problem. The expensive part of the computation is the solution of the linear
system $\mH_f(\vtheta_n) \Delta \vtheta$, which takes $\sO(p^3)$ flops. Even on modern computer systems, this
can quickly become computationally intractable even for problems of modest size. This motivates the development
of Quasi-Newton algorithms, such as BFGS, L-BFGS and L-BFGS-B, discussed in Section \ref{sec:quasi_newton}.

\subsubsection{Radius of convergence and convergence to local solutions}

\subsection{Quasi-Newton Rapson algorithms}
\label{sec:quasi_newton}
L-BFGS-B

\subsubsection{Definition}

\subsubsection{Time and memory complexity}

\section{Numerical matters}

\subsection{Floating point numbers}
\subsection{Numerical differentiation and integration}
\subsection{Numerical linear algebra}
\subsection{Difficulty of inverting linear systems}

\section{Positive semi-definite matrices}
A positive semidefinite matrix $\mLambda$ is a square matrix of size $n \times n$ such that for any vector
$\vx \in \R^n$, $\vx^\top \mLambda \vx \geq 0$. Equivalently, all eigenvalues of $\mLambda$ are greater than or 
equal to 0.

\section{Matrix calculus}

\bibliographystyle{elsarticle-harv}
\bibliography{Chapter_0_zero_inflated_models}

\end{document}
