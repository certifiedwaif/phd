\documentclass{article}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\title{Preliminaries}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Variational approximations}

\subsection{Definition}

Consider a model $p(\vtheta|\vy)$ which is of interest to us but computationally difficult or intractable to 
fit. We may be able to gain much of the same insight from a given data set by fitting an accurate approximation 
of the model, allowing us to summarise the data and perform statistical inference.

Call the approximating model $q(\theta)$. Then we seek a $q(\theta)$ which approximates $p(\vtheta|\vy)$
as closely as possible in some sense. A useful measure of distance between probability distributions is the
Kullback-Leibler divergence, which is defined as

$$
\KL(q || p) \equiv \int q(\vtheta) \log{\frac{q(\theta)}{p(\theta)}} d \vtheta.
$$

We may seek a best approximating distribution $q^*(\vtheta)$ to a given posterior distribution
$p(\vtheta|\vy)$ from a parameteric family of approximating distributions $Q$ by finding

$$
q^*(\vtheta) = \argmin_{q \in Q} \KL \{ {q(\vtheta) || p(\vtheta|\vy)} \}.
$$

% Mean field update

A more thorough introduction to the topic of variational approximations is available in \cite{ormerod10}.

\subsection{Laplace's Method of approximation}

\subsubsection{Definition}

Consider the second order Taylor expansion of $\log f(\vtheta)$ around the mode $\vtheta_m$

$$
\log f(\vtheta) \approx f(\vtheta_m) + (\vtheta - \vtheta_m) \nabla \log f(\vtheta) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\log f''').
$$

Assuming that $\vtheta$ is a stationary point of $\log f$, then $\nabla f(\vtheta) = \vzero$ and so

$$
\log f(\vtheta) \approx f(\vtheta_m) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\| \vtheta - \vtheta_m \|^3)
$$

at such a point. The quadratic form in $\vtheta$ in the approximate expresion for the log likelihood above gives 
rise to a Gaussian approximation for the likelihood $\N(\vtheta_m, \mH_{\log f}(\vtheta_m)^{-1})$.

The approximation is crude but can be quite accurate if the likelihood is symmetric and unimodal.

\subsection{Newton-Raphson}

\subsubsection{Definition}

Consider the second order Taylor expansion of $f(\vtheta)$ around $\vtheta_{n}$

$$
f(\vtheta) \approx f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta x^\top \mH_f(\vtheta_{n}) \Delta x + \sO(\| \Delta \vtheta_{n} \|^3).
$$

This expression attains its' maxima with respect to $\Delta x$ when

$$
\vzero = \mD_{\Delta \vtheta} [f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta \vtheta^\top \mH_f(\vtheta_{n}) \Delta \vtheta] = \nabla f(\vtheta_n) + \mH_f(\vtheta_n) \Delta \vtheta.
$$

Thus taking $\Delta \vtheta = -\nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$, we hope that choosing
$\vtheta_{n + 1} = \vtheta_{n} - \nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$ will be closer to the stationary 
point $\vtheta^*$ of $f$. This choice of $\vtheta_{n+1}$ establishes an iterative algorithm for finding the 
maxima/minima of $f$.

Convergence is achieved when the step length $\| \vtheta_{n+1} - \vtheta_{n} \| < \epsilon$, for some
suitably small choice of $\epsilon$, such as $\epsilon = 10^{-8}$.

\subsubsection{Time and memory complexity}

Execution of this algorithm requires the calculation of a vector addition, dot product and solution of a
linear system for each iteration. The vector addition and dot product can both be performed in $p$ flops,
where $p$ is the dimension of the problem. The expensive part of the computation is the solution of the linear
system $\mH_f(\vtheta_n) \Delta \vtheta$, which takes $\sO(p^3)$ flops. Even on modern computer systems, this
can quickly become computationally intractable for problems of modest size. This motivates the development
of Quasi-Newton algorithms, such as BFGS, L-BFGS and L-BFGS-B, discussed in Section \ref{sec:quasi_newton}.

\subsubsection{Radius of convergence and convergence to local solutions}

\subsection{Quasi-Newton Rapson algorithms}
\label{sec:quasi_newton}
L-BFGS-B

\subsubsection{Definition}

% Wolfe conditions
The problem of interest is to find

$$
\min_\vx f(\vx)
$$

for some smooth $f: \R^n -> \R$. Each step involves approximately solving the subproblem

$$
\min_\alpha f(\vx_k + \alpha \vp_k)
$$

where $\vx_k$ is the current best guess, $\vp_k \in \R^n$ is a search direction and $\alpha$ is the
step length.

Denote a univariate function $\phi$ restricted to the direction $\vp_k$ as
$\phi(\alpha) = f(\vx_k + \alpha \vp_k)$. A step length $\alpha_k$ is said to satisfy the \emph{Wolfe conditions}
is the following two inequalities hold:

\begin{enumerate}
\item[(i)] $f(\vx_k + \alpha_k \vp_k) \leq f(\vx_k) + c_1 \alpha_k \vp_k^\top \nabla f(\vx_k)$ \text{(Armijo rule)}
\item[(ii)] $\vp_k^\top \nabla f(\vx_k + \alpha_k \vp_k) \geq c_2 \vp_k^\top \nabla f(\vx_k)$ \text{(curvature condition)}
\end{enumerate}

The first condition ensures that the step length $\alpha_k$ decreases $f$ sufficiently, while the second condition
ensures that the slope had been sufficiently reduced.

The rationale for imposing the Wolfe conditions in an optimisation algorithm where
$\vx_{k+1} = x_k + \alpha \vp_k$ is to ensure the convergence of the gradient to zero. In particular, if the
cosine of the angle between $\vp_k$ and the gradient,

$$
\cos \theta_k = \frac{\nabla f(\vx_k)^\top \vp_k}{\|\nabla f(\vx_k)\| \|\vp_k\|}
$$

is bounded away from zero and the Wolfe conditions hold, then $\nabla f(\vx_k) \to 0$.

An additional motivation, in the case of a quasi-Newton method is that if $\vp_k = - \mB_k^{-1} \nabla f(\vx_k)$,
where the matrix $\mB_k$ is updated by the BFGS or DFP formula, then if $\mB_k$ is positive definite condition ii
implies $\mB_{k+1}$ is also positive definite.

\subsubsection{Time and memory complexity}

\section{Numerical matters}

\subsection{Floating point numbers}
\subsection{Numerical differentiation and integration}
\subsection{Numerical linear algebra}
\subsection{Difficulty of inverting linear systems}

\section{Positive semi-definite matrices}
A positive semidefinite matrix $\mLambda$ is a square matrix of size $n \times n$ such that for any vector
$\vx \in \R^n$, $\vx^\top \mLambda \vx \geq 0$. Equivalently, all eigenvalues of $\mLambda$ are greater than or 
equal to 0.

\section{Matrix calculus}

Matrix calculus is a very useful and compact notation for the derivatives of functions of vectors and matrices.
A useful introduction is \cite{wand02}, while a more comprehensive treatment can be found in
\cite{MagnusNeudecker99}.

\subsection{Notation}

Given a function $f:\R^{n \times n} \to \R$ of a matrix $\mLambda$, we denote the matrix derivative of $f$ with 
respect to $\mLambda$ as $\mD_\mLambda f$, and the Hessian of $f$ with respect to $\mLambda$ as
$\mH_\mLambda f$ the Hessian with respect to $\mLambda$.

The Hessian matrix is a $d \times d$ matrix whose $(i, j)$-th entry is equal to

$$
\frac{\partial^2}{\partial x_i x_j} f(\vx)
$$

and is denoted by $\mH f(\vx)$.

\section{Splines}
The most general form of the univariate regression problem is

$$
y_i = f(x_i)
$$

where $f: \R \to \R$ is unknown, and we wish to estimate it as $\hat{f}$. We treat the functional form of
$f$ as unknown, and attempt to estimate it purely from the data available to us. Fully non-parametric regression
is a difficult problem to solve, but the problem can be simplified by pre-specifying the points at which the
function may change curvature, the so-called knots.

\subsection{B-Splines}
% This is taken from the Wikipedia page on the subject
A B-Spline is a piecewise polynomial function of degree $< n$ in a variable $x$. It is defined over a
domain $t_0 \leq x \leq t_m, m=n$. The points where $x = t_j$ are known as knots or break-points. The
number of internal knots is equal to the degree of the polynomial if there are no knot multiplicities.
The knots must be in ascending order. The number of knots is the minimum for the degree of the B-spline,
which has a non-zero value in the range between the first and last knot. Each piece of the function is a
polynomial of degree $< n$ between and including adjacent knots. A B-Spline is a continuous function at the
knots. When all internal knots are distinct its derivatives are also continuous up to the derivative of degree
$n - 1$. If internal knots are coincident at a given value of x, the continuity of derivative order is reduced
by 1 for each additional knot.

For any given set of knots, the B-spline is unique, hence the name, B being short for Basis. The usefulness
of B-splines lies in the fact that any spline function of order $n$ on a given set of knots can be expressed
as a linear combination of B-spline:

% B-Splines
$$
S_{n, t}(x) = \sum_i \alpha_i B_{i, n}(x)
$$

where

\begin{align*}
B_{i, 1}(x) &:= \begin{cases}
1 & \text{if } t_i \leq x < t_{i+1} \\
0 & \text{otherwise}
\end{cases} \\
B_{i, k}(x) &:= \frac{x - t_i}{t_{i + k - 1} - t_i} B_{i, k-1} (x) + \frac{t_{i + k} - x}{t_{i + k} - t_{i + 1}} B_{i, k-1} (x)
\end{align*}

% Divided difference notation?
% Lagrange's interpolating polynomials?
% Semiparametric regression / Connection to mixed models
We follow the discussion of semiparametric regression in \cite{RuppertWandCarroll}.
Using a mixed models setup to fit spline models, protects against overfitting.
Construct an $\mZ$ matrix with the appropriate B-Spline function evaluations in each of the rows, where
each column corresponds to a knot.

\subsection{Degrees of freedom}

% From Section 2.5.2, Semiparametric regression

Consider the fixed effects model $\vy = \mX \vbeta$ where $\mX \in \R^{n \times p}$. If the hat matrix is
defined as $\mH = \mX (\mX^\top\mX)^{-1} \mX^\top$ then

\begin{align*}
\tr(\mH) &= \tr[\mX(\mX^\top\mX)^{-1}\mX^\top] \\
&= \tr[\mX^\top\mX (\mX^\top\mX)^{-1}] \\
&= \tr(\mI_p) \\
&= p
\end{align*}

which is the number of parameters in the fixed effects model. Thus we use the trace of the hat matrix as
the definition of the effective degrees of freedom of the model. This definition then naturally extends to
more general smoother matrices which can be used to fit models involving splines or other kinds of smoothing.

\section{Generalised Linear Mixed models}

\subsection{Exponential family}

An exponential family is a set of probability distributions of the form
$$
f_X(x|\vtheta) = h(x) g(\theta) \exp(\eta(\theta) \dot T(x)).
$$

Most commonly used probability distributions in statistics such as normal, exponential, gamma,
chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, Wishart and Inverse Wishart can
be represented in this form.

Canonical form
$$
\vy^\top \mC \vnu - b(\mC \vnu)
$$

Different choices of the function $b:\R \to \R$ lead to different generalised linear mixed models.

Table of b functions

\bibliographystyle{elsarticle-harv}
\bibliography{Chapter_0_preliminary}

\end{document}
