\documentclass{amsart}[12pt]
% \documentclass{usydthesis}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{color}

\title{Introduction}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}

\newcommand{\mgc}[1]{{\color{blue}#1}}
\newcommand{\joc}[1]{{\color{red}#1}}

\begin{document}
\setlength{\parindent}{0pt}
\maketitle

% Outline that John suggested
\section{Real world problems}
The advent of digital computers and the Internet have lead to an explosion in the volume of data being
collected. With technological progress marching on, this trend seems only set to continue and accelerate. But
this data is only of value if it can be analysed and understood.  This incredible increase in the volume of data has lead to corresponding
computational difficulties in processing and modelling such large amounts of data -- so-called Big Data which
is so large that it is difficult to process on one computer. This data raises new challenges which modern
statisticians must be ready to meet. This realisation has lead to an explosion of
interest in data science, incorporating ideas from both statistics and computer science, in recent years.
Machine learning, probabilistic basis, statistical machine learning.
\citep{James:2014:ISL:2517747}
\citep{MacKay:2002:ITI:971143}
\citep{hastie01statisticallearning}
\citep{Murphy:2012:MLP:2380985}

\section{Motivation}
Approaches to modelling data are needed which can handle large volumes of data in a computationally
efficient manner while retaining the probabilistic underpinning of classical statistics and statistical machine
learning that leads to a rigorous underlying theory for inference.

\section{Approach (in broad terms)}
Gaussian Variational approximation for linear mixed models.
Exact inference for model selection for linear models with normal priors.

\section{Why Bayesian?}
The difference between frequentist and Bayesian approaches begins with a difference in philosophy.
Frequentists define an event's probability as its' relative frequency after a large number of trials.
while Bayesians view probability as our reasonable expectation about an event, representing our state of knowledge about the event.

There are many practical reasons to choose Bayesian approaches to modelling data.
It is flexible to complications. Complicated models can be built by chaining together multiple levels of
simple models. These models can then be fit to data by calculating the posterior probability of the
parameters using Bayes' Rule,
\[
	p(\vtheta | \vy) = \frac{p(\vy | \vtheta) p(\vtheta)}{\int p(\vy | \vtheta) p(\vtheta) d \vtheta}
\]
providing the resulting integral can be evaluated or approximated. There are many models which are difficult to
fit under the frequentist paradigm, as the likelihood is difficult to maximise.
Furthermore, as the Bayesian paradigm treats each of the parameters in a model as uncertain, the full
uncertainty associated with all of the parameters can be estimated in the uncertainty in the posterior distribution.
Avoid pitfalls such as significance testing, p-values.
\citep{Janes2014}

The ability to build a model a part at a time and have the uncertainty propagate makes Bayesian modelling 
particularly appropriate for mixed and hierarchical models.
Uncertainty regarding model selection is taken into account

\section{What problems in thesis}
\subsection{GLMM}
Generalised Linear Mixed Models, an extension of Generalised Linear Models to include both fixed and
random effects, are flexible to many complicated modelling situations. The standard technique for fitting
Bayesian versions of these models is to use Monte Carlo Markov Chains techniques such as Gibbs sampling.

\subsection{Model selection}
While new types of model are continually being developed, linear models with normal priors remain a popular
and essential modelling tool owing to the ease of fitting these models, statistical inference on the
parameters and, most importantly, the ease which which these models can be interpreted. But for a data set
with a moderate or large number of parameters, the question is immediately raised of which covariates we should
include in our model. One of the problems that we address in this thesis is \emph{model selection} on linear
models with normal priors.

\section{GLMM}
Linear and generalised linear regression models are the standard tools used by applied statisticians to
explain the relationship between an outcome variable and one or more explanatory variables. They provide a
general method  to analyse quantified relationships between variables within a data set in an easily
interpretable way. A standard assumption is that the outcomes are independent, and that the effect of the
explanatory variables on the outcome is fixed. But if the outcomes are dependent and this assumption is not
met, then linear and generalised linear models can be extended to linear mixed models. These allow us to
incorporate dependencies amongst the  observations via the assumption of a more complicated covariance
structure, including random effects for  different subgroups or longitudinal data and other extensions such as
splines, missing data and measurement error. This additional flexibility makes their application popular in
many fields, such as public health, psychology and agriculture.

While mixed models are very useful for gaining insight into a data set, fitting them can be computationally
challenging. For all but the simplest situations, fitting these models involves computing high-dimensional
integrals which are often analytically and computationally intractable. Thus, approximations must be used.

In the frequentist paradigm, model parameters are fixed and uncertainty enters the model through random
errors, which have associated variance. The data is modelled as a combination of these fixed parameters and
random errors. In the Bayesian paradigm, the uncertainty enters the model by assuming parameters are random
variables, while the data is fixed.

\subsection{Structure of GLMMs}

\section{Generalised Linear Mixed models}
All of the models considered in this thesis belong to the family of generalised linear mixed models - a
flexible set of models which can model continuous or discrete responses and incorporate both fixed and
random effects.

\subsection{Exponential family}

An exponential family is a set of probability distributions of the form
\[
	f_X(x|\vtheta) = h(x) g(\theta) \exp(\eta(\theta)^\top T(x)).
\]

Most commonly used probability distributions in statistics such as the normal, exponential, gamma,
chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, Wishart and Inverse Wishart distributions can
be represented in this form.

\subsubsection{Canonical form}
\[
	\log p(\vnu; \vy) = \vy^\top \mC \vnu - b(\mC \vnu)
\]

Different choices of the function $b:\R \to \R$ lead to different generalised linear mixed models. A set
of available functions and corresponding distributions is presented in Table \ref{tab:b_functions}.

\begin{table}
	\caption{Table of b functions}
	\label{tab:b_functions}
	\begin{tabular}{|l|lll|}
		\hline
		Distribution                         & Parameter & Natural parameter       & Inverse parameter mapping \\
		\hline
		Bernoulli distribution               & $p$       & $\log{\frac{p}{1 - p}}$ & $\frac{1}{1 + e^\eta}$    \\
		Poisson distribution                 & $\lambda$ & $\log \lambda$          & $e^\eta$                  \\
		Normal distribution (known variance) & $\mu$     & $\frac{\mu}{\sigma}$    & $\sigma \eta$             \\
		\hline
	\end{tabular}
\end{table}

\subsection{Mixed Models}

We follow the convention of \citep{Zhao2006}. Generalised Linear Mixed Models from the exponential family with
Gaussian random effects take the general form
$$
\begin{array}{rl}
	\vy | \vbeta, \vu &= \exp{\{ \vy^\top (\mX \vbeta + \mZ \vu) - \vone^\top b(\mX \vbeta + \mZ \vu) + \vone^\top c(\vy) \}}, \\
	\vu | \mG &\sim \N(\vzero, \mG),
\end{array}
$$
where the fixed effects are denoted by the vector $\vbeta$ and the random effects are denoted by $\vu$. The
design matrix for the fixed effects is denoted by $\mX$ and the design matrix for the random effects are
denoted by $\mZ$. The choice of the functions $b$ and $c$ allows this general structure to be adapted to
particular situations -- a choice of $b(x) = e^x$ corresponding to the Poisson family of distributions
specifies a Poisson linear mixed model appropriate for modelling count data, while a choice of $b(x) = \log(1
+ e^x)$ corresponding to the logistic family of distributions specifies a logistic linear mixed model
appropriate to modelling binary data.

Random effects are very flexible in the variety of models they allow us to fit to our data. Through
specification of the covariance structures in the matrix $\mG$ with the appropriate data in the design matrix
$\mZ$, complicated dependencies amongst the responses $\vy$ can be specified, allowing modelling of
longitudinal data, fitting smoothing splines to the data and modelling spatial relationships between
responses. This allows us to fit hierarchical models with random intercepts and slopes, capturing levels of variation within groups within the data.
\citep{Gelman2007}
% TODO: Not happy with how this paragraph is written. I can express this idea better.

\subsection{Spline smoothing}
\section{Splines}
The most general form of the univariate regression problem is
\[
	y_i = f(x_i)
\]

where $f: \R \to \R$ is unknown, and we wish to estimate it as $\hat{f}$. We treat the functional form of
$f$ as unknown, and attempt to estimate it purely from the data available to us. Fully non-parametric regression
is a difficult problem to solve, but the problem can be simplified by pre-specifying the points at which the
function may change curvature, the so-called knots.

\subsection{O'Sullivan Splines}

Let $B_1$, \ldots, $B_{K+4}$ be the cubic B-spline basis functions defined by the knots $\kappa_1$ to
$\kappa_{K+4}$

$B_{ik} = B_k (x)$
$B_x = [B_1(x), \ldots, B_K+4(x)]$

O'Sullivan splines are penalised splines which are penalised using the $\mOmega$ matrix.

Let $\mOmega$ be the $(K+4) \times (K+4)$ matrix where the $(k, k')-th$ element is
\[
	\mOmega_{k k'} = \int_a^b B''_k(x) B''_{k'}(x) dx.
\]
Then the O'Sullivan spline estimate of the true function $f$ at the point $x$ is
$\hat{f}_O(x; \lambda) = \mB_x \hat{\vnu}_O$, where
$\hat{\vnu}_O = (\mB^\top \mB + \lambda \mOmega)^{-1} \mB^\top \vy$.

$\mOmega$ is defined in this way to penalise oscillation, which is measured by the second derivative.

The penalty differs from P-splines in that the P-spline penalty matrix is $\mD_2^\top \mD_2$ where $\mD_2$ is
the second-order differencing matrix.

% Divided difference notation?
% Lagrange's interpolating polynomials?
% Semiparametric regression / Connection to mixed models
We follow the discussion of semiparametric regression in \cite{RuppertWandCarroll}.
Using a mixed models setup to fit spline models protects against overfitting.
Constructing an $\mZ$ matrix with the appropriate B-Spline function evaluations in each of the rows, where
each column corresponds to a knot.

\subsection{Degrees of freedom}

% From Section 2.5.2, Semiparametric regression

Consider the fixed effects model $\vy = \mX \vbeta$ where $\mX \in \R^{n \times p}$. If the hat matrix is
defined as $\mH = \mX (\mX^\top\mX)^{-1} \mX^\top$ then

\begin{align*}
	\tr(\mH) & = \tr[\mX(\mX^\top\mX)^{-1}\mX^\top]  \\
	         & = \tr[\mX^\top\mX (\mX^\top\mX)^{-1}] \\
	         & = \tr(\mI_p)                          \\
	         & = p                                   
\end{align*}

which is the number of parameters in the fixed effects model. Thus we use the trace of the hat matrix as
the definition of the effective degrees of freedom of the model. This definition then naturally extends to
more general smoother matrices which can be used to fit models involving splines or other kinds of smoothing.

\subsubsection{Penalised spline}
\subsubsection{B-splines}
\subsection{B-Splines}
% This is taken from the Wikipedia page on the subject
A B-Spline is a piecewise polynomial function of degree $< n$ in a variable $x$. It is defined over a
domain $t_0 \leq x \leq t_m, m=n$. The points where $x = t_j$ are known as knots or break-points. The
number of internal knots is equal to the degree of the polynomial if there are no knot multiplicities.
The knots must be in ascending order. The number of knots is the minimum for the degree of the B-spline,
which has a non-zero value in the range between the first and last knot. Each piece of the function is a
polynomial of degree $< n$ between and including adjacent knots. A B-Spline is a continuous function at the
knots. When all internal knots are distinct its derivatives are also continuous up to the derivative of degree
$n - 1$. If internal knots are coincident at a given value of x, the continuity of derivative order is reduced
by 1 for each additional knot.

For any given set of knots, the B-spline is unique, hence the name, B being short for Basis. The usefulness
of B-splines lies in the fact that any spline function of order $n$ on a given set of knots can be expressed
as a linear combination of B-spline:

% B-Splines
$$
S_{n, t}(x) = \sum_i \alpha_i B_{i, n}(x)
$$

where

\begin{align*}
	B_{i, 0}(x) & := \begin{cases}                                                                                                        
	1           & \text{if } t_i \leq x < t_{i+1}                                                                                         \\
	0           & \text{otherwise}                                                                                                        
	\end{cases} \\
	B_{i, k}(x) & := \frac{x - t_i}{t_{i + 1} - t_i} B_{i, k-1} (x) + \frac{t_{i + k + 1} - x}{t_{i + k + 1} - t_{i + 1}} B_{i, k-1} (x). 
\end{align*}

% TODO: Divided differences definition

\subsection{Crossed effects}
\subsection{Spatial structure}

\section{Model selection}
% Given a full data matrix $\mX \in \R^{n \times p}$, we may wish to find the subset of variables $\gamma \in \{
% 1 \ldots p \}$ such that $\mX_\gamma \vbeta_\gamma$ describe the data in $\vy$ best.

The problem of selecting a statistical model from a set of candidate models given a data set, hence referred
to as \emph{model selection}, is one of the most important problems encountered in practice by applied
statistical practitioners. It is one of the central tasks of applied statistical analysis, and there is a
correspondingly large literature on the subject.The problem of model selection for normal linear models is
particularly well studied, owing to the popularity and importance of normal linear models in applications.

The bias-variance trade-off is one of the central issues in statistical learning. The guise this issue takes
in model selection is balancing the quality of the model fit against the complexity of the model, in an
attempt to find a compromise between over-fitting and under-fitting, in the hope that the model fit will
generalise well beyond the training data we have observed to the general population and that we haven't simply
learned the noise in the training set.

There have been many approaches to model selection proposed, including criteria based approaches, variable
selection, approaches based on functions of the residual sum of squares, penalised regression such as the
lasso and $L_1$ regression, and Bayesian modelling approaches. Model selection is a difficult problem in high
--dimensional spaces in general because as the dimension of the space increases, the number of possible models
increases combinatorially. Many model selection algorithms use heuristics in an attempt to search the model
space more efficiently but still find an optimal or near-optimal model within a reasonable period of time. A
major motivation for this field of research is the need for a computationally feasible approach to performing
model selection on large scale problems where the number of covariates is large.

Many approaches to model selection have been investigated, with various model selection criteria
proposed to attempt to balance model likelihood against model complexity. One approach to model selection is
to select between entire models, using a model selection criterion. These criterion may have a fixed penalty
for model complexity, such as Akaike's Information Criterion \citep{Akaike1974}, the Risk Inflation Criterion
\citep{Foster1994}, the Schwarz criterion or Bayesian Information Criterion \citep{Schwarz1978}, the Deviance
Information Criterion \citep{Spiegelhalter2016} or the Principle of Minimum Description Length
\citep{Hansen2001}. Alternatively, the penalty may be adaptive/data--dependent as in \citep{George2000}. An
alternative to model selection criterion is to select models based on their posterior probability, such as by
selecting the median probability model as in \citep{Barbieri2004}. In a Bayesian context is to use Bayes
factors to compare the posterior likelihoods of the candidate models to see which is most probable given the
observed data. This can be done, for example, by using Bayes Factors as in \citep{Kass1993}. Rather than
selecting one candidate model, several models can be combined together using Bayesian model  averaging, as in
\citep{Hoeting1999}, \citep{Raftery1997}, \citep{Fernandez2001} or \citep{Papaspiliopoulos2016}. Or model
selection can be made implicit in the model fitting process itself, as in ridge regression \citep{Casella1980},
of which the well-known lasso is a special case \citep{Tibshirani1996}. As \citep{Breiman1996} and
\citep{Efron2013} showed, while  the standard formulation of a linear model is unbiased, the goodness of fit of
these models is numerically  unstable. Breiman showed that by introducing a penalty on the size of the
regression co- efficients such as  in ridge regression, this numerical instability can be avoided. This
reduces the variances of the co-efficient estimates, at the expense of introducing some bias --- 
which is another instance of the bias--variance trade--off.

Many approaches to model selection have been investigated, with various model selection criteria proposed to
attempt to balance model likelihood against model complexity. One approach to model selection is to select
between entire models, using a model selection criterion. These criterion may have a fixed penalty for model
complexity, such as Akaike's Information Criterion \citep{Akaike1974}, the Risk Inflation Criterion
\citep{Foster1994}, the Schwarz criterion or Bayesian Information Criterion \citep{Schwarz1978}, the Deviance
Information Criterion \citep{Spiegelhalter2016} or the Principle of Minimum Description Length
\citep{Hansen2001}. Alternatively, the penalty may be adaptive/data--dependent as in \citep{George2000}.

An alternative to model selection criterion is to select models based on their posterior probability. In a
Bayesian context, this can be accomplished by comparing the posterior likelihoods of the candidate models to
see which is most probable given the observed data. This can be done, for example, by using Bayes Factors as
in \citep{Kass1993}. Alternately, rather than selecting one candidate model, several models can be combined together using Bayesian model  averaging, as in \citep{Hoeting1999}, \citep{Raftery1997}, \citep{Fernandez2001} or \citep{Papaspiliopoulos2016}.

Model selection can also be made implicit in the model fitting process itself, as in ridge regression
\citep{Casella1980}, of which the well-known lasso is a special case \citep{Tibshirani1996}.

As shown in
papers by \citep{Breiman1996} and \citep{Efron2013} showed, while  the standard formulation of a linear model
is unbiased, the goodness of fit of these models is numerically  unstable. Breiman showed that by introducing
a penalty on the size of the regression co- efficients such as  in ridge regression or the lasso, this numerical
instability can be avoided. This reduces the variances of the co-efficient estimates, at the expense of
introducing some bias --- the bias-- variance trade--off.

A special case of model selection is variable selection, where the focus is on selecting individual
covariates, rather than entire models. This approach can either be Fully Bayesian or Empirically Bayesian as
in \citep{Cui2008}. Variable selection approaches involve a stochastic search over the variables in the model
space. This search can be driven by posterior probabilities, as in \citep{Casella2006}, or by Gibbs sampling
approaches such as in \citep{George1993}. These two approaches of model selection and variable selection can
be combined, as in \citep{Geweke1996}. Variable selection can also be accomplished by selecting the median
probability model, consisting of those models whose posterior inclusion probability is at least $1/2$, as in
\citep{Barbieri2004}.

Another approach to model selection is to focus on selecting individual covariates, rather than entire
models. This approach can either be Fully Bayesian or Empirically Bayesian as in \citep{Cui2008}. Variable
selection approaches involve a stochastic search over the variables in the model space. This search can be
driven by posterior probabilities, as in \citep{Casella2006}, or by Gibbs sampling approaches such as in
\citep{George1993}. These two approaches of model selection and variable selection can be combined, as in
\citep{Geweke1996}.

A challenge to applying this method of model selection is that exact model fitting may be computationally
infeasible for models involving even moderate numbers of observations and covariates, and popular alternatives
for fitting Bayesian models such as Monte Carlo Markov Chains (henceforth referred to as MCMC) are still
extremely computationally intensive.

Variational Bayes (see \citep{Ormerod2010}) is a computationally
efficient, deterministic method of fitting Bayesian models to data. Variational Bayes approximates the true
posterior $p(\vy, \vtheta)$ by minimising the KL divergence between the posterior and the  approximating
distribution $q(\vtheta)$.

A linear model with normal priors allows exact inference on the regression and model selection parameters in
closed form, which might appear to negate the beenfits of a variational approximation to the model. However,
the performance of our variational approximation should remain similiar if the priors are altered to cater for
complications such as robustness, while exact Bayesian inference calculations are no longer possible in closed
form in these situations.

\citep{Zellner1986} suggested a particular form of conjugate Normal-Gamma family where the Bayes factors have a
relatively simple form, incorporating a parameter $g$ to control mixing between the model fit from the data
and a prior specification of model fit. This immediately raises the question of how $g$ should be chosen, and
whether it should be fixed or have a prior specification. \citep{Liang2008} showed that fixed choices of $g$
lead to paradoxes such as Bartlett's Paradox and the Information Paradox, and so a prior specification should
be preferred. There are many ways of choosing a prior on $g$. Using a mixture of $g$-priors has the advantage
of adapting the degree of shrinkage to the prior model dependent on the data.

\section{Models and Assumptions}

\section{Frequentist Approaches to Model Selection}
\subsection{Information criteria}

In a frequentist context, there are many functions which can be used to judge which model is best,
such as AIC, BIC etc. These are functions $f: \gamma \to \R^+$ which allow the models under consideration to
be ranked, and the best model chosen from those available. Thus the optimal model selected by an information
criteria is	$\gamma^* = \min_\gamma f(\gamma)$. These functions typically attempt to balance log likelihood 
against the complexity of the model, achieving a compromise between each.

\mgc{AIC, BIC, DIC, Mallow's $C_p$}

Information criteria are frequently used to compare amongst models. Let $\vgamma$ denote the candidate model.
The Information Criteria take the form "-2 times the log-likelihood plus a term penalising for complexity of
the model"
\[
	\text{Information Criteria} = -2 \log p(\vy | \hat{\vtheta}_\vgamma) + \text{complexity penalty}
\]

where $\log p(\vy | \hat{\vtheta_\gamma})$ is the log-likelihood of the model and the complexity penalty is
a function of the sample size $n$ and the number of parameters $p$ of the model. Information criteria attempt
to successfully compromise between goodness of fit and model complexity.

The most popular of the information criteria is the Akaike Information Criterion (AIC) \cite{Akaike1974}. AIC
calculates an estimate of the information lost when a given model is used to represent the process that
generates the data and so is an estimator of the Kullback-Leibler divergence of the true model from the fitted
model. The AIC of the model $\vgamma$ is defined as
\[
	\text{AIC}(\vgamma) = -2 \log p(\vy | \hat{\vtheta}_\vgamma) + 2 p_\vgamma
\]

where $|\vgamma|$ is the number of parameters in the model $\vgamma$. The model with the lowest AIC is selected
as the `best`.

Of a similiar form as the AIC, but derived via a more Bayesian framework is the Bayesian Information Criterion
or BIC. The BIC approximates the posterior probability of the candidate model $\vgamma$. The BIC is defined as
\[
	\text{BIC}(\vgamma) = -2 \log p(\vy | \hat{\vtheta}_\vgamma) + p_\vgamma \log(n).
\]

This is a more severe penalty for model complexity than in the Akaike's Information Criteria when $n$ is
greater than $8$. BIC can be shown to be approximately equivalent to model selection using Bayes Factors
in certain contexts, as shown by \cite{Kass1993}.

% Non-Bayesian
\subsection{Frequentist Approachs to Model Selection}
Many approaches to model selection have been investigated, with various model selection criteria proposed to
attempt to balance model likelihood against model complexity. One approach to model selection is to select
between entire models, using a model selection criterion. These criterion may have a fixed penalty for model
complexity, such as Akaike's Information Criterion \citep{Akaike1974}, the Risk Inflation Criterion
\citep{Foster1994}, the Schwarz criterion or Bayesian Information Criterion \citep{Schwarz1978}, the Deviance
Information Criterion \citep{Spiegelhalter2016} or the Principle of Minimum Description Length
\citep{Hansen2001}. Alternatively, the penalty may be adaptive/data--dependent as in \citep{George2000}.

Model selection can also be made implicit in the model fitting process itself, as in ridge regression
\citep{Casella1980}, of which the well-known lasso is a special case \citep{Tibshirani1996}. As shown in
papers by \citep{Breiman1996} and \citep{Efron2013} showed, while  the standard formulation of a linear model
is unbiased, the goodness of fit of these models is numerically  unstable. Breiman showed that by introducing
a penalty on the size of the regression co- efficients such as  in ridge regression, this numerical
instability can be avoided. This reduces the variances of the co-efficient estimates, at the expense of
introducing some bias -- the bias- variance trade--off.

\subsection{Bayesian Approaches to Model Selection}
An alternative to model selection criterion is to select models based on their posterior probability. In a
Bayesian context, this can be accomplished by comparing the posterior likelihoods of the candidate models to
see which is most probable given the observed data. This can be done, for example, by using Bayes Factors as
in \citep{Kass1993}. Rather than selecting one candidate model, several models can be combined together using
Bayesian model averaging, as in \citep{Hoeting1999}, \citep{Raftery1997}, \citep{Fernandez2001} or
\citep{Papaspiliopoulos2016}.

The problem of model selection can also be considered within a Bayesian framework.
Within this framework, candidate models can be selected by comparing their posterior likelihoods to see
which is most probable given the observed data, as in \citep{Kass1993}. Bayesian model selection approaches
can incorporate prior information, such as the prior probability of selecting each candidate model $\vgamma$.

A special case of model selection is variable selection, where the focus is on selecting individual
covariates, rather than entire models. Variable selection approaches search over the
variables in the model space for the best covariates to include in the candidate model. Due to the large
number of possible combinations of covariates -- typically $2^p$ where $p$ is the number of covariates, such
searches are often stochastic. This approach can either be Fully Bayesian or Empirically Bayesian as in
\citep{Cui2008}.  This search can be driven by posterior probabilities, as in \citep{Casella2006}, or by Gibbs
sampling approaches such as in \citep{George1993}. These two approaches of model selection and variable
selection can be combined, as in \citep{Geweke1996}. Variable selection can also be accomplished by selecting
the median probability model, consisting of those models whose posterior inclusion probability is at least
$1/2$, as in \citep{Barbieri2004}.

\subsection{Criteria AIC/BIC}
\subsection{Bayesian (Post model selection inference)}
\subsection{Penalised regression}
\subsubsection{Lasso}
\subsubsection{Ridge}
\subsubsection{Why parsimony? Model selection and fitting}

\section{Approximate inference}
\subsection{Variational Bayes}
\section{Variational approximations}

\subsection{Definition}

Consider a model $p(\vtheta|\vy)$ which is of interest to us but computationally difficult or intractable to 
fit. We may be able to gain much of the same insight from a given data set by fitting an accurate approximation 
of the model, allowing us to summarise the data and perform statistical inference.

Call the approximating model $q(\theta)$. Then we seek a $q(\theta)$ which approximates $p(\vtheta|\vy)$
as closely as possible in some sense. A useful measure of distance between probability distributions is the
Kullback-Leibler divergence, which is defined as
\[
	\KL(q || p) \equiv \int q(\vtheta) \log{\frac{q(\theta)}{p(\theta)}} d \vtheta.
\]

We may seek a best approximating distribution $q^*(\vtheta)$ to a given posterior distribution
$p(\vtheta|\vy)$ from a parameteric family of approximating distributions $Q$ by finding
\[
	q^*(\vtheta) = \argmin_{q \in Q} \KL \{ {q(\vtheta) || p(\vtheta|\vy)} \}.
\]

If $\vtheta$ is partitioned into $M$ partitions $\vtheta_1$, $\vtheta_2$, \ldots, $\vtheta_M$ then a 
simple form of approximation to adopt is the factored approximation of the form
\[
	q(\vtheta) = \Pi_{i=1}^M q(\vtheta_i)
\]

where each of the density $q(\vtheta_i)$ is a member of a parametric family of density functions. This form of
approximation is computationally convenient, but assumes that the partitions of $\vtheta$ are completely
independent of one another.

The optimal mean field update for each of the parameters $\vtheta_i$ can be shown to be
\[
	q^*(\vtheta_i) \propto \exp \{ \E [\log p(\vy; \vtheta)] \}.
\]

For details of the proof, and a more thorough introduction to the topic of variational approximations, see
\cite{Ormerod2010}.

\subsection{Gaussian Variational Approximation}

In cases where there is a strong dependence between partitions of $\vtheta$, such as between the parameters
$\vmu$ and $\mSigma$ in a hierarchical Gaussian model, a factored approximation may not approximate the true
distribution accurately. In this case, an alternate form of approximation may be used with the parameters
considered together to take their dependence into account. One such form of approximation is the Gaussian
Variational Approximation \cite{Ormerod2012}, which assumes that the distribution of the parameters being 
approximated is multivariate Gaussian. The covariance matrix of the Gaussian allows the approximation to
capture the dependence amongst the elements of $\vtheta$, which increases the accuracy of the variational
approximation relative to the factored approximation.

\subsection{Laplace's Method of approximation}

Laplace's method of approximation is used to approximate integrals of a unimodal function with negative second
derivative at the mode, indicating that the function is decreasing rapidly away from this point. The essential
idea is that if the function is decreasing rapidly away from the mode, the bulk of the area under the function
will be within a neighbourhood of the mode. Thus the integral of the function can be well approximated by an
integral over the neighbourhood of the mode. How large that neighbourhood needs to be is estimated using
how fast the function is changing at the mode, which is estimated by $|f''(x_0)|$.

Consider an exponential integral of the form
\[
	\int_a^b e^{M f(x)} dx
\]

where $f(x)$ is twice differentiable and $f''(x_0) < 0$, $M \in \R$ and $a, b \in \R \cup \{-\infty, \infty\}$.

Let $f(x)$ have a unique global maximum at $x_0$. Then, Taylor expanding about $x_0$, we have

\[
	f(x) = f(x_0) + f'(x_0) (x - x_0) + \frac{1}{2} f''(x_0) (x - x_0)^2 + R
\]

where $R = \BigO((x - x_0)^3)$.

As $f$ has a global maximum at $x_0$, the first derivative of $f$ is zero at $x_0$. Thus, the function $f(x)$
may be approximated by
\[
	f(x) \approx f(x_0) - \frac{1}{2} |f''(x_0)| (x - x_0)^2
\]

for $x$ sufficiently close to $x_0$, as the second derivative is negative at $x_0$. This ensures the
the approximation of the integral

\[
	\int_a^b e^{M f(x)} dx \approx e^{M f(x_0)} \int_a^b e^{-M |f''(x_0)|(x - x_0)^2} dx
\]

is accurate. This is a Gaussian integral, and thus we find that

\[
	\int_a^b e^{M f(x)} dx \approx \sqrt{\frac{2 \pi}{M |f''(x_0)|}} e^{M f(x_0)}
\]

Thus we have approximated our integral by a closed form expression.

\subsubsection{Relative error} % What to write about this? Include the justification/proof?
The approximation above is $O(1/M)$. % See the Relative error section of the Wikipedia page, for instance.
The approximation can be made more accurate by using a Taylor
expansion beyond second order.

\subsubsection{Extending to multiple dimensions}
This approach to approximating integrals extends naturally to multiple dimensions.
Consider the second order Taylor expansion of $\log f(\vtheta)$ around the mode $\vtheta_m$
$$
\log f(\vtheta) \approx f(\vtheta_m) + (\vtheta - \vtheta_m) \nabla \log f(\vtheta) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\|\vtheta - \vtheta_m\|^3).
$$

Assuming that $\vtheta$ is a stationary point of $\log f$, then $\nabla f(\vtheta) = \vzero$ and so
$$
\log f(\vtheta) \approx f(\vtheta_m) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\|\vtheta - \vtheta_m\|^3)
$$

at such a point. The quadratic form in $\vtheta$ in the approximate expression for the log likelihood above
leads to a Gaussian approximation for the likelihood $\N(\vtheta_m, \mH_{\log f}(\vtheta_m)^{-1})$.

The approximation is crude but can be quite accurate if the likelihood is symmetric and unimodal.
\subsection{Other methods: Expectation Propagation (in broad terms)}
\section{My Contributions}
\subsection{Outline}

\section{Introduction}


\subsection{Definitions}

Let $p$ be the dimension of the space of fixed effects, $m$ be the number of individuals in the random effects
and $b$ be the block size for each of those individuals. We use $\vone_p$ and $\vzero_p$ to denote the $p
\times 1$ column vectors with all entries equal to 1 or 0, respectively.

Let $\vy$ be the $n \times 1$ vector. The norm of a column vector $\vv$, defined to be $\sqrt{\vv^\top \vv}$,
is  denoted by $\|\vv\|$. For a $p \times 1$ vector $\va$, we let $\diag{(\va)}$ denote the $p \times p$
matrix with the elements of $\va$ along its' diagonal.

We denote the design matrix of fixed effects with dimensions $n \times p$ as $\mX$ is , and the design matrix
of random  effects with dimensions $n \times m b$ as $\mZ$. The combined design matrix $\mC$ is formed by
appending the columns of $\mX$ to the columns of $\mZ$, giving $\mC = [ \mX, \mZ ]$.

Let $\vtheta$ is the vector of all parameters.
Let $\vbeta$ be the $p \times 1$ column vector of fixed
effects, and $\vu$ the $m b \times 1$ column vector of random effects. $\vnu$ is the
concatenation of these vectors $[\vbeta^\top, \vu^\top]$.
% Let $\vp$ be the $n \times 1$ column vector of probabilities that each observation in $\vy$ is
% non-zero.

Let $\mSigma$ be the covariance matrix of the random effects $\vu$,
and 
$\mPsi$ the covariance matrix prior on $\mSigma$.
These matrices are all of dimension $(p + m b) \times (p + m b)$.


$\expit(x)$ denotes the function $\tfrac{1}{1 + \exp(-x)}$ which is the inverse of the logit
function.

$\text{Bernoulli}(\pi)$ denotes the probability distribution $\pi^k (1 - \pi)^{1-k}$ and
$\text{Inverse Wishart}(\mPsi, v)$ denotes the probability distribution
$$\tfrac{|\mPsi|^\frac{v}{2}}{2^{\frac{vp}{2}} \Gamma_p{(\tfrac{v}{2})}} |\mX|^{-\tfrac{v + p + 1}{2}}
\exp{[-\half \tr{(\mPsi \mX^{-1})}]}$$ where $\Gamma_p{(x)}$ denotes the multivariate gamma function and $\tr$
is the trace function.

\subsection{Bias of MLE}

How does this relate to Bayesian estimation? Maybe it's just interesting background.
Bernstein-von Mises Theorem

\bibliographystyle{elsarticle-harv}
% \bibliography{Chapter_0_preliminary}
\bibliography{references_mendeley}

\end{document}
