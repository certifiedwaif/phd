% \documentclass{amsart}[12pt]
\documentclass{usydthesis}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{color}

\title{Preliminaries}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}

\newcommand{\mgc}[1]{{\color{blue}#1}}
\newcommand{\joc}[1]{{\color{red}#1}}

\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Introduction}

Linear and generalised linear regression models are the standard tools used by applied statisticians to
explain the relationship between an outcome variable and one or more explanatory variables. They provide a
general method  to analyse quantified relationships between variables within a data set in an easily
interpretable way. A standard assumption is that the outcomes are independent, and that the effect of the
explanatory variables on the outcome is fixed. But if the outcomes are dependent and this assumption is not
met, then linear and generalised linear models can be extended to linear mixed models. These allow us to
incorporate dependencies amongst the  observations via the assumption of a more complicated covariance
structure, including random effects for  different subgroups or longitudinal data and other extensions such as
splines, missing data and measurement error. This additional flexibility makes their application popular in
many fields, such as public health, psychology and agriculture.

While mixed models are very useful for gaining insight into a data set, fitting them can be computationally
challenging. For all but the simplest situations, fitting these models involves computing high-dimensional
integrals which are often analytically and computationally intractable. Thus, approximations must be used.

In the frequentist paradigm, model parameters are fixed and uncertainty enters the model through random
errors, which have associated variance. The data is modelled as a combination of these fixed parameters and
random errors. In the Bayesian paradigm, the uncertainty enters the model by assuming parameters are random
variables, while the data is fixed.

Model selection -- the task of selecting a statistical model from a set of candidate models given data, is one
of the fundamental tasks of scientific inquiry. It is one of the central tasks of applied statistical
analysis, and there is a correspondingly large literature on the subject. The problem of model selection for
normal linear models is particularly well studied, owing to the popularity and importance of normal linear
models in applications.
There are many approaches to model selection, including criteria based approaches,
variable selection,
penalised regression,
and Bayesian modelling approaches. Model selection is a difficult problem in high--dimensional space in general
because as the dimension of the space increases, the number of possible models increases combinatorially.
Many model selection algorithms use heuristics in an attempt to search the model space more efficiently but
still find an optimal or near-optimal model.

\mgc{You have so much work to do on this chapter.}

\section{Variational approximations}

\subsection{Definition}

Consider a model $p(\vtheta|\vy)$ which is of interest to us but computationally difficult or intractable to 
fit. We may be able to gain much of the same insight from a given data set by fitting an accurate approximation 
of the model, allowing us to summarise the data and perform statistical inference.

Call the approximating model $q(\theta)$. Then we seek a $q(\theta)$ which approximates $p(\vtheta|\vy)$
as closely as possible in some sense. A useful measure of distance between probability distributions is the
Kullback-Leibler divergence, which is defined as

$$
\KL(q || p) \equiv \int q(\vtheta) \log{\frac{q(\theta)}{p(\theta)}} d \vtheta.
$$

We may seek a best approximating distribution $q^*(\vtheta)$ to a given posterior distribution
$p(\vtheta|\vy)$ from a parameteric family of approximating distributions $Q$ by finding

$$
q^*(\vtheta) = \argmin_{q \in Q} \KL \{ {q(\vtheta) || p(\vtheta|\vy)} \}.
$$

% Mean field update
Factored approximation
Mean field update

A more thorough introduction to the topic of variational approximations is available in \cite{Ormerod2010}.

\subsection{Laplace's Method of approximation}

\subsubsection{Definition}

Consider the second order Taylor expansion of $\log f(\vtheta)$ around the mode $\vtheta_m$
$$
\log f(\vtheta) \approx f(\vtheta_m) + (\vtheta - \vtheta_m) \nabla \log f(\vtheta) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\|\vtheta - \vtheta_m\|^3).
$$

Assuming that $\vtheta$ is a stationary point of $\log f$, then $\nabla f(\vtheta) = \vzero$ and so
$$
\log f(\vtheta) \approx f(\vtheta_m) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO(\|\vtheta - \vtheta_m\|^3)
$$

at such a point. The quadratic form in $\vtheta$ in the approximate expression for the log likelihood above gives 
rise to a Gaussian approximation for the likelihood $\N(\vtheta_m, \mH_{\log f}(\vtheta_m)^{-1})$.

The approximation is crude but can be quite accurate if the likelihood is symmetric and unimodal.

\subsection{Newton-Raphson}

\subsubsection{Definition}

Consider the second order Taylor expansion of $f(\vtheta)$ around $\vtheta_{n}$

$$
f(\vtheta) \approx f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta x^\top \mH_f(\vtheta_{n}) \Delta x + \sO((\Delta \vtheta_{n})^3).
$$

This expression attains its' maxima with respect to $\Delta x$ when

$$
\vzero = \mD_{\Delta \vtheta} [f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta \vtheta^\top \mH_f(\vtheta_{n}) \Delta \vtheta] = \nabla f(\vtheta_n) + \mH_f(\vtheta_n) \Delta \vtheta.
$$

Thus taking $\Delta \vtheta = -\nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$, we hope that choosing
$\vtheta_{n + 1} = \vtheta_{n} - \nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$ will be closer to the stationary 
point $\vtheta^*$ of $f$. This choice of $\vtheta_{n+1}$ establishes an iterative algorithm for finding the 
maxima/minima of $f$.

Convergence is achieved when the step length $\| \vtheta_{n+1} - \vtheta_{n} \| < \epsilon$, for some
suitably small choice of $\epsilon$, such as $\epsilon = 10^{-8}$.

\subsubsection{Time and memory complexity}

Execution of this algorithm requires the calculation of a vector addition, dot product and solution of a
linear system for each iteration. The vector addition and dot product can both be performed in $p$ flops,
where $p$ is the dimension of the problem. The expensive part of the computation is the solution of the linear
system $\mH_f(\vtheta_n) \Delta \vtheta$, which takes $\sO(p^3)$ flops. Even on modern computer systems, this
can quickly become computationally intractable for problems of modest size. This motivates the development
of Quasi-Newton algorithms, such as BFGS, L-BFGS and L-BFGS-B, discussed in Section \ref{sec:quasi_newton}.

\subsubsection{Radius of convergence and convergence to local solutions}

\subsection{Quasi-Newton Rapson algorithms}
\label{sec:quasi_newton}
L-BFGS-B

\subsubsection{Definition}

% Wolfe conditions
The problem of interest is to find

$$
\min_\vx f(\vx)
$$

for some smooth $f: \R^n \to \R$. Each step involves approximately solving the subproblem

$$
\min_\alpha f(\vx_k + \alpha \vp_k)
$$

where $\vx_k$ is the current best guess, $\vp_k \in \R^n$ is a search direction and $\alpha$ is the
step length.

Denote a univariate function $\phi$ restricted to the direction $\vp_k$ as
$\phi(\alpha) = f(\vx_k + \alpha \vp_k)$. A step length $\alpha_k$ is said to satisfy the \emph{Wolfe conditions}
is the following two inequalities hold:

\begin{enumerate}
\item[(i)] $f(\vx_k + \alpha_k \vp_k) \leq f(\vx_k) + c_1 \alpha_k \vp_k^\top \nabla f(\vx_k)$ \text{(Armijo rule)}
\item[(ii)] $\vp_k^\top \nabla f(\vx_k + \alpha_k \vp_k) \geq c_2 \vp_k^\top \nabla f(\vx_k)$ \text{(curvature condition)}
\end{enumerate}

The first condition ensures that the step length $\alpha_k$ decreases $f$ sufficiently, while the second condition
ensures that the slope had been sufficiently reduced.

The rationale for imposing the Wolfe conditions in an optimisation algorithm where
$\vx_{k+1} = x_k + \alpha \vp_k$ is to ensure the convergence of the gradient to zero. In particular, if the
cosine of the angle between $\vp_k$ and the gradient,

$$
\cos \theta_k = \frac{\nabla f(\vx_k)^\top \vp_k}{\|\nabla f(\vx_k)\| \|\vp_k\|}
$$

is bounded away from zero and the Wolfe conditions hold, then $\nabla f(\vx_k) \to 0$.

An additional motivation, in the case of a quasi-Newton method is that if $\vp_k = - \mB_k^{-1} \nabla f(\vx_k)$,
where the matrix $\mB_k$ is updated by the BFGS or DFP formula, then if $\mB_k$ is positive definite condition ii
implies $\mB_{k+1}$ is also positive definite.

\subsubsection{Time and memory complexity}

\section{Numerical matters}

In this section, I'd like to talk about floating point representation and its' consequences, numerical
analysis/error bounds and in particular their relevance to numerical linear algebra.

\subsection{Floating point numbers}

$$\text{sign } \text{significand} \times \text{base}^\text{exponent}$$

IEEE 754: floating point in modern computers

On modern computers, IEEE 754 is ubiquitous, and the base is $2$.

Error bounds on $+, -, *, /, \sqrt{}$

$$
\text{fl}(a \text{ op } b) = (1 + \epsilon) (a \text{ op } b) \text{ provided $\text{fl}(a \text{ op } b)$ neither overflows nor underflows}
$$

where $\epsilon$ is the machine epsilon, the minimum difference between $1$ and the next representable
floating point number.

Exceptional values: $\pm \infty$, $\text{NaN}$

When doing arithmetic on computers, it is important to understand the full ramifications of this.

\begin{enumerate}
\item All numerical values are in fact approximations within an interval.
\item Every arithmetic operation has an associated amount of error, and these errors accumulate as more
			arithmetic operations are performed.
\end{enumerate}

The field of numerical analysis (references Classical and Modern Numerical Analysis,
Demmel's course: http://www.cs.berkeley.edu/~demmel/cs267/lecture21/lecture21.html, Kahan) analyses numerical 
algorithms to assess their error bounds, and attempt to mitigate these problems as much as possible to allow
us to solve numerical problems on modern computers.

\subsection{Truncation and round-off error}
\subsection{Numerical stability}
\subsection{Numerical differentiation and integration}
\subsubsection{Trapezoidal integration}

Trapezoidal integration is a method of approximating the integral of a function $f: \R \to \R$ over the
interval $(a, b)$. First, the interval is partitioned into $x_1, x_2, \ldots, x_{n-1}, x_n$, and then

\[
	\int_a^b f(x) dx \approx \sum_{i=1}^{n-1} \frac{1}{2} \frac{f(x_i) + f(x_{i+1})}{x_{i+1} - x_i}
\]

Note that the points $x_i$ can be chosen arbitrarily within the interval $(a, b)$, and do not need to be
evenly spaced. \mgc{What's the error bound? Error analysis using Taylor's theorem?}

\subsection{Numerical linear algebra}
Condition numbers
LDU decomposition
Cholesky decomposition
\subsection{Difficulty of inverting linear systems}

Sparsity of a matrix does not ensure sparsity of its' inverse.
Inverting a matrix is an inherently unstable operation. The number of floating point operations involved
leads to magnification of error.

\section{Positive semi-definite matrices}
A positive semidefinite matrix $\mLambda$ is a square matrix of size $n \times n$ such that for any vector
$\vx \in \R^n$, $\vx^\top \mLambda \vx \geq 0$. Equivalently, all eigenvalues of $\mLambda$ are greater than or 
equal to 0.

\section{Matrix calculus}

Matrix calculus is a very useful and compact notation for the derivatives of functions of vectors and matrices.
A useful introduction is \cite{wand02}, while a more comprehensive treatment can be found in
\cite{MagnusNeudecker99}.

\subsection{Notation}

Given a function $f:\R^{n \times n} \to \R$ of a matrix $\mLambda$, we denote the matrix derivative of $f$ with 
respect to $\mLambda$ as $\mD_\mLambda f$, and the Hessian of $f$ with respect to $\mLambda$ as
$\mH_\mLambda f$ the Hessian with respect to $\mLambda$.

The Hessian matrix is a $d \times d$ matrix whose $(i, j)$-th entry is equal to

$$
\frac{\partial^2}{\partial x_i x_j} f(\vx)
$$

and is denoted by $\mH f(\vx)$.

\section{Splines}
The most general form of the univariate regression problem is

$$
y_i = f(x_i)
$$

where $f: \R \to \R$ is unknown, and we wish to estimate it as $\hat{f}$. We treat the functional form of
$f$ as unknown, and attempt to estimate it purely from the data available to us. Fully non-parametric regression
is a difficult problem to solve, but the problem can be simplified by pre-specifying the points at which the
function may change curvature, the so-called knots.

\subsection{B-Splines}
% This is taken from the Wikipedia page on the subject
A B-Spline is a piecewise polynomial function of degree $< n$ in a variable $x$. It is defined over a
domain $t_0 \leq x \leq t_m, m=n$. The points where $x = t_j$ are known as knots or break-points. The
number of internal knots is equal to the degree of the polynomial if there are no knot multiplicities.
The knots must be in ascending order. The number of knots is the minimum for the degree of the B-spline,
which has a non-zero value in the range between the first and last knot. Each piece of the function is a
polynomial of degree $< n$ between and including adjacent knots. A B-Spline is a continuous function at the
knots. When all internal knots are distinct its derivatives are also continuous up to the derivative of degree
$n - 1$. If internal knots are coincident at a given value of x, the continuity of derivative order is reduced
by 1 for each additional knot.

For any given set of knots, the B-spline is unique, hence the name, B being short for Basis. The usefulness
of B-splines lies in the fact that any spline function of order $n$ on a given set of knots can be expressed
as a linear combination of B-spline:

% B-Splines
$$
S_{n, t}(x) = \sum_i \alpha_i B_{i, n}(x)
$$

where

\begin{align*}
B_{i, 0}(x) &:= \begin{cases}
1 & \text{if } t_i \leq x < t_{i+1} \\
0 & \text{otherwise}
\end{cases} \\
B_{i, k}(x) &:= \frac{x - t_i}{t_{i + 1} - t_i} B_{i, k-1} (x) + \frac{t_{i + k + 1} - x}{t_{i + k + 1} - t_{i + 1}} B_{i, k-1} (x).
\end{align*}

\subsection{O'Sullivan Splines}

Let $B_1$, \ldots, $B_{K+4}$ be the cubic B-spline basis functions defined by the knots $\kappa_1$ to
$\kappa_{K+4}$

$B_{ik} = B_k (x)$
$B_x = [B_1(x), \ldots, B_K+4(x)]$

O'Sullivan splines are penalised splines which are penalised using the $\mOmega$ matrix.

Let $\mOmega$ be the $(K+4) \times (K+4)$ matrix where the $(k, k')-th$ element is
\[
	\mOmega_{k k'} = \int_a^b B''_k(x) B''_{k'}(x) dx.
\]
Then the O'Sullivan spline estimate of the true function $f$ at the point $x$ is
$\hat{f}_O(x; \lambda) = \mB_x \hat{\vnu}_O$, where
$\hat{\vnu}_O = (\mB^\top \mB + \lambda \mOmega)^{-1} \mB^\top \vy$.

$\mOmega$ is defined in this way to penalise oscillation, which is measured by the second derivative.

The penalty differs from P-splines in that the P-spline penalty matrix is $\mD_2^\top \mD_2$ where $\mD_2$ is
the second-order differencing matrix.

% Divided difference notation?
% Lagrange's interpolating polynomials?
% Semiparametric regression / Connection to mixed models
We follow the discussion of semiparametric regression in \cite{RuppertWandCarroll}.
Using a mixed models setup to fit spline models protects against overfitting.
Constructing an $\mZ$ matrix with the appropriate B-Spline function evaluations in each of the rows, where
each column corresponds to a knot.

\subsection{Degrees of freedom}

% From Section 2.5.2, Semiparametric regression

Consider the fixed effects model $\vy = \mX \vbeta$ where $\mX \in \R^{n \times p}$. If the hat matrix is
defined as $\mH = \mX (\mX^\top\mX)^{-1} \mX^\top$ then

\begin{align*}
\tr(\mH) &= \tr[\mX(\mX^\top\mX)^{-1}\mX^\top] \\
&= \tr[\mX^\top\mX (\mX^\top\mX)^{-1}] \\
&= \tr(\mI_p) \\
&= p
\end{align*}

which is the number of parameters in the fixed effects model. Thus we use the trace of the hat matrix as
the definition of the effective degrees of freedom of the model. This definition then naturally extends to
more general smoother matrices which can be used to fit models involving splines or other kinds of smoothing.

\section{Generalised Linear Mixed models}

\subsection{Exponential family}

An exponential family is a set of probability distributions of the form
$$
f_X(x|\vtheta) = h(x) g(\theta) \exp(\eta(\theta) \dot T(x)).
$$

Most commonly used probability distributions in statistics such as normal, exponential, gamma,
chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, Wishart and Inverse Wishart can
be represented in this form.

Canonical form
$$
\log p(\vnu; \vy) = \vy^\top \mC \vnu - b(\mC \vnu)
$$

Different choices of the function $b:\R \to \R$ lead to different generalised linear mixed models. A set
of available functions and corresponding distributions is presented in Table \ref{tab:b_functions}.

\begin{table}
\caption{Table of b functions}
\label{tab:b_functions}
\begin{tabular}{|l|lll|}
\hline
Distribution & Parameter & Natural parameter & Inverse parameter mapping \\
\hline
Bernoulli distribution & $p$ & $\log{\frac{p}{1 - p}}$ & $\frac{1}{1 + e^\eta}$ \\
Poisson distribution & $\lambda$ & $\log \lambda$ & $e^\eta$ \\
Normal distribution (known variance) & $\mu$ & $\frac{\mu}{\sigma}$ & $\sigma \eta$ \\
\hline
\end{tabular}
\end{table}

\subsection{Bias of MLE}

How does this relate to Bayesian estimation? Maybe it's just interesting background.
Bernstein-von Mises Theorem

\section{Model selection}

\subsection{Frequentist}

Given a full data matrix $\mX \in \R^{n \times p}$, we may wish to find the subset of variables $\gamma \in \{
1 \ldots p \}$ such that $\mX_\gamma \vbeta_\gamma$ describe the data in $\vy$ best, in some sense.

In a frequentist context, there are many functions which can be used to judge which model is best,
such as AIC, BIC etc. These are functions $f: \gamma \to \R^+$ which allow the models under consideration to
be ranked, and the best model chosen from those available. Thus the optimal model under such a measure
f is
$$
\gamma^* = \max_\gamma f(\gamma)
$$

These functions typically attempt to balance log likelihood against the complexity of the model, achieving
a compromise between each.

\mgc{AIC, BIC, DIC, Mallow's $C_p$}

\subsection{Bayesian model selection}

\mgc{Normal model selection with $g$--priors?}

\section{Hamiltonian Monte Carlo}

\cite{Betancourt2017}

Basic idea: Sample the typical set efficiently by following a contour/level set of the likelihood surface
Hamiltonian is a mixture of the position and momentum
If you can't write any more clearly than this, then you don't understand well enough yet

\bibliographystyle{elsarticle-harv}
% \bibliography{Chapter_0_preliminary}
\bibliography{references_mendeley}

\end{document}
