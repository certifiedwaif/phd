\documentclass{article}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\title{Preliminaries}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\setlength{\parindent}{0pt}
\maketitle

\section{Variational approximations}

\subsection{Definition}

Consider a model $p(\vtheta|\vy)$ which is of interest to us but computationally difficult or intractable to 
fit. We may be able to gain much of the same insight from a given data set by fitting an accurate approximation 
of the model, allowing us to summarise the data and perform statistical inference.

Call the approximating model $q(\theta)$. Then we seek a $q(\theta)$ which approximates $p(\vtheta|\vy)$
as closely as possible in some sense. A useful measure of distance between probability distributions is the
Kullback-Leibler divergence, which is defined as

$$
\KL(q || p) \equiv \int q(\vtheta) \log{\frac{q(\theta)}{p(\theta)}} d \vtheta.
$$

We may seek a best approximating distribution $q^*(\vtheta)$ to a given posterior distribution
$p(\vtheta|\vy)$ from a parameteric family of approximating distributions $Q$ by finding

$$
q^*(\vtheta) = \argmin_{q \in Q} \KL \{ {q(\vtheta) || p(\vtheta|\vy)} \}.
$$

% Mean field update
Factored approximation
Mean field update

A more thorough introduction to the topic of variational approximations is available in \cite{ormerod10}.

\subsection{Laplace's Method of approximation}

\subsubsection{Definition}

Consider the second order Taylor expansion of $\log f(\vtheta)$ around the mode $\vtheta_m$

$$
\log f(\vtheta) \approx f(\vtheta_m) + (\vtheta - \vtheta_m) \nabla \log f(\vtheta) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO((\vtheta - \vtheta_m)^3).
$$

Assuming that $\vtheta$ is a stationary point of $\log f$, then $\nabla f(\vtheta) = \vzero$ and so

$$
\log f(\vtheta) \approx f(\vtheta_m) + \half (\vtheta - \vtheta_m)^\top \mH_{\log f}(\vtheta_m) (\vtheta - \vtheta_m) + \sO((\vtheta - \vtheta_m)^3)
$$

at such a point. The quadratic form in $\vtheta$ in the approximate expresion for the log likelihood above gives 
rise to a Gaussian approximation for the likelihood $\N(\vtheta_m, \mH_{\log f}(\vtheta_m)^{-1})$.

The approximation is crude but can be quite accurate if the likelihood is symmetric and unimodal.

\subsection{Newton-Raphson}

\subsubsection{Definition}

Consider the second order Taylor expansion of $f(\vtheta)$ around $\vtheta_{n}$

$$
f(\vtheta) \approx f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta x^\top \mH_f(\vtheta_{n}) \Delta x + \sO((\Delta \vtheta_{n})^3).
$$

This expression attains its' maxima with respect to $\Delta x$ when

$$
\vzero = \mD_{\Delta \vtheta} [f(\vtheta_{n}) + \nabla f(\vtheta_{n}) \Delta \vtheta_{n} + \half \Delta \vtheta^\top \mH_f(\vtheta_{n}) \Delta \vtheta] = \nabla f(\vtheta_n) + \mH_f(\vtheta_n) \Delta \vtheta.
$$

Thus taking $\Delta \vtheta = -\nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$, we hope that choosing
$\vtheta_{n + 1} = \vtheta_{n} - \nabla f(\vtheta_n) [\mH_f(\vtheta_n)]^{-1}$ will be closer to the stationary 
point $\vtheta^*$ of $f$. This choice of $\vtheta_{n+1}$ establishes an iterative algorithm for finding the 
maxima/minima of $f$.

Convergence is achieved when the step length $\| \vtheta_{n+1} - \vtheta_{n} \| < \epsilon$, for some
suitably small choice of $\epsilon$, such as $\epsilon = 10^{-8}$.

\subsubsection{Time and memory complexity}

Execution of this algorithm requires the calculation of a vector addition, dot product and solution of a
linear system for each iteration. The vector addition and dot product can both be performed in $p$ flops,
where $p$ is the dimension of the problem. The expensive part of the computation is the solution of the linear
system $\mH_f(\vtheta_n) \Delta \vtheta$, which takes $\sO(p^3)$ flops. Even on modern computer systems, this
can quickly become computationally intractable for problems of modest size. This motivates the development
of Quasi-Newton algorithms, such as BFGS, L-BFGS and L-BFGS-B, discussed in Section \ref{sec:quasi_newton}.

\subsubsection{Radius of convergence and convergence to local solutions}

\subsection{Quasi-Newton Rapson algorithms}
\label{sec:quasi_newton}
L-BFGS-B

\subsubsection{Definition}

% Wolfe conditions
The problem of interest is to find

$$
\min_\vx f(\vx)
$$

for some smooth $f: \R^n \to \R$. Each step involves approximately solving the subproblem

$$
\min_\alpha f(\vx_k + \alpha \vp_k)
$$

where $\vx_k$ is the current best guess, $\vp_k \in \R^n$ is a search direction and $\alpha$ is the
step length.

Denote a univariate function $\phi$ restricted to the direction $\vp_k$ as
$\phi(\alpha) = f(\vx_k + \alpha \vp_k)$. A step length $\alpha_k$ is said to satisfy the \emph{Wolfe conditions}
is the following two inequalities hold:

\begin{enumerate}
\item[(i)] $f(\vx_k + \alpha_k \vp_k) \leq f(\vx_k) + c_1 \alpha_k \vp_k^\top \nabla f(\vx_k)$ \text{(Armijo rule)}
\item[(ii)] $\vp_k^\top \nabla f(\vx_k + \alpha_k \vp_k) \geq c_2 \vp_k^\top \nabla f(\vx_k)$ \text{(curvature condition)}
\end{enumerate}

The first condition ensures that the step length $\alpha_k$ decreases $f$ sufficiently, while the second condition
ensures that the slope had been sufficiently reduced.

The rationale for imposing the Wolfe conditions in an optimisation algorithm where
$\vx_{k+1} = x_k + \alpha \vp_k$ is to ensure the convergence of the gradient to zero. In particular, if the
cosine of the angle between $\vp_k$ and the gradient,

$$
\cos \theta_k = \frac{\nabla f(\vx_k)^\top \vp_k}{\|\nabla f(\vx_k)\| \|\vp_k\|}
$$

is bounded away from zero and the Wolfe conditions hold, then $\nabla f(\vx_k) \to 0$.

An additional motivation, in the case of a quasi-Newton method is that if $\vp_k = - \mB_k^{-1} \nabla f(\vx_k)$,
where the matrix $\mB_k$ is updated by the BFGS or DFP formula, then if $\mB_k$ is positive definite condition ii
implies $\mB_{k+1}$ is also positive definite.

\subsubsection{Time and memory complexity}

\section{Numerical matters}

In this section, I'd like to talk about floating point representation and its' consequences, numerical
analysis/error bounds and in particular their relevance to numerical linear algebra.

\subsection{Floating point numbers}

$$\text{sign } \text{significand} \times \text{base}^\text{exponent}$$

IEEE 754: floating point in modern computers

On modern computers, IEEE 754 is ubiquitous, and the base is $2$.

Error bounds on $+, -, *, /, \sqrt$

$$
\text{fl}(a \text{ op } b) = (1 + \epsilon) (a \text{ op } b) \text{ provided $\text{fl}(a \text{ op } b)$ neither overflows nor underflows}
$$

where $\epsilon$ is the machine epsilon, the minimum difference between $1$ and the next representable
floating point number.

Exceptional values: $\pm \infty$, $\text{NaN}$

When doing arithmetic on computers, it is important to understand the full ramifications of this.

\begin{enumerate}
\item All numerical values are in fact approximations within an interval.
\item Every arithmetic operation has an associated amount of error, and these errors accumulate as more
			arithmetic operations are performed.
\end{enumerate}

The field of numerical analysis (references Classical and Modern Numerical Analysis,
Demmel's course: http://www.cs.berkeley.edu/~demmel/cs267/lecture21/lecture21.html, Kahan) analyses numerical 
algorithms to assess their error bounds, and attempt to mitigate these problems as much as possible to allow
us to solve numerical problems on modern computers.

Truncation and round-off error
Numerical stability

\subsection{Numerical differentiation and integration}
\subsection{Numerical linear algebra}
Condition numbers
\subsection{Difficulty of inverting linear systems}

Sparsity of a matrix does not ensure sparsity of its' inverse.
Inverting a matrix is an inherently unstable operation. The number of floating point operations involved
leads to magnification of error.

\section{Positive semi-definite matrices}
A positive semidefinite matrix $\mLambda$ is a square matrix of size $n \times n$ such that for any vector
$\vx \in \R^n$, $\vx^\top \mLambda \vx \geq 0$. Equivalently, all eigenvalues of $\mLambda$ are greater than or 
equal to 0.

\section{Matrix calculus}

Matrix calculus is a very useful and compact notation for the derivatives of functions of vectors and matrices.
A useful introduction is \cite{wand02}, while a more comprehensive treatment can be found in
\cite{MagnusNeudecker99}.

\subsection{Notation}

Given a function $f:\R^{n \times n} \to \R$ of a matrix $\mLambda$, we denote the matrix derivative of $f$ with 
respect to $\mLambda$ as $\mD_\mLambda f$, and the Hessian of $f$ with respect to $\mLambda$ as
$\mH_\mLambda f$ the Hessian with respect to $\mLambda$.

The Hessian matrix is a $d \times d$ matrix whose $(i, j)$-th entry is equal to

$$
\frac{\partial^2}{\partial x_i x_j} f(\vx)
$$

and is denoted by $\mH f(\vx)$.

\section{Splines}
The most general form of the univariate regression problem is

$$
y_i = f(x_i)
$$

where $f: \R \to \R$ is unknown, and we wish to estimate it as $\hat{f}$. We treat the functional form of
$f$ as unknown, and attempt to estimate it purely from the data available to us. Fully non-parametric regression
is a difficult problem to solve, but the problem can be simplified by pre-specifying the points at which the
function may change curvature, the so-called knots.

\subsection{B-Splines}
% This is taken from the Wikipedia page on the subject
A B-Spline is a piecewise polynomial function of degree $< n$ in a variable $x$. It is defined over a
domain $t_0 \leq x \leq t_m, m=n$. The points where $x = t_j$ are known as knots or break-points. The
number of internal knots is equal to the degree of the polynomial if there are no knot multiplicities.
The knots must be in ascending order. The number of knots is the minimum for the degree of the B-spline,
which has a non-zero value in the range between the first and last knot. Each piece of the function is a
polynomial of degree $< n$ between and including adjacent knots. A B-Spline is a continuous function at the
knots. When all internal knots are distinct its derivatives are also continuous up to the derivative of degree
$n - 1$. If internal knots are coincident at a given value of x, the continuity of derivative order is reduced
by 1 for each additional knot.

For any given set of knots, the B-spline is unique, hence the name, B being short for Basis. The usefulness
of B-splines lies in the fact that any spline function of order $n$ on a given set of knots can be expressed
as a linear combination of B-spline:

% B-Splines
$$
S_{n, t}(x) = \sum_i \alpha_i B_{i, n}(x)
$$

where

\begin{align*}
B_{i, 1}(x) &:= \begin{cases}
1 & \text{if } t_i \leq x < t_{i+1} \\
0 & \text{otherwise}
\end{cases} \\
B_{i, k}(x) &:= \frac{x - t_i}{t_{i + k - 1} - t_i} B_{i, k-1} (x) + \frac{t_{i + k} - x}{t_{i + k} - t_{i + 1}} B_{i, k-1} (x)
\end{align*}

% Divided difference notation?
% Lagrange's interpolating polynomials?
% Semiparametric regression / Connection to mixed models
We follow the discussion of semiparametric regression in \cite{RuppertWandCarroll}.
Using a mixed models setup to fit spline models, protects against overfitting.
Construct an $\mZ$ matrix with the appropriate B-Spline function evaluations in each of the rows, where
each column corresponds to a knot.

\subsection{Degrees of freedom}

% From Section 2.5.2, Semiparametric regression

Consider the fixed effects model $\vy = \mX \vbeta$ where $\mX \in \R^{n \times p}$. If the hat matrix is
defined as $\mH = \mX (\mX^\top\mX)^{-1} \mX^\top$ then

\begin{align*}
\tr(\mH) &= \tr[\mX(\mX^\top\mX)^{-1}\mX^\top] \\
&= \tr[\mX^\top\mX (\mX^\top\mX)^{-1}] \\
&= \tr(\mI_p) \\
&= p
\end{align*}

which is the number of parameters in the fixed effects model. Thus we use the trace of the hat matrix as
the definition of the effective degrees of freedom of the model. This definition then naturally extends to
more general smoother matrices which can be used to fit models involving splines or other kinds of smoothing.

\section{Generalised Linear Mixed models}

\subsection{Exponential family}

An exponential family is a set of probability distributions of the form
$$
f_X(x|\vtheta) = h(x) g(\theta) \exp(\eta(\theta) \dot T(x)).
$$

Most commonly used probability distributions in statistics such as normal, exponential, gamma,
chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, Wishart and Inverse Wishart can
be represented in this form.

Canonical form

$$
\log p(\vnu; \vy) = \vy^\top \mC \vnu - b(\mC \vnu)
$$

Different choices of the function $b:\R \to \R$ lead to different generalised linear mixed models. A set
of available functions and corresponding distributions is presented in Table \ref{tab:b_functions}.

\begin{table}
\caption{Table of b functions}
\label{tab:b_functions}
\begin{tabular}{|l|lll|}
\hline
Distribution & Parameter & Natural parameter & Inverse parameter mapping \\
\hline
Bernoulli distribution & $p$ & $\log{\frac{p}{1 - p}}$ & $\frac{1}{1 + e^\eta}$ \\
Poisson distribution & $\lambda$ & \log \lambda & e^\eta \\
Normal distribution (known variance) & $\mu$ & $\frac{\mu}{\sigma}$ & $\sigma \eta$ \\
\hline
\end{tabular}
\end{table}

\subsection{Bias of MLE}

How does this relate to Bayesian estimation? Maybe it's just interesting background.
Bernstein-von Mises Theorem

\section{Model selection}

Given a full data matrix $\mX \in \R^{n \times p}$, we may wish to find the subset of variables
$\gamma \in \{ 1 \ldots p \}$ such that $\mX_\gamma \vbeta$ describe the data in $\vy$ best, in some
sense.

In a frequentist context, there are many functions which can be used to judge which model is best,
such as AIC, BIC etc. These are functions $f: \gamma \to \R^+$ which allow the models under consideration to
be ranked, and the best model chosen from those available. Thus the optimal model under such a measure
f is

$$
\gamma^* = \max_\gamma f(\gamma)
$$

These functions typically attempt to balance log likelihood against the complexity of the model, achieving
a compromise between each.

\bibliographystyle{elsarticle-harv}
\bibliography{Chapter_0_preliminary}

\end{document}
