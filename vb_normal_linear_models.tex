\section{Variational Bayes}

Let $\KL(q||p) = \int q(\vtheta) \log{\left( \frac{q(\vtheta)}{p(\vtheta|\vy)} \right)} d \vtheta$ be the
Kuhlback-Leibner divergence between $q$ and $p$, a measure of the distance between the probability
distributions $p$ and $q$.

The desired posterior distribution $p(\vtheta | \vy)$ typically requires the calculation of an analytically
intractable integral for all but the simplest models with conjugate priors. Variational Bayes approximates the
full posterior with a simplified approximating distribution $q(\vtheta)$. We relate the true and
approximating distributions as follows:

\begin{align*}
	\log p(\vy) & = \log p(\vy) \int q(\vtheta) d \vtheta = \int q(\vtheta) \log p(\vy) d \vtheta                                    \\
	            & = \int q(\vtheta) \log \left\{ \frac{p(\vy, \vtheta) / q(\vtheta)}{p(\vy|\vtheta) / q(\vtheta)} \right\} d \vtheta \\
	            & = \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\} d \vtheta +                              
	\int q(\vtheta) \log\left\{ \frac{q(\vtheta)}{p(\vtheta|\vy)} \right\} d \vtheta \\
	            & = \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\} d \vtheta +                              
	\KL(q||p) \\
	            & \geq \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\} d \vtheta.                            
\end{align*}

as $\KL(q||p) \geq 0$ for all probability densities $p$ and $q$. The last quantity is the variational lower
bound $\underline{p}(\vtheta) \equiv \int q(\vtheta) \log\left\{ \frac{p(\vy, \vtheta)}{q(\vtheta)} \right\}
d\vtheta$. By the inequality above, this is guaranteed to bound the true probability distribution from below.

The approximation is fit by iteratively maximising the variational lower bound using a sequence of mean field
updates, with each update guaranteed to increase the variational lower bound relative to the previous
iteration. This sequence of mean field updates reduces the KL divergence between the true probability
distribution $p(\vy)$ and the $q(\vtheta)$. The process converges when the variational lower bound no longer
increases. By the above derivation, this can be seen to be equivalent to the KL divergence between the posterior distribution and the approximating distribution being minimised.

A popular form of approximation is to restrict $q(\vtheta)$ to a subclass of product densities by partitioning
$\vtheta = (\vtheta_1, \vtheta_2, \ldots, \vtheta_{M-1}, \vtheta_M)$ and assuming independence between the
partitioned parameters:
\begin{equation*}
	q(\vtheta) \equiv q(\vtheta_1) q(\vtheta_2) \ldots q(\vtheta_{n-1}) q(\vtheta_n).
\end{equation*}

\noindent This allows the calculation of the optimal approximating densities $q_i^*(\vtheta_i)$ as
\begin{equation*}
	q_i^*(\vtheta_i) \propto \exp \left \{ \E_{-\vtheta_i} \log p(\vy, \vtheta) \right \}, 1 \leq i \leq M,
\end{equation*}

\noindent We choose a factored Variational Bayes approximation of the form
\begin{align*}
	q(\vtheta) = q(\vbeta) q(\sigma^2) q(g). 
\end{align*}

\noindent Then $q(\vbeta) = \N(\vmu_{q(\vbeta)}, \mSigma_{q(\vbeta)})$, $q(\sigma^2) = \IG(\alpha_{q(\vbeta)}, \beta_{q(\vbeta)})$ and $q(g) = \text{Beta Prime}(\alpha_{q(g)}, \beta_{q(g)})$.

\subsection{Naive mean field update}
\label{sec:naive_mean_field_updates}

We first present an algorithm for optimising the model fit to the data using mean field updates performed
iteratively on all variational parameters. The derivation of the naive mean field updates is presented in
Appendix \ref{sec:appendix}.

\subsubsection{Numerical integration of $\tau_g$}
\label{sec:num_int}

Define $\tau_{\sigma^2} \equiv \E_q \left[ \sigma^{-2} \right]$ and $\tau_g \equiv \E_q \left[ g^{-1}
\right]$. We can calculate $\tau_g$ numerically using the following iterative numerical scheme.

First, we choose an initial guess $\tau_g^{(1)} = \E_q [g^{-1}] = (1 - R^2) [1 + (p / 2 + a + 1)/b]$. Then
define

\begin{align*}
	\tau_g^{(i+1)} \leftarrow \int_0^\infty g^{\left(b - \frac{p}{2} - 1\right)}                                   
	(1 + g)^{- (a + b + 2)}                                                                                        
	\exp \left \{- \frac{1}{2} g^{-1}  (1 + \tau_g^{(i)})^{-1} [\tau_{\sigma^2} (1 + \tau_g^{(i)})^{-1} n R^2 + p] 
	\right \} dg                                                                                                   
\end{align*}

\noindent where $\tau_{\sigma^2} = [1 - (1 + \tau_g^{(i)})^{-1} R^2]^{-1}$. This integral can be calculated
numerically using Laplace's approximation. % TODO: You need to detail how this was done.
This process is repeated until convergence.

Let $\nu - 1 = b - \frac{p}{2}$, 
$\beta = \frac{1}{2} (1 + \tau_g)^{-1} (\tau_{\sigma^2} n R^2 + p)$, 
$\mu - 1 = (a + b + 2)$ and $\gamma = 1$. 

\begin{algorithm}
	\caption{Fit VB approximation of linear model}
	\label{alg:algorithm_one}
	\begin{algorithmic}
		\REQUIRE $\alpha_{q(\sigma^2)} \leftarrow \frac{n + p}{2}, \nu_{q(g)} - 1 \leftarrow b - \frac{p}{2}$, $\mu_{q(g)} - 1 \leftarrow (a + b + 2)$
		\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
		\STATE $\vmu_{q(\vbeta)} \leftarrow (1 + \tau_g)^{-1} \vbetahatls$
		\STATE $\mSigma_{q(\vbeta)} \leftarrow [\tau_{\sigma^2} (1 + \tau_g)]^{-1} (\mX^\top \mX)^{-1}$
		\STATE $\beta_{q(\sigma^2)} \leftarrow  \frac{1}{2} {n[1 - (1 + \tau_g)^{-1} R^2] + \tau_{\sigma^2}^{-1} p}$
		\STATE $\beta_{q(g)} \leftarrow \frac{1}{2} (1 + \tau_g)^{-1} [\tau_{\sigma^2} (1 + \tau_g)^{-1} n R^2 + p]$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\subsection{Mean field updates}
\label{sec:mean_field_updates}

A substantially simpler algorithm  which involves optimising over only one variational parameter can be
obtained from Algorithm \ref{alg:algorithm_one} by utlising the following identities.

First, we note that when assuming that $\vy^\top \vy / n = 1$,
\[\vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy =\|\vy\|^2 R^2 = n R^2.\]

\noindent Second, observe that when $\vmu = (1 + \tau_g)^{-1} \widehat{\vbeta_{LS}}$ and $\mSigma = \tau_{\sigma^2}^{-1} (1 + \tau_g)^{-1} (\mX^\top \mX)^{-1}$,
\begin{align*}
	\vmu^\top \mX^\top \mX \vmu & = (1 + \tau_g)^{-2} \widehat{\vbeta_{LS}}^\top \mX^\top \mX \widehat{\vbeta_{LS}}                                                        \\
	                            & = (1 + \tau_g)^{-2} \vy^\top \mX (\mX^\top \mX)^{-1} \cancel{\mX^\top \mX} \cancel{(\mX^\top \mX)^{-1}} \mX^\top \vy \\
	                            & = (1 + \tau_g)^{-2} n R^2.                                                                                           
\end{align*}

\noindent Third, utilising the above two identities, $s$ can be simplified to
\begin{align*}
	s & = \frac{1}{2} [\|\vy\|^2 - 2 \vmu^\top \mX^\top \vy + (1 + \tau_g) \vmu^\top \mX^\top \mX \vmu (1 + \tau_g) +  \tr (\mX^\top \mX \mSigma)]                 \\
	  & = \frac{1}{2} [n - \cancel{2} (1 + \tau_g) n R ^2 + \cancel{(1 + \tau_g) n R^2} + \tau_{\sigma^2} \tr (\cancel{\mX\top \mX} \cancel{(\mX^\top \mX)^{-1}})] \\
	  & = \frac{1}{2} \{ n[1 - (1 + \tau_g)^{-1} R^2] + p \tau_{\sigma^2}^{-1} \}.                                                                                 
\end{align*}

\noindent Fourth, recalling that $\tau_{\sigma^2} = \E_q [\sigma^{-2}] = r/s = \frac{n+p}{n[1 - (1 + \tau_g)^{-1} R^2] + p \tau_{\sigma^2}^{-1}}$, we can solve for $\tau_{\sigma^2}$ in terms of $\tau_g$ to obtain
\[
	\tau_{\sigma^2} = [1 + (1 + \tau_g)^{-1}]^{-1}.
\]

\noindent Hence
\begin{align*}
	s & = \frac{r}{\tau_{\sigma^2}} = \frac{1}{2} (n + p) [1 - (1 + \tau_g)^{-1} R^2], \text{ and }                \\
	c & = \frac{\tau_{\sigma^2}}{2}[\vmu^\top \mX^\top \mX \vmu + \tr (\mX^\top \mX \mSigma)]                      \\
	  & = \frac{\tau_{\sigma^2}}{2}\left[(1 + \tau_g)^{-2} n R^2 + \tau_{\sigma^2}^{-1} (1 + \tau_g)^{-1} p\right] \\
	  & = \frac{1}{2} [ \tau_{\sigma^2} (1 + \tau_g)^{-2} n R^2 + (1 + \tau_g)^{-1} p]                             \\
	  & = \frac{1}{2} \{ [1 + (1 + \tau_g)^{-1}]^{-1} (1 + \tau_g)^{-2} n R^2 + (1 + \tau_g)^{-1} p\}.             
\end{align*}

\noindent It follows that all of the variational parameter updates in Algorithm \ref{alg:algorithm_one} can be expressed as functions
of $n$, $p$, $R^2$ and $\tau_g$. Thus optimisation can be performed on $\tau_g$ alone by repeatedly using the
scheme presented in \ref{sec:num_int}. Once $\tau_g$ is fully optimised, the other variational parameters can
be calculated from it as shown in Algorithm \ref{alg:algorithm_two}.

\begin{algorithm}
	\caption{Fit VB approximation of linear model}
	\label{alg:algorithm_two}
	\begin{algorithmic}
		\REQUIRE $\nu_{q(g)} - 1 \leftarrow b - \frac{p}{2}$, $\mu_{q(g)} - 1 \leftarrow (a + b + 2)$ \\
		\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
		\STATE Calculate $\tau_{g}$ using numerical integration in Section \ref{sec:num_int}
		\ENDWHILE
		\STATE $\tau_{\sigma^2} \leftarrow \{[1 - (1 + \tau_g)^{-1}] R^2\}^{-1}$
		\STATE $\beta_{q(g)} \leftarrow \left(\frac{n (1 + \tau_g)^{-1}}{[1 - (1 + \tau_g)^{-1}]} + p \right)$
		\STATE $\vmu_{q(\vbeta)} \leftarrow (1 + \tau_g)^{-1} (\mX^\top \mX)^{-1} \mX^\top \vy$
		\STATE $\mSigma_{q(\vbeta)} \leftarrow \tau_{\sigma^2}^{-1} (1 + \tau_{g})^{-1}(\mX^\top \mX)^{-1}$
	\end{algorithmic}
\end{algorithm}

\subsection{Model Selection in Variational Bayes}

We perform model selection by using the above Variational Bayes approximation to each
candidate linear model and averaging over the space of available models.
\begin{align*}
	\underline{p}(\vy) & = \sum_\vgamma \underline{p}(\vy|\vgamma) p(\vgamma),                                  \\
	p(\vgamma|\vy)     & \approx \frac{\underline{p}(\vy|\vgamma) p(\vgamma)}{\underline{p}(\vy)} \text { and } \\
	p(\vbeta|\vy)      & \approx \sum_\vgamma q(\vbeta|\vgamma) p(\vgamma|\vy).                                 \\
\end{align*}
