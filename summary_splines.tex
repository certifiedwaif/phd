\documentclass{amsart}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}
\section{Splines summary}
From Elements of Statistical Learning

Assume we have bivariate data X and Y. Consider two random variables X and Y.
Suppose that we wish to construct a statisical model for the joint distribution 
P(X, Y), in order to study the relationships between X and Y in our data. We
could construct a statistical model
\begin{align*}
	Y = f(X) + \epsilon	
\end{align*}
where the random error has $E(\epsilon) = 0$ and is independent of X. What form
should the function f take? General functions f can be approximated by linear
combinations of basis functions of the form
\begin{align*}
	f_\theta(x) = \sum_{k=1}^K h_k(x) \theta_k
\end{align*}

Then we can fit such an $f_\theta$ to our data by minimising the residual
sum of squares
\begin{align*}
	RSS(\theta) = \sum_{i=1}^N (y_i - f_\theta(x_i))^2
\end{align*}

as a function of $\theta$. We can use least squares % you have this at home
to fit $\hat{f_{\theta}}$ to our function f in closed form.

\begin{align*}
	\hat{\theta} = (B^TB + \lambda \Omega)^{-1}B^T y
\end{align*}

Ormerod and Wand did this using O'Sullivan splines in their 2008 paper. This
form of spline apparently has numerical advantages. The co-efficient matrix
$B$ where $\{B\}_{ik} = B_{k}(x_i)$ is
almost banded, as splines have highly localised support.
$\Omega = \int_{-\infty}^{\infty} B''_{k}(x) B''_{k'}(x) dx$

\section{Things I don't understand}
\begin{enumerate}
	\item How did Wand and Ormerod change the basis from B Splines to O'Sullivan
	splines?
	\item Why was using Simpson's rule to exactly approximate the penalty matrix
	such a huge win versus the differencing matrix?
	\item Why do we fit splines as variance components?
	\item Numerics, OMG? Why does the Cholesky decomposition retain sparsity,
	LU decomposition, banded matrices
\end{enumerate}

Penalised splines
Fit using Least squares
Numerical advantages to O'Sullivan splines, using in Wand/Ormerod 2008.

From Machine Learning: A Probabilistic Perspective
% TODO: Move this to the splines.tex summary file
Splines are in \S 15.4.6.2, which is a chapter on Gaussian processes.

All of \S 15.4.6 is a gold mine.

\section{\S 15.4.6 Smoothing splines compared to GPs}
\emph{Smoothing splines} are a widely used non-parametric method for smoothly interpolating
data (see Green and Silverman 1994). They are a special case of Gaussian processes, as we
will see. They are used when the input is 1 or 2 dimensional.

\subsection{\S 15.4.6.1 Univariate splines}
The basic idea is to fit a function f by minimising the discrepancy to the data plus a
smoothing term that penalizes functions that are ``too wiggly''. If we penalise the
m'th derivative of the function, the objective function becomes
\[
J(f) = \sum_{i=1}^N (f(x_i) - y_i)^2 + \lambda \int \left(\frac{d^m}{dx^m} f(x)\right)^2 dx
\]

One can show (Green and Silverman 1994) that the solution is a \emph{piecewise polynomial}
where the polynomials have order $2m - 1$ in the interior lines $[x_{i-1}, x_i]$ (denoted I),
and order $m - 1$ in the two outermost intervals $(-\infty, x_1]$ and $[x_N, \infty)$:
\[
f(x) = \sum_{j=0}^{m-1} \beta_j x^j + I(x \in I)\left( \sum_{i=1}^N \alpha_i (x - x_i)^{2m - 1}_{+}\right) + I(x \notin I)\left(\sum_{i=1}^N \alpha_i (x - x_i)^{m-1}_{+}\right)
\]

For example, if $m=2$, we get the (natural) \emph{cubic spline}
\[
f(x) = \beta_0 + \beta_1 x + \mathcal{I}(x \in I)\left(\sum_{i=1}^N \alpha_i (x - x_i)^3_+ \right) +
\mathcal{I}(x \notin I) \left( \sum_{i=1}^N \alpha_i (x - x_i)_+ \right)
\]

which is a series of truncated cubic polynomials, where left hand sides are located at
each of the N training points. (The fact that the model is linear on the edges prevents it
from extrapolating too wildly beyond the range of the data, if we drop the requirement,
we get an ``unrestricted'' spline.)

We can clearly fit this model using the ridge regression:
$\hat{w} = (\Phi^T \Phi + \lambda I_N)^{-1} \Phi^T \vec{y}$, where the columns of $\Phi$ are
1, $x_i$ and $(x - x_i)^3_+$ for $i=2:N-1$ and $(x - x_i)_+$ for $i=1$ or $i=N$. However, we
can also derive an O(N) time method (Green and Silverman 1994, \S 2.3.3).

\subsection{\S 15.4.6.2 Regression splines}

In general, we can place the polynomials at a fixed set of K locations known as \emph{knots},
denoted $\xi_k$. The result is called a \emph{regression spline}. This is a parametric model,
which uses basis function expansion of the following form (where we drop the interior/
exterior distinction for simplicity):
\[
f(x) = \beta_0 + \beta_1 x + \sum_{i=1}^K \alpha_j (x - \xi_k)^3_+
\]

Choosing the number and locations of the knots is just like choosing the number and values 
of the support vectors in \S 14.3.2. If we imposse an $l_2$ regulariser on the regression
co-efficients $\alpha_j$, the method is known as \emph{penalized splines}. See
\S 9.6.1 for a practical example of penalised splines.

\end{document}
