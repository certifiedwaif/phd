@misc{StanDevelopmentTeam2016,
annote = {R package version 2.14.1},
author = {{Stan Development Team}},
title = {{{\{}RStan{\}}: the {\{}R{\}} interface to {\{}Stan{\}}}},
year = {2016}
}
@article{Carpenter2016,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matt and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
file = {:home/markg/Downloads/stan-paper-revision-feb2015.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {probabilistic programming, Bayesian inference, alg},
number = {Ii},
title = {{Journal of Statistical Software Stan : A Probabilistic Programming Language}},
volume = {VV},
year = {2016}
}
@article{KimWand2017,
author = {Kim, Andy S. I. Kim and Wand, M. P.},
file = {:home/markg/Downloads/KimWand.pdf:pdf},
keywords = {bayesian computing,factor graphs,infer,mean field variational bayes,net},
pages = {1--27},
title = {{On Expectation Propagation for Generalized , Linear and Mixed Models}},
year = {2017}
}
@misc{Tierney1986,
abstract = {This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.},
author = {Tierney, Luke and Kadane, Joseph B.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1986.10478240},
file = {:home/markg/Downloads/laplace.pdf:pdf},
isbn = {0162-1459},
issn = {1537274X},
keywords = {Asymptotic expansions,Bayesian inference,Computation of integrals,Laplace method},
number = {393},
pages = {82--86},
title = {{Accurate approximations for posterior moments and marginal densities}},
volume = {81},
year = {1986}
}
@article{Rue2009,
abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (gener- alized) additive models, smoothing spline models, state space models, semiparametric regres- sion, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models.We consider approximate Bayesian inference in a popular subset of struc- tured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables.The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis.We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its gen- erality, which makes it possible to performBayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0913v5},
author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
doi = {10.1111/j.1467-9868.2008.00700.x},
eprint = {arXiv:1405.0913v5},
file = {:home/markg/Downloads/inla-rss.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
number = {2},
pages = {319--392},
title = {{Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations}},
volume = {71},
year = {2009}
}
@article{Barber2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08337v1},
author = {Barber, Rina Foygel and Drton, Mathias and Tan, Kean Ming},
doi = {10.1007/978-3-319-27099-9_2},
eprint = {arXiv:1503.08337v1},
file = {:home/markg/Downloads/1503.08337.pdf:pdf},
isbn = {9783319270975},
issn = {21978549},
journal = {Abel Symposia},
keywords = {and phrases,bayesian inference,generalized linear models,laplace ap-,logistic regression,model selection,proximation,variable selection},
number = {2012},
pages = {15--36},
title = {{Laplace approximation in high-dimensional Bayesian regression}},
volume = {11},
year = {2016}
}
@article{Sinica2017,
author = {Hall, P and Ormerod, Jt and Wand, Mp},
file = {:home/markg/Downloads/24309276.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,asymptotic theory,generalized linear mixed models,kullback-,liebler divergence,longitudinal data analysis,maximum likelihood estimation},
number = {1},
pages = {369--389},
title = {{Theory of Gaussian variational approximation for a Poisson mixed model}},
volume = {21},
year = {2011}
}
@article{Luts2015,
abstract = {Fast variational approximate algorithms are developed for Bayesian semiparametric regression when the response variable is a count, i.e. a non-negative integer. We treat both the Poisson and Negative Binomial families as models for the response variable. Our approach utilizes recently developed methodology known as non-conjugate variational message passing. For concreteness, we focus on generalized additive mixed models, although our variational approximation approach extends to a wide class of semiparametric regression models such as those containing interactions and elaborate random effect structure.},
archivePrefix = {arXiv},
arxivId = {1309.4199},
author = {Luts, J. and Wand, M. P.},
doi = {10.1214/14-BA932},
eprint = {1309.4199},
file = {:home/markg/Downloads/LutsWand15.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Approximate bayesian inference,Generalized additive mixed models,Mean field variational bayes,Penalized splines,Real-time semiparametric regression},
number = {4},
pages = {991--1023},
title = {{Variational inference for count response semiparametric regression}},
volume = {10},
year = {2015}
}
@article{Carbonetto2012,
abstract = {The Bayesian approach to variable selection in regression is a powerful tool for tackling many scientific problems. Inference for variable selection models is usually implemented using Markov chain Monte Carlo (MCMC). Because MCMC can impose a high computational cost in studies with a large number of variables, we assess an alternative to MCMC based on a simple variational approximation. Our aim is to retain useful features of Bayesian variable selection at a reduced cost. Using simulations designed to mimic genetic association studies, we show that this simple variational approximation yields posterior inferences in some settings that closely match exact values. In less restrictive (and more realistic) conditions, we show that posterior probabilities of inclusion for individual variables are often incorrect, but variational estimates of other useful quantities—including posterior distributions of the hyperparameters—are remarkably accurate. We illustrate how these results guide the use of variational inference for a genome-wide association study with thousands of samples and hundreds of thousands of variables.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.4400v1},
author = {Carbonetto, Peter and Stephens, Matthew},
doi = {10.1214/12-BA703},
eprint = {arXiv:1208.4400v1},
file = {:home/markg/Downloads/euclid.ba.1339616726.pdf:pdf},
isbn = {1931-6690},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Genetic association studies,Monte carlo,Variable selection,Variational inference},
number = {1},
pages = {73--108},
title = {{Scalable variational inference for bayesian variable selection in regression, and its accuracy in genetic association studies}},
volume = {7},
year = {2012}
}
@article{Wang2017,
author = {Wang, Min and Maruyama, Yuzo},
doi = {10.1016/j.jspi.2017.10.007},
file = {:home/markg/Downloads/1-s2.0-S037837581730191X-main.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
number = {xxxx},
pages = {1--11},
publisher = {Elsevier B.V.},
title = {{Posterior consistency of g-prior for variable selection with a growing number of parameters}},
volume = {11},
year = {2017}
}
@article{Castillo2015,
abstract = {We study full Bayesian procedures for high-dimensional linear regression under sparsity constraints. The prior is a mixture of point masses at zero and continuous distributions. Under compatibility conditions on the design matrix, the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector, and to give optimal prediction of the response vector. It is also shown to select the correct sparse model, or at least the coefficients that are significantly different from zero. The asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification.$\backslash$n$\backslash$nPublished in: Annals of Statistics 2015, Vol. 43, No. 5, 1986-2018},
archivePrefix = {arXiv},
arxivId = {1403.0735},
author = {Castillo, Ismael and Schmidt-Hieber, Johannes and {Van Der Vaart}, Aad},
doi = {10.1214/15-AOS1334},
eprint = {1403.0735},
file = {:home/markg/Downloads/euclid.aos.1438606851.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian inference,Sparsity},
number = {5},
pages = {1986--2018},
title = {{Bayesian linear regression with sparse priors}},
volume = {43},
year = {2015}
}
@article{Fan2001,
abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coef? cients simultaneously. Hence they enable us to construct con? dence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on 401ˆ5, and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
file = {:home/markg/Downloads/penlike.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hard thresholding,lasso,nonnegative garrote,oracle estimator,penalized likelihood,scad,soft thresholding},
number = {456},
pages = {1348--1360},
pmid = {21796725},
title = {{Variable Selection via Nonconcave Penalized}},
volume = {96},
year = {2001}
}
@book{Zhang2010,
abstract = {We propose MC+, a fast, continuous, nearly unbiased and accurate method of penalized variable selection in high-dimensional linear regression. The LASSO is fast and continuous, but biased. The bias of the LASSO may prevent consistent variable selection. Subset selection is unbiased but computationally costly. The MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear unbiased selection (PLUS) algorithm. The MCP provides the convexity of the penalized loss in sparse regions to the greatest extent given certain thresholds for variable selection and unbiasedness. The PLUS computes multiple exact local minimizers of a possibly nonconvex penalized loss function in a certain main branch of the graph of critical points of the penalized loss. Its output is a continuous piecewise linear path encompassing from the origin for infinite penalty to a least squares solution for zero penalty. We prove that at a universal penalty level, the MC+ has high probability of matching the signs of the unknowns, and thus correct selection, without assuming the strong irrepresentable condition required by the LASSO. This selection consistency applies to the case of {\$}p\backslashgg n{\$}, and is proved to hold for exactly the MC+ solution among possibly many local minimizers. We prove that the MC+ attains certain minimax convergence rates in probability for the estimation of regression coefficients in {\$}\backslashell{\_}r{\$} balls. We use the SURE method to derive degrees of freedom and {\$}C{\_}p{\$}-type risk estimates for general penalized LSE, including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the estimated degrees of freedom, we propose an estimator of the noise level for proper choice of the penalty level.},
archivePrefix = {arXiv},
arxivId = {1002.4734},
author = {Zhang, Cun Hui},
booktitle = {Annals of Statistics},
doi = {10.1214/09-AOS729},
eprint = {1002.4734},
file = {:home/markg/Downloads/euclid.aos.1266586618.pdf:pdf},
isbn = {9040210063},
issn = {00905364},
keywords = {Correct selection,Degrees of freedom,Least squares,Mean squared error,Minimax,Model selection,Nonconvex minimization,Penalized estimation,Risk estimation,Selection consistency,Sign consistency,Unbiasedness,Variable selection},
number = {2},
pages = {894--942},
title = {{Nearly unbiased variable selection under minimax concave penalty}},
volume = {38},
year = {2010}
}
@manual{Croissant2016,
annote = {R package version 0.3-1},
author = {Croissant, Yves},
title = {{Ecdat: Data Sets for Econometrics}},
year = {2016}
}
@article{Zeugner2015,
author = {Zeugner, Stefan and Feldkircher, Martin},
doi = {10.18637/jss.v068.i04},
journal = {Journal of Statistical Software},
number = {4},
pages = {1--37},
title = {{Bayesian Model Averaging Employing Fixed and Flexible Priors: The {\{}BMS{\}} Package for {\{}R{\}}}},
volume = {68},
year = {2015}
}
@manual{Clyde2017,
annote = {R package version 1.4.4},
author = {Clyde, Merlise},
title = {{BAS: Bayesian Adaptive Sampling for Bayesian Model Averaging}},
year = {2017}
}
@article{Young2010,
author = {Young, Derek S},
journal = {Journal of Statistical Software},
number = {5},
pages = {1--39},
title = {{tolerance: An R Package for Estimating Tolerance Intervals}},
volume = {36},
year = {2010}
}
@manual{Bove2013,
annote = {R package version 0.0-4},
author = {Bov{\'{e}}, D S and Colavecchia, F D and Forrey, R C and Gasaneo, G and Michel, N L J and Shampine, L F and Stoitsov, M V and Watts, H A},
title = {{appell: Compute Appell's F1 hypergeometric function}},
year = {2013}
}
@article{Hankin2006,
author = {Hankin, Robin K S},
journal = {R News},
month = {oct},
number = {4},
title = {{Special functions in R: introducing the gsl package}},
volume = {6},
year = {2006}
}
@book{Johnson1995,
author = {Johnson, N L and Kotz, S and Balakrishnan, N},
publisher = {Wiley},
title = {{Continuous Univariate Distributions, Volume 2 (2nd Edition)}},
year = {1995}
}
@article{Lindley1957,
author = {Lindley, D V},
journal = {Biometrika},
pages = {187--192},
title = {{A statistical paradox}},
volume = {44},
year = {1957}
}
@manual{Weisstein2009,
author = {Weisstein, E W},
title = {{Appell Hypergeometric Function}},
year = {2009}
}
@unpublished{OrmerodEtal2017,
author = {Ormerod, J T and Stewart, M and Yu, W and Romanes, S E},
title = {{Bayesian hypothesis tests with diffuse priors: Can we have our cake and eat it too?}},
year = {2017}
}
@article{Bayarri2007,
author = {Bayarri, M J and Garc{\'{i}}a-Donato, Gonzalo},
journal = {Biometrika},
number = {1},
pages = {135--152},
publisher = {[Oxford University Press, Biometrika Trust]},
title = {{Extending Conventional Priors for Testing General Hypotheses in Linear Models}},
volume = {94},
year = {2007}
}
@book{Gradshteyn2007,
author = {Gradshteyn, I S and Ryzhik, I M},
publisher = {Academic Press},
title = {{Tables of Integrals, Series, and Products}},
year = {2007}
}
@book{PrudnikovEtal1986,
author = {Prudnikov, A P and Brychkov, Y A and Marichev, O I},
publisher = {Gordon and Breach},
title = {{Integrals and Series (Vols. 1–3)}},
year = {1986}
}
@article{Pearson2017,
author = {Pearson, John W and Olver, Sheehan and Porter, Mason A},
journal = {Numerical Algorithms},
number = {3},
pages = {821--866},
title = {{Numerical methods for the computation of the confluent and Gauss hypergeometric functions}},
volume = {74},
year = {2017}
}
@book{Abramowitz1972,
author = {Abramowitz, M and Stegun, I A},
publisher = {New York: Dover Publications},
title = {{Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables}},
year = {1972}
}
@article{Berger1998,
author = {Berger, J O and Pericchi, L R and Varshavsky, J A},
journal = {Sankhy$\backslash$={\{}a{\}}: The Indian Journal of Statistics, Series A},
number = {3},
pages = {307--321},
publisher = {Springer},
title = {{Bayes factors and marginal distributions in invariant situations}},
volume = {60},
year = {1998}
}
@article{Bartlett1957,
author = {Bartlett, M S},
journal = {Biometrika},
number = {3},
pages = {533--534},
title = {{A comment on D.V. Lindley's statistical paradox}},
volume = {44},
year = {1957}
}
@article{Kass1995b,
author = {Kass, Robert E and Wasserman, Larry},
journal = {Journal of the American Statistical Association},
number = {431},
pages = {928--934},
title = {{A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion}},
volume = {90},
year = {1995}
}
@article{Zellner1980b,
author = {Zellner, A and Siow, A},
journal = {Trabajos de Estadistica Y de Investigacion Operativa},
number = {1},
pages = {585--603},
title = {{Posterior odds ratios for selected regression hypotheses}},
volume = {31},
year = {1980}
}
@article{DeBoor1972,
abstract = {In computational dealings with splines, the question of representation is of primary importance. For splines of fixed order on a fixed partition, this is a question of choice of basis, since such splines form a linear space. Only three kinds of bases for spline spaces have actually been given serious attention; those consisting of truncated power functions, of cardinal splines, and of B-splines. Truncated power bases are known to be open to severe ill- conditioning, while cardinal splines are difficult to calculate. By contrast, bases consisting of B-splines are well-conditioned, at least for orders {\textless} 20. Such bases are also local in the sense that at every point only a fixed number (equal to the order) of B-splines is nonzero. B-splines are also evaluated quite easily, using their definition as a divided difference of the truncated power function. Unfortunately, such calculations are ill-conditioned, particularly for partitions of widely varying interval lengths, as is indicated by the fact that special measures have to be taken in case of coincident knots. In this note, a different way of evaluating B-splines is discussed which is very well conditioned yet efficient, and which needs no special adjustments in case of coincident knots. It is also shown that the condition of the B-spline basis increases exponentially with the order.},
author = {de Boor, Carl},
doi = {10.1016/0021-9045(72)90080-9},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Boor - 1972 - On calculating with B-splines.pdf:pdf},
issn = {10960430},
journal = {Journal of Approximation Theory},
number = {1},
pages = {50--62},
title = {{On calculating with B-splines}},
volume = {6},
year = {1972}
}
@article{Andrieu2003,
abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
archivePrefix = {arXiv},
arxivId = {1109.4435v1},
author = {Andrieu, Christophe and {De Freitas}, Nando and Doucet, Arnaud and Jordan, Michael I.},
doi = {10.1023/A:1020281327116},
eprint = {1109.4435v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrieu et al. - 2003 - An introduction to MCMC for machine learning.pdf:pdf},
isbn = {08856125 (ISSN)},
issn = {08856125},
journal = {Machine Learning},
keywords = {MCMC,Markov chain Monte Carlo,Sampling,Stochastic algorithms},
number = {1-2},
pages = {5--43},
pmid = {178037200001},
title = {{An introduction to MCMC for machine learning}},
volume = {50},
year = {2003}
}
@article{Raymond2014,
abstract = {Variational inference is a powerful concept that underlies many iterative approximation algorithms; expectation propagation, mean-field methods and belief propagations were all central themes at the school that can be perceived from this unifying framework. The lectures of Manfred Opper introduce the archetypal example of Expectation Propagation, before establishing the connection with the other approximation methods. Corrections by expansion about the expectation propagation are then explained. Finally some advanced inference topics and applications are explored in the final sections.},
archivePrefix = {arXiv},
arxivId = {1409.6179},
author = {Raymond, Jack and Manoel, Andre and Opper, Manfred},
doi = {10.1162/NECO_a_00104},
eprint = {1409.6179},
file = {:home/markg/Downloads/tm.pdf:pdf},
isbn = {978-0-387-30768-8, 978-0-387-30164-8},
issn = {1530-888X},
pmid = {21222527},
title = {{Expectation Propagation}},
url = {http://arxiv.org/abs/1409.6179},
year = {2014}
}
@article{Blei2005,
abstract = {This note describes Minka's expectation propagation algorithm. It draws from his thesis and both UAI papers},
author = {Blei, David},
file = {:home/markg/Downloads/Blei2003.pdf:pdf},
number = {0},
pages = {1--4},
title = {{Expectation Propagation Explanation}},
year = {2005}
}
@article{Minka2001,
abstract = {One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, ``Expectation Propagation,'' unifies and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction---propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Minka, Thomas P},
doi = {10.1016/j.conb.2011.12.004},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/Downloads/minka-thesis.pdf:pdf},
isbn = {9788578110796},
issn = {09594388},
journal = {Ph.D. Thesis},
keywords = {Belief Propagation,MRF Inference,Markov Random Fields},
pages = {1--482},
pmid = {2688543},
title = {{A family of algorithms for approximate Bayesian inference}},
year = {2001}
}
@article{Minka2013,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P.},
eprint = {1301.2294},
file = {:home/markg/Downloads/minka-ep-uai.pdf:pdf},
isbn = {1-55860-800-1},
title = {{Expectation Propagation for approximate Bayesian inference}},
url = {http://arxiv.org/abs/1301.2294},
year = {2013}
}
@article{Gelman2014,
abstract = {A common approach for Bayesian computation with big data is to partition the data into smaller pieces, perform local inference for each piece separately, and finally combine the results to obtain an approximation to the global posterior. Looking at this from the bottom up, one can perform separate analyses on individual sources of data and then want to combine these in a larger Bayesian model. In either case, the idea of distributed modeling and inference has both conceptual and computational appeal, but from the Bayesian perspective there is no general way of handling the prior distribution: if the prior is included in each separate inference, it will be multiply-counted when the inferences are combined; but if the prior is itself divided into pieces, it may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma, expectation propagation (EP) has been proposed as a prototype for distributed Bayesian inference. The central idea is to factor the likelihood according to the data partitions, and to iteratively combine each factor with an approximate model of the prior and all other parts of the data, thus producing an overall approximation to the global posterior at convergence. In this paper, we give an introduction to EP and an overview of some recent developments of the method, with particular emphasis on its use in combining inferences from partitioned data. In addition to distributed modeling of large datasets, our unified treatment also includes hierarchical modeling of data with a naturally partitioned structure.},
archivePrefix = {arXiv},
arxivId = {1412.4869},
author = {Gelman, Andrew and Vehtari, Aki and Jyl{\"{a}}nki, Pasi and Sivula, Tuomas and Tran, Dustin and Sahai, Swupnil and Blomstedt, Paul and Cunningham, John P. and Schiminovich, David and Robert, Christian},
eprint = {1412.4869},
file = {:home/markg/Downloads/1412.4869.pdf:pdf},
title = {{Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data}},
url = {http://arxiv.org/abs/1412.4869},
year = {2014}
}
@article{Hoerl2017,
author = {Hoerl, Arthur E and Kennard, Robert W},
file = {:home/markg/Downloads/1267351.pdf:pdf},
number = {1},
pages = {55--67},
title = {{Biased Estimation for Nonorthogonal Problems}},
volume = {12},
year = {2017}
}
@book{Murphy:2012:MLP:2380985,
author = {Murphy, Kevin P},
isbn = {0262018020, 9780262018029},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012}
}
@book{hastie01statisticallearning,
address = {New York, NY, USA},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
keywords = {ml statistics},
publisher = {Springer New York Inc.},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@book{Janes2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Janes, E.T.},
booktitle = {Igarss 2014},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/Downloads/JaynesProbabilityTheory.pdf:pdf},
isbn = {9780874216561},
issn = {0343-6993},
keywords = {high resolution images,research,risks management,sustainable reconstruction},
number = {1},
pages = {1--5},
pmid = {4362089},
title = {{Probability Theory: The Logic of Science}},
year = {2014}
}
@article{Society2016a,
author = {Society, The Econometric},
file = {:home/markg/Downloads/1913469.pdf:pdf},
number = {3},
title = {{Some Generalized Functions for the Size Distribution of Income Author ( s ): James B . McDonald Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/1913469 Accessed : 27-05-2016 09 : 21 UTC Your use of the JSTOR archive indicat}},
volume = {52},
year = {2016}
}
@article{Hastie2015,
abstract = {Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of l1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso. In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling.},
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
doi = {10.1201/b18401-1},
file = {:home/markg/Downloads/SLS.pdf:pdf},
isbn = {978-1-4987-1217-0},
issn = {0306-7734},
journal = {Crc},
pages = {362},
title = {{Statistical Learning with Sparsity: The Lasso and Generalizations}},
year = {2015}
}
@article{Dubey1970,
abstract = {In this paper a compound gamma distribution has been derived by compounding a gamma distribution with another gamma distribution. The resulting compound gamma distribution has been reduced to the Beta distributions of the first kind and the second kind and to theF distribution by suitable transformations. This includes theLomax distribution as a special case which enjoys a useful property. Moment estimators for two of its parameters are explicitly obtained, which tend to a bivariate normal distribution. The paper contains expressions for a bivariate probability density function, its conditional expectation, conditional variance and the product moment correlation coefficient. Finally, all the parameters of the compound gamma distribution are explicitly expressed in terms of the functions of the moments of the functions of random variables in two different ways.},
author = {Dubey, Satya D.},
doi = {10.1007/BF02613934},
file = {:home/markg/Downloads/10.1007{\%}2FBF02613934.pdf:pdf},
issn = {1435926X},
journal = {Metrika: International Journal for Theoretical and Applied Statistics},
number = {1},
pages = {27--31},
title = {{Compound gamma, beta and F distributions}},
volume = {16},
year = {1970}
}
@article{Som2016,
author = {Som, Agniva and Hans, Christopher M. and MacEachern, Steven N.},
doi = {10.1093/biomet/asw037},
file = {:home/markg/Downloads/asw037.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {Bayesian linear model,Information paradox,Lindley's paradox,Mixture of normal distributions,Model selection,Shrinkage estimator},
number = {4},
pages = {993--999},
title = {{A conditional Lindley paradox in Bayesian linear models}},
volume = {103},
year = {2016}
}
@article{Garcia-Donato2016,
archivePrefix = {arXiv},
arxivId = {1607.04543},
author = {Garcia-Donato, Gonzalo and Forte, Anabel},
doi = {10.18637/jss.v000.i00},
eprint = {1607.04543},
file = {:home/markg/Downloads/1611.08118.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {Computational efficiency,Large data,Latent models,Structural models,Wavelet variance},
number = {2012},
pages = {submitted},
title = {{BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R}},
year = {2016}
}
@article{Xu2007,
abstract = {The genetic variance of a quantitative trait is often controlled by the segregation of multiple interacting loci. Linear model regression analysis is usually applied to estimating and testing effects of these quantitative trait loci (QTL). Including all the main effects and the effects of interaction (epistatic effects), the dimension of the linear model can be extremely high. Variable selection via stepwise regression or stochastic search variable selection (SSVS) is the common procedure for epistatic effect QTL analysis. These methods are computationally intensive, yet they may not be optimal. The LASSO (least absolute shrinkage and selection operator) method is computationally more efficient than the above methods. As a result, it has been widely used in regression analysis for large models. However, LASSO has never been applied to genetic mapping for epistatic QTL, where the number of model effects is typically many times larger than the sample size. In this study, we developed an empirical Bayes method (E-BAYES) to map epistatic QTL under the mixed model framework. We also tested the feasibility of using LASSO to estimate epistatic effects, examined the fully Bayesian SSVS, and reevaluated the penalized likelihood (PENAL) methods in mapping epistatic QTL. Simulation studies showed that all the above methods performed satisfactorily well. However, E-BAYES appears to outperform all other methods in terms of minimizing the mean-squared error (MSE) with relatively short computing time. Application of the new method to real data was demonstrated using a barley dataset.},
author = {Xu, Shizhong},
doi = {10.1111/j.1541-0420.2006.00711.x},
file = {:home/markg/Downloads/Xu-2007-Biometrics.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
keywords = {LASSO,Maximum likelihood,Mixed model,QTL mapping,Shrinkage},
number = {2},
pages = {513--521},
pmid = {17688503},
title = {{An empirical Bayes method for estimating epistatic effects of quantitative trait loci}},
volume = {63},
year = {2007}
}
@article{Huang2016,
abstract = {There has been an intense development on the estimation of a sparse regression coefficient vector in statistics, machine learning and related fields. In this paper, we focus on the Bayesian approach to this problem, where sparsity is incorporated by the so-called spike-and-slab prior on the coefficients. Instead of replying on MCMC for posterior inference, we propose a fast and scalable algorithm based on variational approximation to the posterior distribution. The updating scheme employed by our algorithm is different from the one proposed by Carbonetto and Stephens (2012). Those changes seem crucial for us to show that our algorithm can achieve asymptotic consistency even when the feature dimension diverges exponentially fast with the sample size. Empirical results have demonstrated the effectiveness and efficiency of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.07640v1},
author = {Huang, Xichen and Wang, Jin and Liang, Feng},
eprint = {arXiv:1602.07640v1},
file = {:home/markg/Downloads/1602.07640.pdf:pdf},
journal = {ArXiv e-prints},
keywords = {Statistics - Computation,Statistics - Methodology},
pages = {arXiv:1602.07640},
title = {{A Variational Algorithm for Bayesian Variable Selection}},
url = {http://adsabs.harvard.edu/abs/2016arXiv160207640H},
volume = {1602},
year = {2016}
}
@article{Teh2006,
abstract = {Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational Bayesian inference for LDA.},
author = {Teh, Yee Whye and Newman, David and Welling, Max and Neaman, D},
file = {:home/markg/Downloads/4535.pdf:pdf;:home/markg/Downloads/3113-a-collapsed-variational-bayesian-inference-algorithm-for-latent-dirichlet-allocation.pdf:pdf},
isbn = {0-262-19568-2},
issn = {1049-5258},
journal = {Neural Information Processing Systems},
keywords = {LDA: collapsed variational Bayes,Taylor expansion,approximation},
pages = {1353----1360},
title = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
url = {http://papers.nips.cc/paper/3113-a-collapsed-variational-bayesian-inference-algorithm-for-latent-dirichlet-allocation},
year = {2006}
}
@book{zuur_mixed_2009,
address = {New York, NY},
author = {Zuur, Alain F and Ieno, Elena N and Walker, Neil and Saveliev, Anatoly A and Smith, Graham M},
isbn = {978-0-387-87457-9},
keywords = {imported},
publisher = {Springer New York},
series = {Statistics for {\{}Biology{\}} and {\{}Health{\}}},
title = {{Mixed effects models and extensions in ecology with {\{}R{\}}}},
year = {2009}
}
@article{10.2307/2579146,
abstract = {The sociology of science has clearly established the presence of sex differences in scientific productivity and position. This article examines the processes leading to the lower productivity of female scientists at the completion of their doctoral training. Collaboration with the mentor is found to be the most important factor affecting productivity. For females, opportunities for collaboration are significantly decreased by having young children. As a consequence, the presence of young children has an adverse, indirect effect on the productivity of female scientists during graduate study. This effect does not exist for males. In addition to differences in the process of collaboration, many small differences that disadvantage women and advantage men are found in the levels of resources affecting productivity and in the mechanisms by which resources are translated into productivity. The concentration of small disadvantages provides a further explanation of sex differences in productivity at the start of the career. Since early advantages and disadvantages have been found to accumulate, this article provides an essential first step in understanding sex differences in scientific productivity and position that emerge during the career.},
author = {Long, J Scott},
issn = {00377732, 15347605},
journal = {Social Forces},
number = {4},
pages = {1297--1316},
publisher = {Oxford University Press},
title = {{The Origins of Sex Differences in Science}},
volume = {68},
year = {1990}
}
@book{magnus99,
abstract = {This book provides a self-contained and unified treatment of matrix differential calculus, aimed at econometricians and statisticians. It can be used as a textbook for senior undergraduate or graduate courses on the subject, and will also be a valuable source for professional econometricians and statisticians who want to learn and apply these important techniques. The authors base their approach on differentials rather than derivatives, and they show that the use of differentials is elegant, easy and considerably more useful in applications. No specialist knowledge of matrix algebra or calculus is required, since the basics of matrix algebra are covered in the first three chapters with a thorough treatment of multivariable calculus provided in Chapters Four to Seven. Exercises are included in each chapter, and many examples which illustrate applications of the theory are considered in detail.},
author = {Magnus, Jan R and Neudecker, Heinz},
edition = {Second},
isbn = {0471986321 9780471986324 047198633X 9780471986331},
keywords = {calculus economics linear.algebra matrix textbook},
publisher = {John Wiley},
title = {{Matrix Differential Calculus with Applications in Statistics and Econometrics}},
year = {1999}
}
@article{Cruz-Uribe2002,
abstract = {We give error bounds for the trapezoidal rule and Simpson's rule for " rough " con-tinuous functions—for instance, functions which are H{\"{o}}lder continuous, of bounded variation, or which are absolutely continuous and whose derivative is in L p . These differ considerably from the classical results, which require the functions to have continuous higher derivatives. Further, we show that our results are sharp, and in many cases precisely characterize the functions for which equality holds. One consequence of these results is that for rough functions, the error esti-mates for the trapezoidal rule are better (that is, have smaller constants) than those for Simpson's rule.},
author = {Cruz-Uribe, D and Neugebauer, C J and Lafayette, W},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cruz-Uribe, Neugebauer, Lafayette - 2002 - Sharp error bounds for the trapezoidal rule and Simpson's rule.pdf:pdf},
issn = {00142980},
journal = {Journal of Inequalities in Pure and Applied Mathematics},
keywords = {2000 mathematics subject classification,26a42,41a55,and phrases,numerical integration,s rule,simpson,trapezoidal rule},
number = {4},
pages = {1--22},
title = {{Sharp error bounds for the trapezoidal rule and Simpson's rule}},
volume = {3},
year = {2002}
}
@article{Hall2016,
author = {Hall, Daniel B},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall - 2016 - Zero-Inflated Poisson and Binomial Regression with Random Effects A Case Study Author ( s ) Daniel B . Hall Published by.pdf:pdf},
keywords = {em algorithm,excess zeros,generalized linear mixed model,heterogeneity,mixed effects,overdispersion,repeated measures},
number = {4},
pages = {1030--1039},
title = {{Zero-Inflated Poisson and Binomial Regression with Random Effects : A Case Study Author ( s ): Daniel B . Hall Published by : International Biometric Society Stable URL : http://www.jstor.org/stable/2677034 REFERENCES Linked references are available on JS}},
volume = {56},
year = {2016}
}
@article{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {1206.1901},
author = {Neal, Radford M.},
doi = {doi:10.1201/b10905-6},
eprint = {1206.1901},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:pdf},
isbn = {9781420079418},
issn = {{\textless}null{\textgreater}},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
pmid = {25246403},
title = {{MCMC using Hamiltonian dynamics}},
year = {2011}
}
@book{trefethen97,
author = {Trefethen, Lloyd N and Bau, David},
isbn = {0898713617},
keywords = {eigenvalues linear.algebra matrix numerical numeri},
publisher = {SIAM},
title = {{Numerical Linear Algebra}},
year = {1997}
}
@book{Golub:1996:MC:248979,
address = {Baltimore, MD, USA},
author = {Golub, Gene H and {Van Loan}, Charles F},
isbn = {978-1-4214-0794-4},
publisher = {Johns Hopkins University Press},
title = {{Matrix Computations (4th Ed.)}},
year = {2013}
}
@article{Ishwaran2005,
abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
archivePrefix = {arXiv},
arxivId = {math/0505633},
author = {Ishwaran, Hemant and Rao, J. Sunil},
doi = {10.1214/009053604000001147},
eprint = {0505633},
file = {:home/markg/Downloads/0505633.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
number = {2},
pages = {730--773},
pmid = {3448605},
primaryClass = {math},
title = {{Spike and slab variable selection: Frequentist and Bayesian strategies}},
volume = {33},
year = {2005}
}
@article{Chipman2014,
author = {Chipman, Hugh and George, Edward I and Mcculloch, Robert E and Clyde, M and Foster, Dean P and Stine, Robert},
doi = {10.1214/lnms/1215540964},
file = {:home/markg/Downloads/ims.pdf:pdf},
isbn = {0-940600-52-8},
issn = {0749-2170},
journal = {Lecture Notes-Monograph Series},
number = {2001},
pages = {65--134},
title = {{The Practical Implementation of Bayesian Model Selection}},
volume = {38},
year = {2014}
}
@book{Press:2007:NRE:1403886,
address = {New York, NY, USA},
author = {Press, William H and Teukolsky, Saul A and Vetterling, William T and Flannery, Brian P},
edition = {3},
isbn = {0521880688, 9780521880688},
publisher = {Cambridge University Press},
title = {{Numerical Recipes 3rd Edition: The Art of Scientific Computing}},
year = {2007}
}
@article{Madigan1994,
author = {Madigan, David and Raftery, Adrian E},
file = {:home/markg/Downloads/MadiganRaftery-JASA-1994.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {428},
pages = {1535--1546},
title = {{Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window}},
volume = {89},
year = {1994}
}
@article{Ward2008,
abstract = {Many tools have become available for biologists for evaluating competing ecological models - models may be judged based on the fit to data alone (e.g. likelihood), or more formal statistical criteria may be used. Because of the implied assumptions of each tool, model selection criteria should be chosen a priori for the problem at hand, - a model that is considered 'good' in its explanatory power may not be the best choice for a problem that requires prediction. In this paper, I review the behavior and assumptions of the four most commonly used statistical criteria (Akaike's Information Criterion, AIC; Schwarz or Bayesian Information Criterion, BIC; Deviance Information Criterion, DIC; Bayes factors). Second, I illustrate differences in these model selection tools by applying the four criteria to thousands of simulated abundance trajectories. With the simulation model known, I examine whether each of the criteria are useful in selecting models to evaluate simple questions, such as whether time series support evidence of density dependent population growth. Across simulations, the maximum likelihood criteria consistently favored simpler population models when compared to Bayesian criteria. Among the Bayesian criteria, the Bayes factor favored the correct simulation model more frequently than the Deviance Information Criterion. There was considerable uncertainty in the ability of the Bayes factor to discriminate between models, this tool selected the simulation model slightly more frequently than other approaches. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Ward, Eric J.},
doi = {10.1016/j.ecolmodel.2007.10.030},
file = {:home/markg/Downloads/1-s2.0-S0304380007005558-main.pdf:pdf},
isbn = {03043800},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {AIC,BIC,Bayes factor,DIC,Model selection},
number = {1-2},
pages = {1--10},
pmid = {2742},
title = {{A review and comparison of four commonly used Bayesian and maximum likelihood model selection tools}},
volume = {211},
year = {2008}
}
@article{Rockova2014,
abstract = {In multiple regression under the normal linear model, the presence of multicollinearity is well known to lead to unreliable and unstable maximum likelihood estimates. This can be particularly troublesome for the problem of variable selection where it becomes more difficult to distinguish between subset models. Here we show how adding a spike-and-slab prior mitigates this difficulty by filtering the likelihood surface into a posterior distribution that allocates the relevant likelihood information to each of the subset model modes. For identification of promising high posterior models in this setting, we consider three EM algorithms, the fast closed form EMVS version of Rockova and George (2014) and two new versions designed for variants of the spike-and-slab formulation. For a multimodal posterior under multicollinearity, we compare the regions of convergence of these three algorithms. Deterministic annealing versions of the EMVS algorithm are seen to substantially mitigate this multimodality. A single simple running example is used for illustration throughout.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1007/s40300-014-0047-y},
eprint = {NIHMS150003},
file = {:home/markg/Downloads/art{\%}3A10.1007{\%}2Fs40300-014-0047-y.pdf:pdf},
isbn = {8585348585},
issn = {2281695X},
journal = {Metron},
keywords = {Deterministic annealing,EM algorithm,EMVS,Q-prior,Variable selection},
number = {2},
pages = {217--229},
pmid = {25419004},
title = {{Negotiating multicollinearity with spike-and-slab priors}},
volume = {72},
year = {2014}
}
@book{MacKay:2002:ITI:971143,
address = {New York, NY, USA},
author = {MacKay, David J C},
isbn = {0521642981},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference {\&} Learning Algorithms}},
year = {2002}
}
@article{Rockova2016,
author = {Ro{\v{c}}kov{\'{a}}, Veronika},
file = {:home/markg/Downloads/particleEM.pdf:pdf},
pages = {1--31},
title = {{Particle EM for Variable Selection}},
year = {2016}
}
@article{Betancourt2017,
abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
archivePrefix = {arXiv},
arxivId = {1701.02434},
author = {Betancourt, Michael},
eprint = {1701.02434},
file = {:home/markg/Downloads/1701.02434.pdf:pdf},
title = {{A Conceptual Introduction to Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1701.02434},
year = {2017}
}
@article{Besag1989,
abstract = {This paper addresses the problem of aggregating a number of expert opinions which have been expressed in some numerical form in order to reflect individual uncertainty vis-a-vis a quantity of interest. The primary focus is consensus belief formation and expert use, although some relevant aspects of group decision making are also reviewed. A taxonomy of solutions is presented which serves as the framework for a survey of recent theoretical developments in the area. A number of current research directions are mentioned and an extensive, current annotated bibliography is included.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Besag, Julian and Green, Peter and Higdon, David and Mengersen, Kerrie},
doi = {10.1214/ss/1177013604},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/Downloads/euclid.ss.1177013525.pdf:pdf},
isbn = {0412343908},
issn = {0883-4237},
journal = {Statistics},
number = {1},
pages = {409--435},
pmid = {20948974},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to Statistical Science. {\textregistered} www.jstor.org}},
url = {http://projecteuclid.org/euclid.ss/1177010123},
volume = {10},
year = {1989}
}
@article{Lee2016,
author = {Lee, Cathy Yuen Yi and Wand, Matt P.},
doi = {10.1002/bimj.201500007},
file = {:home/markg/Downloads/Lee{\_}et{\_}al-2016-Biometrical{\_}Journal.pdf:pdf},
issn = {15214036},
journal = {Biometrical Journal},
keywords = {Bayesian computing,Longitudinal data,Matrix decomposition,Multilevel model,Variational approximations},
number = {4},
pages = {868--895},
title = {{Streamlined mean field Variational Bayes for longitudinal and multilevel data analysis}},
volume = {58},
year = {2016}
}
@article{Ghosh2015,
author = {Ghosh, Joyee},
doi = {10.1002/wics.1352},
file = {:home/markg/Documents/ghosh{\_}wirescompstats{\_}2015.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Bayesian model averaging,Bayesian variable selection,Highest probability model,Marginal inclusion probability,Markov chain Monte Carlo},
number = {3},
pages = {185--193},
title = {{Bayesian model selection using the median probability model}},
volume = {7},
year = {2015}
}
@article{McGrory2007,
abstract = {Variational methods, which have become popular in the neural computing/machine learning literature, are applied to the Bayesian analysis of mixtures of Gaussian distributions. It is also shown how the deviance information criterion, (DIC), can be extended to these types of model by exploiting the use of variational approximations. The use of variational methods for model selection and the calculation of a DIC are illustrated with real and simulated data. The variational approach allows the simultaneous estimation of the component parameters and the model complexity. It is found that initial selection of a large number of components results in superfluous components being eliminated as the method converges to a solution. This corresponds to an automatic choice of model complexity. The appropriateness of this is reflected in the DIC values. ?? 2006 Elsevier B.V. All rights reserved.},
author = {McGrory, C. A. and Titterington, D. M.},
doi = {10.1016/j.csda.2006.07.020},
file = {:home/markg/Documents/1-s2.0-S0167947306002362-main.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Bayesian analysis,Deviance information criterion (DIC),Mixtures,Variational approximations},
number = {11},
pages = {5352--5367},
title = {{Variational approximations in Bayesian model selection for finite mixture distributions}},
volume = {51},
year = {2007}
}
@article{Journal2017,
author = {Journal, Source and Statistical, Royal and Series, Society and Methodology, B Statistical},
file = {:home/markg/Documents/3088868.pdf:pdf},
number = {2},
pages = {413--428},
title = {{Modelling and Smoothing Parameter Estimation with Multiple Quadratic Penalties Author ( s ): S . N . Wood Published by : Wiley for the Royal Statistical Society Stable URL : http://www.jstor.org/stable/3088868 Linked references are available on JSTOR for }},
volume = {62},
year = {2017}
}
@article{Clyde2004,
abstract = {...  importance of poste- rior computation, especially for large p, this work also contained prescient suggestions such as importance  sampling and branch ... variable selection quickly took off when it became apparent that MCMC algorithms could be used to simulate a (sequentially ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {1105.5054v1},
author = {Clyde, M and George, E I},
doi = {Doi 10.1214/088342304000000035},
eprint = {1105.5054v1},
file = {:home/markg/Documents/euclid.ss.1089808274.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayes factors,butions,classification and regression trees,linear and nonparametric regression,model averaging,monte carlo,objective prior distri-,reversible jump markov chain,variable selection},
number = {1},
pages = {81--94},
title = {{Model uncertainty}},
url = {http://www.jstor.org/stable/10.2307/4144374{\%}5Cnpapers2://publication/uuid/E6D37250-49DF-4C7A-863A-2BD7C1215934},
volume = {19},
year = {2004}
}
@article{You2014,
abstract = {The concept of degrees of freedom plays an important role in statistical modeling and is commonly used for measuring model complexity. The number of unknown parameters, which is typically used as the degrees of freedom in linear regression models, may fail to work in some modeling procedures, in particular for linear mixed effects models. In this article, we propose a new definition of generalized degrees of freedom in linear mixed effects models. It is derived from using the sum of the sensitivity of the expected fitted values with respect to their underlying true means. We explore and compare data perturbation and the residual bootstrap to empirically estimate model complexity. We also show that this empirical generalized degrees of freedom measure satisfies some desirable properties and is useful for the selection of linear mixed effects models.},
author = {You, Chong and M{\"{u}}ller, Samuel and Ormerod, John T.},
doi = {10.1007/s11222-014-9488-7},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/You, M{\"{u}}ller, Ormerod - 2014 - On generalized degrees of freedom with application in linear mixed models selection.pdf:pdf},
isbn = {1122201494},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {bootstrap,deviance,information criterion,resampling},
pages = {199--210},
publisher = {Springer US},
title = {{On generalized degrees of freedom with application in linear mixed models selection}},
year = {2014}
}
@article{Muller2016,
author = {M{\"{u}}ller, Samuel and Welsh, A H},
doi = {10.1198/016214505000000529},
file = {:home/markg/Documents/27590673.pdf:pdf},
keywords = {bootstrap model selection,outlier,robust model selection,schwarz bayesian information criterion,stratified bootstrap},
number = {March},
pages = {1297--1310},
title = {{Outlier Robust Model Selection in Linear Regression Outlier Robust Model Selection in Linear Regression}},
volume = {1459},
year = {2016}
}
@book{Venables2002,
address = {New York},
annote = {ISBN 0-387-95457-0},
author = {Venables, W N and Ripley, B D},
edition = {Fourth},
publisher = {Springer},
title = {{Modern Applied Statistics with S}},
year = {2002}
}
@article{Tarr2015,
abstract = {The mplot package provides an easy to use implementation of model stability and variable inclusion plots (M$\backslash$"uller and Welsh 2010; Murray, Heritier, and M$\backslash$"uller 2013) as well as the adaptive fence (Jiang, Rao, Gu, and Nguyen 2008; Jiang, Nguyen, and Rao 2009) for linear and generalised linear models. We provide a number of innovations on the standard procedures and address many practical implementation issues including the addition of redundant variables, interactive visualisations and approximating logistic models with linear models. An option is provided that combines our bootstrap approach with glmnet for higher dimensional models. The plots and graphical user interface leverage state of the art web technologies to facilitate interaction with the results. The speed of implementation comes from the leaps package and cross-platform multicore support.},
archivePrefix = {arXiv},
arxivId = {1509.07583},
author = {Tarr, Garth and M{\"{u}}ller, Samuel and Welsh, Alan},
eprint = {1509.07583},
file = {:home/markg/Documents/1509.07583v1.pdf:pdf},
keywords = {generalised linear,linear models,mixed models,model selection,variable selection},
number = {1996},
title = {{mplot: An R Package for Graphical Model Stability and Variable Selection Procedures}},
url = {http://arxiv.org/abs/1509.07583},
year = {2015}
}
@misc{Tibshirani1996,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
eprint = {11/73273},
file = {:home/markg/Documents/lasso.pdf:pdf},
isbn = {0849320240},
issn = {00359246},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression Selection and Shrinkage via the Lasso}},
volume = {58},
year = {1996}
}
@article{Casella1980,
author = {Casella, G},
file = {:home/markg/Documents/euclid.aos.1176345141.pdf:pdf},
journal = {The Annals of Statistics},
pages = {1036--1056},
title = {{Minimax Ridge Regression Estimation}},
volume = {8},
year = {1980}
}
@article{Zou2006,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection. The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
archivePrefix = {arXiv},
arxivId = {NIHMS201118},
author = {Zou, Hui},
doi = {10.1198/016214506000000735},
eprint = {NIHMS201118},
file = {:home/markg/Documents/zou2006.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {asymptotic normality,lasso,minimax,oracle inequality,oracle procedure,variable selection},
number = {476},
pages = {1418--1429},
pmid = {20122298},
title = {{The Adaptive Lasso and Its Oracle Properties}},
volume = {101},
year = {2006}
}
@article{Akaike1974,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/Documents/01100705.pdf:pdf},
isbn = {0018-9286 VO - 19},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
pmid = {1100705},
title = {{A New Look at the Statistical Model Identification}},
volume = {19},
year = {1974}
}
@article{Casella1985,
author = {Casella, George},
doi = {10.2307/2288496},
file = {:home/markg/Documents/2288496.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {matrix conditioning,mean,numerical stability},
number = {391},
pages = {753},
title = {{Condition Numbers and Minimax Ridge Regression Estimators}},
url = {http://www.jstor.org/stable/2288496?origin=crossref},
volume = {80},
year = {1985}
}
@article{Knight2000,
author = {Knight, Keith and Fu, Wenjiang},
file = {:home/markg/Documents/euclid.aos.1015957397.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1356--1378},
title = {{ASYMPTOTICS FOR LASSO-TYPE ESTIMATORS By Keith Knight 1 and Wenjiang Fu 2}},
volume = {28},
year = {2000}
}
@misc{,
file = {:home/markg/Documents/rmh1997.pdf:pdf},
title = {rmh1997.pdf}
}
@misc{Mitchell1988,
abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation $\sigma$, where ln($\sigma$) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter $\gamma$, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing $\gamma$, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against $\gamma$ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose $\gamma$. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
author = {Mitchell, Tj J. and Beauchamp, Jj J.},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2290129},
file = {:home/markg/Documents/joelucas1.pdf:pdf},
isbn = {Journal of the American Statistical Association, Vol. 83, No. 404, December 1988: pp. 1023–1032},
issn = {01621459},
number = {404},
pages = {1023--1032},
title = {{Bayesian Variable Selection in Linear Regression}},
volume = {83},
year = {1988}
}
@article{Pauler1999,
abstract = {In this article we consider tests of variance components using Bayes factors. Such tests arise in many fields of application, including medicine, agriculture, and engineering. When using Bayes factors, the choice of prior distribution on the parameter of interest is of great importance; we propose a "unit-information" reference method for variance component models. The calculation of Bayes factors in this context is not straightforward; there are well-documented difficulties with Markov chain Monte Carlo approaches such as Gibbs sampling, and the usual Laplace approximation is not appropriate, due to the boundary null hypothesis. We describe both an importance sampling approach and an analytical approximation for calculating the numerator and denominator of the Bayes factor. The importance sampling approach is straightforward to implement and also forms the basis for a rejection algorithm that allows generation of samples from the posterior distributions under the null and alternative hypotheses. We suggest that the proposal for the rejection algorithm be based on the likelihood of a subset of the data. For large samples, we develop a boundary Laplace approximation that is accurate to order op(l). We investigate the accuracy of the approximation via simulation, and examine its relationship to the Schwarz criterion. We illustrate the importance sampling/rejection method and boundary Laplace approximation on a number of examples, including a challenging two-way, highly unbalanced dataset and compare our methods with frequentist alternatives.},
author = {Pauler, Dk and Wakefield, Jc and Kass, Re},
doi = {10.1080/01621459.1999.10473877},
file = {:home/markg/Documents/2669938.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {boundary problem,importance sampling,laplace,reference prior,rejection method,s method,schwarz crite-},
number = {448},
pages = {1242--1253},
title = {{Bayes Factors and Approximations for Variance Component Models.}},
url = {http://www.questia.com/PM.qst?a=o{\&}se=gglsc{\&}d=5002342459},
volume = {94},
year = {1999}
}
@article{Clyde2000,
abstract = {Wavelet shrinkage estimation is an increasingly popular method for signal denoising and compression. Although Bayes estimators can provide excellent mean-squared error ({\{}MSE){\}} properties, the selection of an effective prior is a difficult task. To address this problem, we propose empirical Bayes ({\{}EB){\}} prior selection methods for various error distributions including the normal and the heavier-tailed Student t-distributions. Under such {\{}EB{\}} prior distributions, we obtain threshold shrinkage estimators based on model selection, and multiple-shrinkage estimators based on model averaging. These {\{}EB{\}} estimators are seen to be computationally competitive with standard classical thresholding methods, and to be robust to outliers in both the data and wavelet domains. Simulated and real examples are used to illustrate the flexibility and improved {\{}MSE{\}} performance of these methods in a wide variety of settings.},
author = {Clyde, Merlise and George, Edward I},
doi = {10.1111/1467-9868.00257},
file = {:home/markg/Documents/Clyde George 00 JRSSB.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society Series B},
number = {4},
pages = {681--698},
title = {{Flexible Empirical Bayes Estimation for Wavelets}},
volume = {62},
year = {2000}
}
@article{Fernandez2001,
abstract = {In contrast to a posterior analysis given a particular sampling model, posterior model probabilities in the context of model uncertainty are typically rather sensitive to the specification of the prior. In particular, 'diffuse' priors on model-specific parameters can lead to quite unexpected consequences. Here we focus on the practically relevant situation where we need to entertain a (large) number of sampling models and we have (or wish to use) little or no subjective prior information. We aim at providing an 'automatic' or 'benchmark' prior structure that can be used in such cases. We focus on the normal linear regression model with uncertainty in the choice of regressors. We propose a partly non-informative prior structure related to a natural conjugate g-prior specification, where the amount of subjective information requested from the user is limited to the choice of a single scalar hyperparameter g0j. The consequences of different choices for g0j are examined. We investigate theoretical properties, such as consistency of the implied Bayesian procedure. Links with classical information criteria are provided. More importantly, we examine the finite sample implications of several choices of g0j in a simulation study. The use of the MC3 algorithm of Madigan and York (Int. Stat. Rev. 63 (1995) 215), combined with efficient coding in Fortran, makes it feasible to conduct large simulations. In addition to posterior criteria, we shall also compare the predictive performance of different priors. A classic example concerning the economics of crime will also be provided and contrasted with results in the literature. The main findings of the paper will lead us to propose a 'benchmark' prior specification in a linear regression context with model uncertainty. ?? 2001 Elsevier Science S.A. All rights reserved.},
author = {Fern{\'{a}}ndez, Carmen and Ley, Eduardo and Steel, Mark F J},
doi = {10.1016/S0304-4076(00)00076-2},
file = {:home/markg/Documents/1-s2.0-S0304407600000762-main.pdf:pdf},
isbn = {0304-4076},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Bayes factors,Markov chain Monte Carlo,Posterior odds,Prior elicitation},
number = {2},
pages = {381--427},
title = {{Benchmark priors for Bayesian model averaging}},
volume = {100},
year = {2001}
}
@article{Hansen2001,
author = {Hansen, Mark H and Yu, B},
doi = {10.1198/016214501753168398},
file = {:home/markg/Documents/2670311.pdf:pdf},
isbn = {0162145017531},
issn = {0162-1459},
journal = {Journal of the American Statistical {\ldots}},
keywords = {aic,bayesian methods},
number = {454},
pages = {746--774},
title = {{Model Selection and the Principle of Minimum Description Length}},
volume = {96},
year = {2001}
}
@misc{,
file = {:home/markg/Documents/kass1995.pdf:pdf},
title = {{kass1995BayesFactors}}
}
@article{George2000a,
abstract = {The problem of variable selection is one of the most pervasive model selection problems in statistical applications. Often referred to as the problem of subset selection, it arises when one wants to model the relationship between a variable of interest and a subset of potential explanatory variables or predictors, but there is uncertainty about which subset to use. This vignette reviews some of the key developments that have led to the wide variety of approaches for this problem.},
author = {George, Edward I and George, Edward},
doi = {10.2307/2669776},
file = {:home/markg/Documents/The Variable Selection Problem.pdf:pdf},
isbn = {0162-1459},
issn = {01621459 (ISSN)},
journal = {Journal of the American Statistical Association},
number = {452},
pages = {1304--1308},
title = {{The Variable Selection Problem}},
volume = {95},
year = {2000}
}
@article{Johnstone2005,
abstract = {This paper explores a class of empirical Bayes methods for level-dependent threshold selection in wavelet shrinkage. The prior considered for each wavelet coefficient is a mixture of an atom of probability at zero and a heavy-tailed density. The mixing weight, or sparsity parameter, for each level of the transform is chosen by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold. Details of the calculations needed for implementing the procedure are included. In practice, the estimates are quick to compute and there is software available. Simulations on the standard model functions show excellent performance, and applications to data drawn from various fields of application are used to explore the practical performance of the approach. By using a general result on the risk of the corresponding marginal maximum likelihood approach for a single sequence, overall bounds on the risk of the method are found subject to membership of the unknown function in one of a wide range of Besov classes, covering also the case of f of bounded variation. The rates obtained are optimal for any value of the parameter p in (0,$\backslash$infty], simultaneously for a wide range of loss functions, each dominating the L{\_}q norm of the $\backslash$sigmath derivative, with $\backslash$sigma$\backslash$ge0 and 0{\textless}q$\backslash$le2.},
archivePrefix = {arXiv},
arxivId = {math/0508281},
author = {Johnstone, Iain M. and Silverman, Bernard W.},
doi = {10.1214/009053605000000345},
eprint = {0508281},
file = {:home/markg/Documents/0508281.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Adaptivity,Bayesian inference,Nonparametric regression,Smoothing,Sparsity},
number = {4},
pages = {1700--1752},
primaryClass = {math},
title = {{Empirical Bayes selection of wavelet thresholds}},
volume = {33},
year = {2005}
}
@article{Leamer1978,
abstract = {The computation and selection of constrained regressions may be motivated$\backslash$nby prior information and, if so, a regression selection strategy$\backslash$nreveals the implicit prior. The selection strategies of principal$\backslash$ncomponent regression, stepwise regression, and imposing equality$\backslash$nconstraints are connected with prior densities which are uniform$\backslash$non spheres, hyperbolas, and cones, respectively. Omitting variables$\backslash$nin a predetermined order reveals lexicographic priors.},
author = {Leamer, Edward E},
doi = {10.1080/01621459.1978.10480058},
file = {:home/markg/Documents/2286604.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Regression; Constrained regression; Bayesian infer},
number = {363},
pages = {580--587},
title = {{{\{}R{\}}egression selection strategies and revealed priors}},
url = {http://www.jstor.org/stable/2286604},
volume = {73},
year = {1978}
}
@misc{Geweke1996,
author = {Geweke, J},
booktitle = {Bayesian Statistics 5 (Edited by Bernardo, J.M., Berger, J. O., Dawid, A. P. and Smith, A. F. M.),},
file = {:home/markg/Documents/wp539.pdf:pdf},
pages = {609--620},
title = {{Variable selection and model comparison in regression}},
year = {1996}
}
@misc{George1993,
abstract = {Abstract A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability?the promising ones?can then be identified by their more frequent appearance in the Gibbs sample. Abstract A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability?the promising ones?can then be identified by their more frequent appearance in the Gibbs sample.},
author = {George, Edward I. and McCulloch, Robert E.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1993.10476353},
file = {:home/markg/Documents/george+mcculloch-1993.pdf:pdf},
isbn = {263627362},
issn = {0162-1459},
number = {423},
pages = {881--889},
title = {{Variable Selection via Gibbs Sampling}},
volume = {88},
year = {1993}
}
@article{Casella2006,
abstract = {A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis–Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples.},
author = {Casella, George and Moreno, El{\'{i}}as},
doi = {10.1198/016214505000000646},
file = {:home/markg/Documents/ObjectiveBayes.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hastings algorithm,intrinsic prior,methods,metropolis,monte carlo markov chain,normal linear regression},
number = {473},
pages = {157--167},
title = {{Objective Bayesian Variable Selection}},
volume = {101},
year = {2006}
}
@article{Berger2001,
abstract = {The basics of the Bayesian approach to model selection are first presented, as well as the motivations for the Bayesian approach. We then review four methods of developing default Bayesian procedures that have undergone con- siderable recent development, the Conventional Prior approach, the Bayes Information Criterion, the Intrinsic Bayes Factor, and the Fractional Bayes Factor. As part of the review, these methods are illustrated on examples involving the normal linear model. The later part of the chapter focuses on comparison of the four approaches, and includes an extensive discussion of criteria for judging model selection procedures.},
author = {Berger, James O and Pericchi, Luis R},
doi = {10.1214/lnms/1215540968},
file = {:home/markg/Documents/euclid.lnms.1215540968.pdf:pdf},
isbn = {07492170},
issn = {07492170},
journal = {IMS Lecture Notes - Monograph Series},
keywords = {Mathematical statistics,Statistics,methods,statistic},
pages = {135--207},
title = {{Objective Bayesian Methods for Model Selection: Introduction and Comparison}},
volume = {38},
year = {2001}
}
@article{Berger2012,
author = {Berger, By James and Pericchi, Luis R and Varshavsky, Julia A},
file = {:home/markg/Documents/25051210.pdf:pdf},
number = {3},
pages = {307--321},
title = {{Indian Statistical Institute Bayes Factors and Marginal Distributions in Invariant Situations Author ( s ): James O . Berger , Luis R . Pericchi and Julia A . Varshavsky Reviewed work ( s ): Source : Sankhyā : The Indian Journal of Statistics , Series A (}},
volume = {60},
year = {1998}
}
@article{Barbieri2004,
abstract = {Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0406464v1},
author = {Barbieri, Maria Maddalena and Berger, James O.},
doi = {10.1214/009053604000000238},
eprint = {0406464v1},
file = {:home/markg/Documents/euclid.aos.1085408489.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian linear models,Predictive distribution,Squared error loss,Variable selection},
number = {3},
pages = {870--897},
primaryClass = {arXiv:math},
title = {{Optimal predictive model selection}},
volume = {32},
year = {2004}
}
@article{Butler2002,
abstract = {In this paper we present Laplace approximations for two functions of matrix argument: the Type I confluent hypergeometric function and the Gauss hypergeometric function. Both of these functions play an important role in distribution theory in multivariate analysis, but ...},
author = {Butler, R.W. and a.T.a. Wood},
file = {:home/markg/Documents/euclid.aos.1031689021.pdf:pdf},
issn = {0090-5364},
journal = {Annals of statistics},
number = {4},
pages = {1155--1177},
title = {{Laplace approximations for hypergeometric functions with matrix argument}},
url = {http://www.jstor.org/stable/1558699},
volume = {30},
year = {2002}
}
@article{Zucchini2000,
abstract = {This paper is an introduction to model selection intended for nonspecialists who have knowledge of the statistical concepts covered in a typical first (occasionally second) statistics course. The intention is to explain the ideas that generate frequentist methodology for model selection, for example the Akaike information criterion, bootstrap criteria, and cross-validation criteria. Bayesian methods, including the Bayesian information criterion, are also mentioned in the context of the framework outlined in the paper. The ideas are illustrated using an example in which observations are available for the entire population of interest. This enables us to examine and to measure effects that are usually invisible, because in practical applications only a sample from the population is observed. The problem of selection bias, a hazard of which one needs to be aware in the context of model selection, is also discussed. Copyright 2000 Academic Press.},
author = {Zucchini, W},
doi = {10.1006/jmps.1999.1276},
file = {:home/markg/Documents/241801c342dbdcf4cb212eef4634797c15b9.pdf:pdf},
isbn = {00222496},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
number = {1},
pages = {41--61},
pmid = {10733857},
title = {{An Introduction to Model Selection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10733857},
volume = {44},
year = {2000}
}
@article{Breiman1985,
abstract = {In regression analysis the response variable Y and the predictor variables X1, ⋯, Xp are often replaced by functions $\theta$(Y) and $\phi$1(X1), ⋯, $\phi$p(Xp). We discuss a procedure for estimating those functions $\theta$* and $\phi$*1, ⋯, $\phi$*p that minimize e{\^{}}2 = E{\{}$\backslash$lbrack $\theta$(Y) - $\backslash$sum{\^{}}p{\_}{\{}j = 1{\}} $\phi${\_}j(X{\_}j) $\backslash$rbrack{\^{}}2{\}}/ $\backslash$operatorname{\{}var{\}} $\backslash$lbrack$\theta$(Y) $\backslash$rbrack, given only a sample {\{}(yk, xk1, ⋯, xkp), 1 ≤ k ≤ N{\}} and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, $\theta$* and $\phi$* satisfy $\rho$* = $\rho$($\theta$*, $\phi$*) = max$\theta$,$\phi$$\rho$[$\theta$(Y), $\phi$(X)], where $\rho$ is the product moment correlation coefficient and $\rho$* is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables.},
author = {Breiman, Leo and Friedman, Jerome H},
doi = {10.1080/01621459.1985.10478157},
file = {:home/markg/Documents/2288473.pdf:pdf},
isbn = {01621459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {smoothing},
number = {391},
pages = {580--598},
title = {{Estimating Optimal Transformations for Multiple Regression and Correlation}},
volume = {80},
year = {1985}
}
@article{Congress,
author = {Congress, International and Lindley, D V},
file = {:home/markg/Documents/44-3-4-533.pdf:pdf},
journal = {Methods},
number = {3},
pages = {533--534},
title = {{Miscellanea 533}}
}
@article{Hoeting1996,
abstract = {We suggest a method for simultaneous variable selection and outlier identification based on the computation of posterior model probabilities. This avoids the problem that the model you select depends upon the order in which variable selection and outlier identification are carried out. Our method can find multiple outliers and appears to be successful in identifying masked outliers. We also address the problem of model uncertainty via Bayesian model averaging. For problems where the number of models is large, we suggest a Markov chain Monte Carlo approach to approximate the Bayesian model average over the space of all possible variables and outliers under consideration. Software for implementing this approach is described. In an example, we show that model averaging via simultaneous variable selection and outlier identification improves predictive performance and provides more accurate prediction intervals as compared to any single model that might reasonably be selected.},
author = {Hoeting, Jennifer and Raftery, Adrian E and Madigan, David},
doi = {10.1016/0167-9473(95)00053-4},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoeting, Raftery, Madigan - 1996 - A Method for Simultaneous Variable Selection and Outlier Identification in Linear Regression.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {Bayesian model averaging,Markov chain Monte Carlo model composition,Masking,Model uncertainty,Posterior model probability},
number = {3},
pages = {251--270},
title = {{A Method for Simultaneous Variable Selection and Outlier Identification in Linear Regression}},
url = {http://www.sciencedirect.com/science/article/pii/0167947395000534},
volume = {22},
year = {1996}
}
@article{Raftery1997,
author = {Raftery, A E and Madigan, D and Hoeting, J A},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery, Madigan, Hoeting - 1997 - Bayesian Model Averaging for Linear Regression.pdf:pdf},
journal = {Journal of the American Statistical Association},
pages = {179--191},
title = {{Bayesian Model Averaging for Linear Regression}},
volume = {92},
year = {1997}
}
@article{Mueller2007,
abstract = {In this paper, we extend to generalized linear models (including logistic and other binary regression models, Poisson regression and gamma regression models) the robust model selection methodology developed by Mueller and Welsh (2005; JASA) for linear regression models. As in Mueller and Welsh (2005), we combine a robust penalized measure of fit to the sample with a robust measure of out of sample predictive ability which is estimated using a post-stratified m-out-of-n bootstrap. A key idea is that the method can be used to compare different estimators (robust and nonrobust) as well as different models. Even when specialized back to linear regression models, the methodology presented in this paper improves on that of Mueller and Welsh (2005). In particular, we use a new bias-adjusted bootstrap estimator which avoids the need to centre the explanatory variables and to include an intercept in every model. We also use more sophisticated arguments than Mueller and Welsh (2005) to establish an essential monotonicity condition.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.2349v1},
author = {Mueller, Samuel and Welsh, a H},
eprint = {arXiv:0711.2349v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mueller, Welsh - 2007 - Robust model selection in generalized linear models.pdf:pdf},
journal = {arXiv07112349v1 statME},
keywords = {bootstrap model selection,estimation,generalized linear models,paired bootstrap,robust,robust model selection,stratified bootstrap},
number = {2005},
pages = {24},
title = {{Robust model selection in generalized linear models}},
url = {http://arxiv.org/abs/0711.2349},
year = {2007}
}
@article{Volinsky2000,
abstract = {We investigate the Bayesian Information Criterion (BIC) for variable selection in models for censored survival data. Kass and Wasserman (1995, Journal of the American Statistical Association 90, 928-934) showed that BIC provides a close approximation to the Bayes factor when a unit-information prior on the parameter space is used. We propose a revision of the penalty term in BIC so that it is defined in terms of the number of uncensored events instead of the number of observations. For a simple censored data model, this revision results in a better approximation to the exact Bayes factor based on a conjugate unit-information prior. In the Cox proportional hazards regression model, we propose defining BIC in terms of the maximized partial likelihood. Using the number of deaths rather than the number of individuals in the BIC penalty term corresponds to a more realistic prior on the parameter space and is shown to improve predictive performance for assessing stroke risk in the Cardiovascular Health Study.},
author = {Volinsky, C T and Raftery, a E},
doi = {10.1111/j.0006-341X.2000.00256.x},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Volinsky, Raftery - 2000 - Bayesian information criterion for censored survival models.pdf:pdf},
isbn = {0006-341X (Print)},
issn = {0006-341X},
journal = {Biometrics},
keywords = {bayes factor,cox proportional hazards model,exponential distribution,partial likelihood},
number = {1},
pages = {256--262},
pmid = {10783804},
title = {{Bayesian information criterion for censored survival models.}},
volume = {56},
year = {2000}
}
@article{Kass1993,
abstract = {... See Slate (1992) for a detailed discussion of sample sizes re - quired to obtain posterior Normality (which would guarantee accuracy of Laplace's method ) ... is available in S and in LispStat ( Tierney , 1989, 1990 ). ... For the general formulation see Kass ...},
author = {Kass, Robert E and Raftery, Adrian E},
doi = {10.2307/2291091},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kass, Raftery - 1993 - Bayes Factors and Model Uncertainty.pdf:pdf},
issn = {01621459},
journal = {Technical Report},
number = {254},
pages = {1--73},
title = {{Bayes Factors and Model Uncertainty}},
year = {1993}
}
@misc{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - socmeth1995.pdf.pdf:pdf},
title = {socmeth1995.pdf}
}
@article{Hoeting1999,
abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of- sample predictive performance. We also provide a catalogue of currently available BMA software.},
author = {Hoeting, Jennifer a and Madigan, David and Raftery, Adrian E and Volinsky, C T},
doi = {10.2307/2676803},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoeting et al. - 1999 - Bayesian model averaging a tutorial.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayesian graphical models,Bayesian model averaging,Markov chain Monte Carlo.,learning,model uncertainty},
number = {4},
pages = {382--417},
title = {{Bayesian model averaging: a tutorial}},
volume = {14},
year = {1999}
}
@article{Lesieur2017,
abstract = {This article is an extended version of previous work of the authors [40, 41] on low-rank matrix estimation in the presence of constraints on the factors into which the matrix is factorized. Low-rank matrix factorization is one of the basic methods used in data analysis for unsupervised learning of relevant features and other types of dimensionality reduction. We present a framework to study the constrained low-rank matrix estimation for a general prior on the factors, and a general output channel through which the matrix is observed. We draw a paralel with the study of vector-spin glass models - presenting a unifying way to study a number of problems considered previously in separate statistical physics works. We present a number of applications for the problem in data analysis. We derive in detail a general form of the low-rank approximate message passing (Low- RAMP) algorithm, that is known in statistical physics as the TAP equations. We thus unify the derivation of the TAP equations for models as different as the Sherrington-Kirkpatrick model, the restricted Boltzmann machine, the Hopfield model or vector (xy, Heisenberg and other) spin glasses. The state evolution of the Low-RAMP algorithm is also derived, and is equivalent to the replica symmetric solution for the large class of vector-spin glass models. In the section devoted to result we study in detail phase diagrams and phase transitions for the Bayes-optimal inference in low-rank matrix estimation. We present a typology of phase transitions and their relation to performance of algorithms such as the Low-RAMP or commonly used spectral methods.},
archivePrefix = {arXiv},
arxivId = {1701.00858},
author = {Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
eprint = {1701.00858},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lesieur, Krzakala, Zdeborov{\'{a}} - 2017 - Constrained Low-rank Matrix Estimation Phase Transitions, Approximate Message Passing and Applica.pdf:pdf},
pages = {1--64},
title = {{Constrained Low-rank Matrix Estimation: Phase Transitions, Approximate Message Passing and Applications}},
url = {http://arxiv.org/abs/1701.00858},
year = {2017}
}
@article{Papaspiliopoulos2016,
abstract = {We show how to carry out fully Bayesian variable selection and model averaging in linear models when both the number of observations and covariates are large. We work under the assumption that the Gram matrix is block-diagonal. Apart from orthogonal regression and various contexts where this is satisfied by design, this framework may serve in future work as a basis for computational approximations to more general design matrices with clusters of correlated predictors. Our approach returns the most probable model of any given size without resorting to numerical integration, posterior probabilities for any number of models by evaluating a single one-dimensional integral that can be computed upfront, and other quantities of interest such as variable inclusion probabilities and model averaged regression estimates by carrying out an adaptive, deterministic one-dimensional numerical integration. This integration and model search are done using novel schemes we introduce in this article. We do not require Markov Chain Monte Carlo. The overall computational cost scales linearly with the number of blocks, which can be processed in parallel, and exponentially with the block size, rendering it most adequate in situations where predictors are organized in many moderately-sized blocks.},
archivePrefix = {arXiv},
arxivId = {1606.03749},
author = {Papaspiliopoulos, Omiros and Rossell, David},
eprint = {1606.03749},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papaspiliopoulos, Rossell - 2016 - Scalable Bayesian variable selection and model averaging under block orthogonal design.pdf:pdf},
title = {{Scalable Bayesian variable selection and model averaging under block orthogonal design}},
year = {2016}
}
@article{Forte,
archivePrefix = {arXiv},
arxivId = {1612.02357},
author = {Forte, Anabel and Garc, Gonzalo},
eprint = {1612.02357},
file = {:home/markg/Documents/1612.02357.pdf:pdf},
pages = {1--27},
title = {{Methods and Tools for Bayesian Variable Selection and Model Averaging in Univariate Linear Regression}}
}
@article{Nott2012,
abstract = {Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture models and our article considers flexible mixtures of heteroscedastic regression (MHR) models where the response distribution is a normal mixture, with the component means, variances and mixture weights all varying as a function of covariates. Our article develops fast variational approximation methods for inference. Our motivation is that alternative computationally intensive MCMC methods for fitting mixture models are difficult to apply when it is desired to fit models repeatedly in exploratory analysis and model choice. Our article makes three contributions. First, a variational approximation for MHR models is described where the variational lower bound is in closed form. Second, the basic approximation can be improved by using stochastic approximation methods to perturb the initial solution to attain higher accuracy. Third, the advantages of our approach for model choice and evaluation compared to MCMC based approaches are illustrated. These advantages are particularly compelling for time series data where repeated refitting for one step ahead prediction in model choice and diagnostics and in rolling window computations is very common. Supplemental materials for the article are available online.},
author = {Nott, David J. and Tan, Siew Li and Villani, Mattias and Kohn, Robert},
doi = {10.1080/10618600.2012.679897},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nott et al. - 2012 - Regression Density Estimation With Variational Methods and Stochastic Approximation.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Bayesian model selection,heteroscedasticity,mixtures of experts,stochastic approximation,variational approximation},
number = {3},
pages = {797--820},
title = {{Regression Density Estimation With Variational Methods and Stochastic Approximation}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2012.679897},
volume = {21},
year = {2012}
}
@article{Tan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1205.3906v3},
author = {Tan, Linda S. L. and Nott, David J.},
doi = {10.1214/13-STS418},
eprint = {arXiv:1205.3906v3},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2013 - Variational Inference for Generalized Linear Mixed Models Using Partially Noncentered Parametrizations.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Variational Bayes,and phrases,hierarchical centering,longitudinal data analysis,nonconjugate models,tional message passing,varia-,variati,variational bayes},
number = {2},
pages = {168--188},
title = {{Variational Inference for Generalized Linear Mixed Models Using Partially Noncentered Parametrizations}},
url = {http://projecteuclid.org/euclid.ss/1369147910},
volume = {28},
year = {2013}
}
@article{Tan2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.4949v4},
author = {Tan, Linda S L and Nott, David J.},
doi = {10.1214/14-BA885},
eprint = {arXiv:1208.4949v4},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2014 - A stochastic variational framework for fitting and diagnosing generalized linear mixed models.pdf:pdf;:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2014 - A stochastic variational framework for fitting and diagnosing generalized linear mixed models(2).pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Conflict diagnostics,Hierarchical models,Identify divergent units,Nonconjugate variational message passing,Stochastic approximation,Variational bayes},
number = {4},
pages = {963--1004},
title = {{A stochastic variational framework for fitting and diagnosing generalized linear mixed models}},
volume = {9},
year = {2014}
}
@article{Tan2014,
abstract = {Mixtures of linear mixed models (MLMMs) are useful for clustering grouped data and can be estimated by likelihood maximization through the Expectation–Maximization algorithm. A suitable number of components is then determined conventionally by comparing different mixture models using penalized log-likelihood criteria such as Bayesian information criterion. We propose fitting MLMMs with variational methods, which can perform parameter estimation and model selection simultaneously. We describe a variational approximation for MLMMs where the variational lower bound is in closed form, allowing for fast evaluation and develop a novel variational greedy algorithm for model selection and learning of the mixture components. This approach handles algorithm initialization and returns a plausible number of mixture components automatically. In cases of weak identifiability of certain model parameters, we use hierarchical centering to reparameterize the model and show empirically that there is a gain in efficiency in ...},
archivePrefix = {arXiv},
arxivId = {1112.4675},
author = {Tan, Siew Li and Nott, David J.},
doi = {10.1080/10618600.2012.761138},
eprint = {1112.4675},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2014 - Variational Approximation for Mixtures of Linear Mixed Models.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Hierarchical centering,Mixture models,Model selection,Variational Bayes},
number = {January 2015},
pages = {564--585},
title = {{Variational Approximation for Mixtures of Linear Mixed Models}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/10618600.2012.761138{\#}.U5jAhZRdUu4},
volume = {23},
year = {2014}
}
@article{Tan2016,
abstract = {We consider the problem of learning a Gaussian variational approximation to the posterior distribution for a high-dimensional parameter, where we impose sparsity in the precision matrix to reflect appropriate conditional independence structure in the model. Incorporating sparsity in the precision matrix allows the Gaussian variational distribution to be both flexible and parsimonious, and the sparsity is achieved through parameterization in terms of the Cholesky factor. Efficient stochastic gradient methods which make appropriate use of gradient information for the target distribution are developed for the optimization. We consider alternative estimators of the stochastic gradients which have lower variation and are more stable. Our approach is illustrated using generalized linear mixed models and state space models for time series.},
archivePrefix = {arXiv},
arxivId = {1605.05622},
author = {Tan, Linda S. L. and Nott, David J.},
eprint = {1605.05622},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2016 - Gaussian variational approximation with sparse precision matrix.pdf:pdf},
keywords = {gaussian variational approximation,matrix,sparse precision,stochastic gradient algorithms,variational bayes},
number = {2014},
pages = {16},
title = {{Gaussian variational approximation with sparse precision matrix}},
url = {http://arxiv.org/abs/1605.05622},
year = {2016}
}
@article{Rue2016,
abstract = {The key operation in Bayesian inference, is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre- Simon Laplace (1774). This simple idea approximates the integrand with a second order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of Integrated Nested Laplace Approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model-abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we will discuss the reasons for the success of the INLA-approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute and why LGMs make such a useful concept for Bayesian computing.},
archivePrefix = {arXiv},
arxivId = {1604.00860},
author = {Rue, H{\aa}vard and Riebler, Andrea and S{\o}rbye, Sigrunn H and Illian, Janine B and Simpson, Daniel P and Lindgren, Finn K},
eprint = {1604.00860},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rue et al. - 2016 - Bayesian Computing with INLA A Review.pdf:pdf},
keywords = {approximate bayesian,gaussian markov random fields,inference,laplace approximations,latent gaussian models,numerical integration,sparse matrices},
pages = {1--26},
title = {{Bayesian Computing with INLA: A Review}},
url = {http://arxiv.org/abs/1604.00860},
year = {2016}
}
@book{James:2014:ISL:2517747,
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
isbn = {1461471370, 9781461471370},
publisher = {Springer Publishing Company, Incorporated},
title = {{An Introduction to Statistical Learning: With Applications in R}},
year = {2014}
}
@article{You,
author = {You, Chong and Ormerod, John T},
file = {:home/markg/Dropbox/Downloads/linearmodelv7.pdf:pdf},
keywords = {akaike information criterion,bayesian information criterion,consistency,deviance information criterion,markov chain monte carlo},
number = {2},
title = {{On Variational Bayes Estimation and Variational Bayes Information Criteria for Linear Regression Models}},
volume = {61}
}
@article{Wilson2015,
abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost {\$}O(n){\$} for {\$}n{\$} training points, and predictions cost {\$}O(1){\$} per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
archivePrefix = {arXiv},
arxivId = {1511.02222},
author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1511.02222},
file = {:home/markg/Dropbox/Downloads/1611.00336.pdf:pdf},
journal = {Artificial Intelligence and Statistics (AISTATS)},
number = {Nips},
pages = {1--19},
title = {{Deep Kernel Learning}},
url = {http://arxiv.org/abs/1511.02222},
volume = {51},
year = {2015}
}
@article{Santis1999,
abstract = {In the Bayesian approach to model selection and hypothesis testing, the Bayes factor plays a central role. However, the Bayes factor is very sensitive to prior distributions of parameters. This is a problem especially in the presence of weak prior information on the parameters of the models. The most radical consequence of this fact is that the Bayes factor is undetermined when improper priors are used. Nonetheless, extending the non-informative approach of Bayesian analysis to model selection/testing procedures is important both from a theoretical and an applied viewpoint. The need to develop automatic and robust methods for model comparison has led to the introduction of several alternative Bayes factors. In this paper we review one of these methods: the fractional Bayes factor (O'Hagan, 1995). We discuss general properties of the method, such as consistency and coherence. Furthermore, in addition to the original, essentially asymptotic justifications of the fractional Bayes factor, we provide further finite-sample motivations for its use. Connections and comparisons to other automatic methods are discussed and several issues of robustness with respect to priors and data are considered. Finally, we focus on some open problems in the fractional Bayes factor approach, and outline some possible answers and directions for future research. /// Dans l'approche Bayesienne relative {\`{a}} la s{\'{e}}lection d'un model et {\`{a}} la v{\'{e}}rification d'une hypoth{\`{e}}se, le facteur de Bayes joue une r{\^{o}}le fondamental. Toutefois le facteur de Bayes est tr{\`{e}}s sensible aux distributions {\`{a}} priori des param{\`{e}}tres. Ceci constitue un probl{\`{e}}me surtout en pr{\'{e}}sence d'une faible information {\`{a}} priori en ce qui concerne les param{\`{e}}tres des models. La cons{\'{e}}quence la plus radical de ce fait est que le facteur de Bayes est undetermin{\'{e}} quand les distributions {\`{a}} priori non informatives sont utilis{\'{e}}es. Cepandant, il est important d'{\'{e}}largir l'approche non informative de l'analyse Bayesienne {\`{a}} l'effet soit de d{\'{e}}terminer la s{\'{e}}lection d'un model que de v{\'{e}}rifier une hypoth{\`{e}}se. La necessit{\'{e}} de d{\'{e}}velopper des m{\'{e}}thodes automatiques et robustes pour la comparaison des models, a amen{\'{e}} {\`{a}} l'introduction des plusieurs facteurs de Bayes alternatifs. Cette {\'{e}}tude prend en consideration les resultats principaux relatifs {\`{a}} une de ces methodes, {\`{a}} savoir le facteur de Bayes fractionnaire. Nous analysons les caracteristique g{\'{e}}n{\'{e}}rales de cette methode telles que sa consistance et sa coh{\'{e}}rence. De plus en sus des justifications asyntotiques donn{\'{e}}es {\`{a}} l'origine au facteur fractionnaire de Bayes nous apportons d'autres raisons qui demontrent le bien fond{\'{e}} de son utilisation dans le domaine d'un {\'{e}}chantillonage fini. Nous prenons aussi en consideration par comparaison d'autres methodes automatiques et nous examinons d'autres caracteristiques telles que la robustesse par rapport aux les distributions {\`{a}} priori et aux donn{\'{e}}es. En conclusion, nous attirons l'attention sur certains probl{\`{e}}mes non encore resolus et proposons des solutions qui peuvent {\`{e}}tre explor{\'{e}}es d'avantage.},
author = {Santis, Fulvio De and Spezzaferri, Fulvio},
doi = {10.1055/s-0031-1292046},
file = {:home/markg/Downloads/Santis{\_}et{\_}al-1999-International{\_}Statistical{\_}Review.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
keywords = {bayes factors,bayesian inference,fractional bayes factor,model comparison},
number = {3},
pages = {267--286},
pmid = {22048938},
title = {{Methods for Default and Robust Bayesian Model Comparison: The Fractional Bayes Factor Approach}},
url = {http://www.jstor.org/stable/1403706},
volume = {67},
year = {1999}
}
@article{Bayarri2008,
author = {Bayarri, MJ and Garc{\'{i}}a-Donato, G},
file = {:home/markg/Dropbox/Downloads/06-23.pdf:pdf},
journal = {Journal of the Royal Statistical Society, series B},
number = {70},
pages = {981--1003},
title = {{Divergence Based Priors for Bayesian Hypothesis testing}},
url = {ftp://stat.duke.edu/pub/WorkingPapers/06-23.pdf},
volume = {1},
year = {2008}
}
@article{Maruyama2005,
abstract = {Let y=A$\backslash$beta+$\backslash$epsilon, where y is an N$\backslash$times1 vector of observations, $\backslash$beta is a p$\backslash$times1 vector of unknown regression coefficients, A is an N$\backslash$times p design matrix and $\backslash$epsilon is a spherically symmetric error term with unknown scale parameter $\backslash$sigma. We consider estimation of $\backslash$beta under general quadratic loss functions, and, in particular, extend the work of Strawderman [J. Amer. Statist. Assoc. 73 (1978) 623-627] and Casella [Ann. Statist. 8 (1980) 1036-1056, J. Amer. Statist. Assoc. 80 (1985) 753-758] by finding adaptive minimax estimators (which are, under the normality assumption, also generalized Bayes) of $\backslash$beta, which have greater numerical stability (i.e., smaller condition number) than the usual least squares estimator. In particular, we give a subclass of such estimators which, surprisingly, has a very simple form. We also show that under certain conditions the generalized Bayes minimax estimators in the normal case are also generalized Bayes and minimax in the general case of spherically symmetric errors.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508282v1},
author = {Maruyama, Yuzo and Strawderman, William E.},
doi = {10.1214/009053605000000327},
eprint = {0508282v1},
file = {:home/markg/Dropbox/Downloads/euclid.aos.1123250228.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Condition number,Generalized bayes,Minimaxity,Ridge regression},
number = {4},
pages = {1753--1770},
primaryClass = {arXiv:math},
title = {{A new class of generalized Bayes minimax ridge regression estimators}},
volume = {33},
year = {2005}
}
@article{Cui2008,
abstract = {For the problem of variable selection for the normal linear model, fixed penalty selection criteria such as AIC, Cp, BIC and RIC correspond to the posterior modes of a hierarchical Bayes model for various fixed hyperparameter settings. Adaptive selection criteria obtained by empirical Bayes estimation of the hyperparameters have been shown by George and Foster [2000. Calibration and Empirical Bayes variable selection. Biometrika 87(4), 731-747] to improve on these fixed selection criteria. In this paper, we study the potential of alternative fully Bayes methods, which instead margin out the hyperparameters with respect to prior distributions. Several structured prior formulations are considered for which fully Bayes selection and estimation methods are obtained. Analytical and simulation comparisons with empirical Bayes counterparts are studied. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Cui, Wen and George, Edward I.},
doi = {10.1016/j.jspi.2007.02.011},
file = {:home/markg/Dropbox/Downloads/CG JSPI 2008.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Hyperparameter uncertainty,Model selection,Objective Bayes},
number = {4},
pages = {888--900},
title = {{Empirical Bayes vs. fully Bayes variable selection}},
volume = {138},
year = {2008}
}
@misc{Kass1995,
abstract = {To compute a Bayes factor for testing H0: $\psi$ = $\psi$0 in the presence of a nuisance parameter $\beta$, priors under the null and alternative hypotheses must be chosen. As in Bayesian estimation, an important problem has been to define automatic, or "reference," methods for determining priors based only on the structure of the model. In this article we apply the heuristic device of taking the amount of information in the prior on $\psi$ equal to the amount of information in a single observation. Then, after transforming $\beta$ to be "null orthogonal" to $\psi$, we take the marginal priors on $\beta$ to be equal under the null and alternative hypotheses. Doing so, and taking the prior on $\psi$ to be Normal, we find that the log of the Bayes factor may be approximated by the Schwarz criterion with an error of order Op(n-1/2), rather than the usual error of order Op(1). This result suggests the Schwarz criterion should provide sensible approximate solutions to Bayesian testing problems, at least when the hypotheses are nested. When instead the prior on $\psi$ is elliptically Cauchy, a constant correction term must be added to the Schwarz criterion; the result then becomes a multidimensional generalization of Jeffreys's method.},
author = {Kass, Robert E and Wasserman, Larry},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2291327},
file = {:home/markg/Dropbox/Downloads/nested.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
keywords = {bayes information criterion,laplace,model selection,null-orthogonal parameters,orthogonal parameters,s method},
number = {431},
pages = {773--795},
pmid = {190},
title = {{A Reference Bayesian Test for Nested Hypotheses and its Relationship to the Schwarz Criterion}},
volume = {90},
year = {1995}
}
@article{Ibrahim2001,
abstract = {How to assess a Bayesian Model ?},
author = {Ibrahim, Joseph G and Chen, M--H. and Sinha, Debajyoti},
file = {:home/markg/Dropbox/Downloads/A11n23.pdf:pdf},
issn = {1017-0405},
journal = {Statistica Sinica},
keywords = {and phrases,calibration,distribution,model selection,predictive,predictive criterion,variable selection},
pages = {419--443},
title = {{Criterion--based models for {\{}B{\}}ayesian model assessment}},
volume = {11},
year = {2001}
}
@article{Berger2016,
author = {Berger, James O and Pericchi, Luis R and Journal, Source and Statistical, American and Mar, No and Berger, James and Pericchi, Luis R},
file = {:home/markg/Dropbox/Downloads/2291387.pdf:pdf},
keywords = {asymptotic bayes factors,hypothesis testing,noninformative prior,posterior probability,training sample},
number = {433},
pages = {109--122},
title = {{The Intrinsic Bayes Factor for Model Selection and Prediction Stable URL : http://www.jstor.org/stable/2291387 Linked references are available on JSTOR for this article : The Intrinsic Bayes Factor for Model Selection and Prediction}},
volume = {91},
year = {2016}
}
@article{Zellner1986,
author = {Zellner, A.},
journal = {Studies in Bayesian Econometrics},
pages = {233--243},
title = {{On Assessing Prior Distributions and Bayesian Regression Analysis with g Prior Distributions}},
volume = {6},
year = {1986}
}
@article{Saul1995,
author = {Saul, Lawrence K and Jordan, Michael I},
file = {:home/markg/Downloads/1155-exploiting-tractable-substructures-in-intractable-networks.pdf:pdf},
title = {{Exploiting Tractable Substructures in Intractable Networks}},
year = {1995}
}
@article{Foster1994,
author = {Foster, Dean P. and George, Edward I.},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foster, George - 1994 - Risk{\_}Inflation.Pdf.pdf:pdf},
journal = {Annals of Statistics},
number = {4},
pages = {1947--1975},
title = {{The Risk Inflation Criterion for Multiple Regression}},
volume = {22},
year = {1994}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1989 - Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annal.pdf:pdf},
number = {4},
pages = {367--393},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}},
volume = {4},
year = {1989}
}
@article{Spiegelhalter2016,
author = {Spiegelhalter, David J and Carlin, Bradley P},
file = {:home/markg/Downloads/3088806.pdf:pdf},
number = {4},
pages = {583--639},
title = {{Bayesian Measures of Model Complexity and Fit Author ( s ): David J . Spiegelhalter , Nicola G . Best , Bradley P . Carlin and Angelika van der Linde Source : Journal of the Royal Statistical Society . Series B ( Statistical Methodology ), Vol . 64 , Publ}},
volume = {64},
year = {2016}
}
@article{Society2016,
author = {Jeffreys, Harold},
file = {:home/markg/Downloads/97883.pdf:pdf},
number = {1670},
pages = {397--417},
title = {{Spectroscopic Mode Gr{\"{u}}neisen Parameters for Diamond Author ( s ): B . J . Parsons Source : Proceedings of the Royal Society of London . Series A , Mathematical and Physical Published by : Royal Society Stable URL : http://www.jstor.org/stable/79223}},
volume = {352},
year = {1946}
}
@article{Amari1998b,
author = {Amari, Shun-ichi and Amari, Shun-ichi},
file = {:home/markg/Downloads/Amari1998a.pdf:pdf},
journal = {Neural Computation},
pages = {251--276},
title = {{Natural Gradient Works Ef ciently in Learning}},
volume = {276},
year = {1998}
}
@article{Tran2015,
abstract = {Variational Bayes (VB) is rapidly becoming a popular tool for Bayesian inference in statistical modeling. However, the existing VB algorithms are restricted to cases where the likelihood is tractable, which precludes the use of VB in many interesting models such as in state space models and in approximate Bayesian computation (ABC), where application of VB methods was previously impossible. This paper extends the scope of application of VB to cases where the likelihood is intractable, but can be estimated unbiasedly. The proposed VB method therefore makes it possible to carry out Bayesian inference in many statistical models, including state space models and ABC. The method is generic in the sense that it can be applied to almost all statistical models without requiring a model-based derivation, which is a drawback of many existing VB algorithms. We also show how the proposed method can be used to obtain highly accurate VB approximations of marginal posterior distributions.},
archivePrefix = {arXiv},
arxivId = {1503.08621},
author = {Tran, Minh-Ngoc and Nott, David J. and Kohn, Robert},
eprint = {1503.08621},
file = {:home/markg/Downloads/1503.08621.pdf:pdf},
keywords = {approximate bayesian computation,marginal likelihood,natural gradient,state space models,stochastic optimization},
pages = {28},
title = {{Variational Bayes with Intractable Likelihood}},
url = {http://arxiv.org/abs/1503.08621},
volume = {117546},
year = {2015}
}
@article{Zellner1980,
author = {Zellner, A and Siow, A},
file = {:home/markg/Dropbox/Downloads/art{\%}3A10.1007{\%}2FBF02888369.pdf:pdf},
journal = {Bayesian Statistics},
keywords = {bayesian odds ratios,hypothesis testing,regression hypotheses},
number = {1978},
pages = {585--648},
title = {{Posterior odds ratio for selected regression hypothesis}},
year = {1980}
}
@article{Bayarri2012,
abstract = {In objective Bayesian model selection, no single criterion has emerged as dominant in defining objective prior distributions. In- deed, many criteria have been separately proposed and utilized to propose differing prior choices. We first formalize the most general and compelling of the various criteria that have been suggested, to- gether with a new criterion. We then illustrate the potential of these criteria in determining objective model selection priors by consider- ing their application to the problem of variable selection in normal linear models. This results in a new model selection objective prior with a number of compelling properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1209.5240v1},
author = {Bayarri, M. J. and Berger, J. O. and Forte, A. and Garc??a-Donato, G.},
doi = {10.1214/12-AOS1013},
eprint = {arXiv:1209.5240v1},
file = {:home/markg/Dropbox/1209.5240.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Model selection,Objective Bayes,Variable selection},
number = {3},
pages = {1550--1577},
title = {{Criteria for Bayesian model choice with application to variable selection}},
volume = {40},
year = {2012}
}
@article{Jang2006,
abstract = {The penalized quasi-likelihood (PQL) approach is the most common estimation procedure for the generalized linear mixed model (GLMM). However, it has been noticed that the PQL tends to underestimate variance components as well as regression coefficients in the previous literature. In this paper, we numerically show that the biases of variance component estimates by PQL are systematically related to the biases of regression coefficient estimates by PQL, and also show that the biases of variance component estimates by PQL increase as random effects become more heterogeneous.},
author = {Jang, W. and Lim, J.},
file = {:home/markg/Dropbox/Downloads/05-21.pdf:pdf},
keywords = {and phrases,generalized linear mixed models,heterogeneity,penalized quasi-likelihood estimator,variance components},
pages = {05--21},
title = {{PQL Estimation Biases in Generalized Linear Mixed Models}},
year = {2006}
}
@misc{Tibshirani2001,
abstract = {We propose a method (the ‘gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
author = {Tibshirani, R and Walther, G and Hastie, T},
booktitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
doi = {10.1111/1467-9868.00293},
file = {:home/markg/Dropbox/Downloads/gap.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
pages = {411--423},
pmid = {306526},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00293/abstract},
volume = {63},
year = {2001}
}
@article{Rockova2013,
abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose {\{}EMVS{\}}, a deterministic alternative to stochastic search based on an {\{}EM{\}} algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posterior model probabilities, {\{}EMVS{\}} rapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the {\{}EMVS{\}} framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multi-modality associated with variable selection priors. The usefulness the {\{}EMVS{\}} approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
author = {Ro{\v{c}}kov{\'{a}}, V and George, Ei},
doi = {10.1080/01621459.2013.869223},
file = {:home/markg/Dropbox/Downloads/EMVS The EM Approach to Bayesian Variable Selection.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {dynamic posterior exploration,high dimensionality,regularization plots,sparsity,ssvs},
number = {December},
pages = {37--41},
title = {{EMVS: The EM Approach to Bayesian Variable Selection}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.869223},
volume = {1459},
year = {2013}
}
@article{Wand2011,
abstract = {We develop strategies for mean field variational Bayes approximate inference for Bayesian hierarchical models containing elaborate distributions. We loosely define elaborate distributions to be those having more complicated forms compared with common distributions such as those in the Normal and Gamma families. Examples are Asymmetric Laplace, Skew Normal and Generalized Ex- treme Value distributions. Such models suffer from the difficulty that the parameter updates do not admit closed form solutions. We circumvent this problem through a combination of (a) specially tailored auxiliary variables, (b) univariate quadrature schemes and (c) finite mixture approximations of troublesome density functions. An accuracy assessment is conducted and the new methodology is illustrated in an application.},
author = {Wand, Matthew P. and Ormerod, John T. and Padoan, Simone A. and Fr??hrwirth, Rudolf},
doi = {10.1214/11-BA631},
file = {:home/markg/Dropbox/Downloads/euclid.ba.1339616546.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Auxiliary mixture sampling,Bayesian inference,Quadrature,Variational methods},
number = {4},
pages = {847--900},
title = {{Mean field Variational Bayes for elaborate distributions}},
volume = {6},
year = {2011}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:home/markg/Dropbox/Downloads/euclid.aos.1176344136.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the dimension of a model}},
volume = {6},
year = {1978}
}
@book{Gradshteyn1988,
abstract = {Laplace's Integral},
archivePrefix = {arXiv},
arxivId = {978-0-12-373637-6},
author = {Gradshteyn, I. S.},
booktitle = {American Journal of Physics},
doi = {10.1119/1.15756},
eprint = {978-0-12-373637-6},
file = {:home/markg/Dropbox/Downloads/Table{\_}of{\_}Integrals{\_}Series{\_}and{\_}Products{\_}Tablicy{\_}Integralov{\_}Summ{\_}Rjadov{\_}I{\_}Proizvedennij{\_}Engl.{\_}2.pdf:pdf},
isbn = {0122947576},
issn = {00029505},
number = {10},
pages = {958},
pmid = {16328207},
title = {{Tables of Integrals, Series, and Products}},
volume = {56},
year = {1988}
}
@article{Tierney1989,
abstract = {Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions LUKE TIERNEY, ROBERT E. KASS, and JOSEPH B. KADANE* Tierney and Kadane (1986) presented a simple second-order approximation for posterior expectations of positive functions. They used Laplace's method for asymptotic evaluation of integrals, in which the integrand is written as J(8)exp( -nh(8)) and the function h is approximated by a quadratic. The form in which they applied Laplace's method, however, was fully exponential: The integrand was written instead as exp[ -nh(8) + log J(8)]; this allowed first-order approximations to be used in the numerator and denominator of a ratio of integrals to produce a second-order expansion for the ratio. Other second-order expansions (Hartigan 1965; Johnson 1970; Lindley 1961, 1980; Mosteller and Wallace 1964) require computation of more derivatives of the log-likelihood function. In this article we extend the fully exponential method to apply to expectations and variances of nonpositive functions. To obtain a second-order approximation to an expectation E(g(8)), we use the fully exponential method to approximate the moment-generating function E(exp(sg(8))), whose integrand is positive, and then differentiate the result. This method is formally equivalent to that of Lindley and that of Mosteller and Wallace, yet does not require third derivatives of the likelihood function. It is also equivalent to another alternative approach to the approximation of E(g(O)): We may add a large constant c to g(8), apply the fully exponential method to E(c + g(8)), and subtract c; on passing to the limit as c tends to infinity we regain the approximation based on the moment-generating function. Furthermore, the second derivative of the logarithm of the approximation E(exp(sg(8))), which is an approximate cumulant-generating function, yields a simple second-order approximation to the variance. In deriving these results we omit rigorous justification of formal manipulations, which may be found in Kass, Tierney, and Kadane (in press). Although our point of view is Bayesian, our results have applications to non-Bayesian inference as well (DiCiccio 1986).},
author = {Tierney, Luke and Kass, Robert E. and Kadane, Joseph B.},
doi = {10.1080/01621459.1989.10478824},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Kass, Kadane - 1989 - Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {407},
pages = {710--716},
title = {{Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478824},
volume = {84},
year = {1989}
}
@article{Tierney1989a,
author = {Tierney, Luke and Kass, Robert E. and Kadane, Joseph B.},
doi = {10.1093/biomet/76.3.425},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Kass, Kadane - 1989 - Approximate marginal densities of nonlinear functions.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Asymptotic normality,Laplace's method,Saddlepoint method},
number = {3},
pages = {425--433},
title = {{Approximate marginal densities of nonlinear functions}},
volume = {76},
year = {1989}
}
@article{Breiman1996,
abstract = {In model selection, usually a "best" predictor is chosen from a collection {\{}$\mu$̂({\textperiodcentered}, s){\}} of predictors where $\mu$̂({\textperiodcentered}, s) is the minimum least-squares predictor in a collection Us of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in Us. If L is the data used to derive the sequence {\{}$\mu$̂({\textperiodcentered}, s){\}}, the procedure is called unstable if a small change in L can cause large changes in {\{}$\mu$̂({\textperiodcentered}, s){\}}. With a crystal ball, one could pick the predictor in {\{}$\mu$̂({\textperiodcentered}, s){\}} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\{}$\mu$̂'({\textperiodcentered}, s){\}} and then averaging over many such predictor sequences.},
author = {Breiman, Leo},
doi = {10.1214/aos/1032181158},
file = {:home/markg/Dropbox/Downloads/euclid.aos.1032181158.pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Regression,cross-validation,prediction error,predictive loss,subset selection},
number = {6},
pages = {2350--2383},
title = {{Heuristics of instability in model selection}},
volume = {24},
year = {1996}
}
@article{Piironen2016,
abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. Better and much less varying results are obtained by incorporating all the uncertainties into a full encompassing model and projecting this information onto the submodels. The reference model projection appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08650v1},
author = {Piironen, Juho and Vehtari, Aki},
doi = {10.1007/s11222-016-9649-y},
eprint = {arXiv:1503.08650v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piironen, Vehtari - 2016 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian model selection,Cross-validation,Projection,Reference model,Selection bias},
pages = {1--25},
title = {{Comparison of Bayesian predictive methods for model selection}},
year = {2016}
}
@article{George2000,
abstract = {For the problem of variable selection for the normal linear model, selection criteria such as AIC, Cp, BIC and RIC have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed-penalty criterion across a variety of orthogonal and nonorthogonal set-ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed.},
author = {George, Edward I and Foster, Dean P},
doi = {10.1093/biomet/87.4.731},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Foster - 2000 - Calibration and Empirical Bayes Variable Selection.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {731--747},
title = {{Calibration and Empirical Bayes Variable Selection}},
volume = {87},
year = {2000}
}
@article{Bhadra2016,
abstract = {Predictive performance in shrinkage regression suffers from two major difficulties: (i) the amount of relative shrinkage is monotone in the singular values of the design matrix and (ii) the amount of shrinkage does not depend on the response variables. Both of these factors can translate to a poor prediction performance, the risk of which can be explicitly quantified using Stein's unbiased risk estimate. We show that using a component-specific local shrinkage term that can be learned from the data under a suitable heavy-tailed prior, in combination with a global term providing shrinkage towards zero, can alleviate both these difficulties and consequently, can result in an improved risk for prediction. Demonstration of improved prediction performance over competing approaches in a simulation study and in a pharmacogenomics data set confirms the theoretical findings.},
archivePrefix = {arXiv},
arxivId = {1605.04796},
author = {Bhadra, Anindya and Datta, Jyotishka and Li, Yunfan and Polson, Nicholas G. and Willard, Brandon},
eprint = {1605.04796},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhadra et al. - 2016 - Prediction risk for global-local shrinkage regression.pdf:pdf},
number = {May},
title = {{Prediction risk for global-local shrinkage regression}},
url = {http://arxiv.org/abs/1605.04796},
year = {2016}
}
@article{Meinshausen2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0809.2932v2},
author = {Meinshausen, Nicolai},
eprint = {arXiv:0809.2932v2},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meinshausen - 2009 - Stability selection (Slides).pdf:pdf},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
pages = {1--30},
title = {{Stability selection (Slides)}},
year = {2009}
}
@article{DeLeeuw1992,
abstract = {The problem of estimating the dimensionality of a model occurs in various forms in applied statistics: estimating the number of factors in factor analysis, estimating the degree of a polynomial describing the data, selecting the variables to be introduced in a multiple regression equation, estimating the order of an AR or MA time series model, and so on.},
author = {DeLeeuw, J.},
doi = {10.1016/0049-3848(86)90167-2},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeLeeuw - 1992 - Introduction to Akaike (1973) Information Theory and an Extension of the Maximum Likelihood Principle.pdf:pdf},
isbn = {0172-7397},
issn = {00493848},
journal = {Breakthroughs in Statistics Volume I: Foundations and Basic Theory},
number = {May},
pages = {599--609},
pmid = {217},
title = {{Introduction to Akaike (1973) Information Theory and an Extension of the Maximum Likelihood Principle}},
year = {1992}
}
@article{Summation1993,
author = {Summation, Point},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Summation - 1993 - The accuracy.pdf:pdf},
keywords = {1,ams subject classifications,and all,floating point summation,inner products,introduction,means,norms,numbers are ubiquitous in,orderings,primary 65g05,puting,rounding error analysis,scientific com-,secondary 65b10,sums of floating point,they occur when evaluating,variances},
number = {4},
pages = {783--799},
title = {{The accuracy}},
volume = {14},
year = {1993}
}
@article{Miller1984,
abstract = {Computational algorithms for selecting subsets of regression variables are discussed. Only linear models and the least-squares criterion are considered. The use of planar- rotation algorithms, instead of Gauss-Jordan methods, is advocated. The advantages and disadvantages of a number of "cheap" search methods are described for use when it is not feasible to carry out an exhaustive search for the best-fitting subsets. Hypothesis testing for three purposes is considered, namely (i) testing for zero regression coefficients for remaining variables, (ii) comparing subsets and (iii) testing for any predictive value in a selected subset. Three small data sets are used to illustrate these tests. Spjotvoll's (1972a) test is discussed in detail, though an extension to this test appears desirable. Estimation problems have largely been overlooked in the past. Three types of bias are identified, namely that due to the omission of variables, that due to competition for selection and that due to the stopping rule. The emphasis here is on competition bias, which can be of the order of two or more standard errors when coefficients are estimated from the same data as were used to select the subset. Five possible ways of handling this bias are listed. This is the area most urgently requiring further research. Mean squared errors of prediction and stopping rules are briefly discussed. Com- petition bias invalidates the use of existing stopping rules as they are commonly applied to try to produce optimal prediction equations},
author = {Miller, By Alan J},
doi = {10.2307/2981576},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1984 - Selection of Subsets of Regression Variables(2).pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
keywords = {CONDITIONAL LIKELIHOOD,CRITERIA,LEAST SQUARES,MALLOWS' C AKAIKE'S INFORMATION,MEAN SQUARED ERRORS OF PREDICTION,MULTIPLE REGRESSION,PREDICTION,STEPWISE REGRESSION,SUBSET SELECTION,VARIABLE SELECTION},
number = {3},
pages = {389--425},
title = {{Selection of Subsets of Regression Variables}},
volume = {147},
year = {1984}
}
@article{??zaltin2011,
abstract = {The most widely used progress measure for branch-and-bound (B{\&}B) algorithms when solving mixed-integer programs (MIPs) is the MIP gap. We introduce a new progress measure that is often much smoother than the MIP gap. We propose a double exponential smoothing technique to predict the solution time of B{\&}B algorithms and evaluate the prediction method using three MIP solvers. Our computational experiments show that accurate predictions of the solution time are possible, even in the early stages of B{\&}B algorithms.},
author = {??zaltin, Osman Y. and Hunsaker, Brady and Schaefer, Andrew J.},
doi = {10.1287/ijoc.1100.0405},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/zaltin, Hunsaker, Schaefer - 2011 - Predicting the solution time of branch-and-bound algorithms for mixed-integer programs.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {INFORMS Journal on Computing},
keywords = {Branch-and-bound algorithm,Mixed-integer programming,Solution time prediction},
number = {3},
pages = {392--403},
title = {{Predicting the solution time of branch-and-bound algorithms for mixed-integer programs}},
volume = {23},
year = {2011}
}
@article{Bertsimas2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.03133v1},
author = {Bertsimas, Dimitris and King, Angela},
doi = {10.1214/15-AOS1388},
eprint = {arXiv:1507.03133v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsimas, King - 2014 - Best Subset Selection via a Modern Optimization Lens.pdf:pdf},
isbn = {0001415123},
pages = {1--63},
title = {{Best Subset Selection via a Modern Optimization Lens}},
year = {2014}
}
@article{Pearson2014,
abstract = {The two most commonly used hypergeometric functions are the confluent hypergeometric function and the Gauss hypergeometric function. We review the available techniques for accurate, fast, and reliable computation of these two hypergeometric functions in different parameter and variable regimes. The methods that we investigate include Taylor and asymptotic series computations, Gauss-Jacobi quadrature, numerical solution of differential equations, recurrence relations, and others. We discuss the results of numerical experiments used to determine the best methods, in practice, for each parameter and variable regime considered. We provide 'roadmaps' with our recommendation for which methods should be used in each situation.},
archivePrefix = {arXiv},
arxivId = {1407.7786},
author = {Pearson, John W. and Olver, Sheehan and Porter, Mason a.},
eprint = {1407.7786},
file = {:home/markg/Downloads/1407.7786v2.pdf:pdf},
keywords = {1,33c05,33c15,41a58,41a60,ams subject classifications,computation of special functions,confluent hypergeometric function,gauss hy-,introduction,methods for computing the,paper is to review,pergeometric function,primary,secondary,the aim of this},
pages = {41},
title = {{Numerical Methods for the Computation of the Confluent and Gauss Hypergeometric Functions}},
url = {http://arxiv.org/abs/1407.7786},
year = {2014}
}
@article{Masada2009,
abstract = {In this paper, we propose an acceleration of collapsed variational Bayesian (CVB) inference for latent Dirichlet allocation (LDA) by using Nvidia CUDA compatible devices. While LDA is an efficient Bayesian multi-topic document model, it requires complicated computations for parameter estimation in comparison with other simpler document models, e.g. probabilistic latent semantic indexing, etc. Therefore, we accelerate CVB inference, an efficient deterministic inference method for LDA, with Nvidia CUDA. In the evaluation experiments, we used a set of 50,000 documents and a set of 10,000 images. We could obtain inference results comparable to sequential CVB inference.},
author = {Masada, Tomonari and Hamada, Tsuyoshi and Shibata, Yuichiro and Oguri, Kiyoshi},
doi = {10.1007/978-3-642-02568-6_50},
file = {:home/markg/Downloads/CVM{\_}CUDA.pdf:pdf},
isbn = {3642025676},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {491--500},
title = {{Accelerating Collapsed Variational Bayesian inference for latent dirichlet allocation with nvidia CUDA compatible devices}},
volume = {5579 LNAI},
year = {2009}
}
@article{Wilson2010,
author = {Wilson, Melanie A},
file = {:home/markg/Downloads/melaniew.pdf:pdf},
journal = {Analysis},
keywords = {GWAS},
pages = {1--99},
title = {{Bayesian model uncertainty and prior choice with applications to genetic association studies}},
year = {2010}
}
@article{Million2007,
author = {Million, Elizabeth and Million, Elizabeth},
file = {:home/markg/Downloads/million-paper.pdf:pdf},
pages = {1--7},
title = {{The Hadamard Product}},
year = {2007}
}
@misc{Mainland2013,
abstract = {Stream fusion [6] is a powerful technique for automatically trans- forming high-level sequence-processing functions into efficient im- plementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all. In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together mul- tiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are im- plemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C. Categories},
author = {Mainland, Geoffrey and Leshchinskiy, Roman and {Peyton Jones}, Simon},
booktitle = {the 18th ACM SIGPLAN international conference},
doi = {10.1145/2500365.2500601},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mainland, Leshchinskiy, Peyton Jones - 2013 - Exploiting Vector Instructions with Generalized Stream Fusion.pdf:pdf},
isbn = {9781450323260},
issn = {15232867},
title = {{Exploiting Vector Instructions with Generalized Stream Fusion}},
year = {2013}
}
@article{Clyde2012,
abstract = {Monte Carlo algorithms are commonly used to identify a set of models for Bayesian model selection or model averaging. Because empirical frequencies of models are often zero or one in high-dimensional problems, posterior probabilities calculated from the observed marginal likelihoods, renormalized over the sampled models, are often employed. Such estimates are the only recourse in several newer stochastic search algorithms. In this paper, we prove that renormalization of posterior probabilities over the set of sampled models generally leads to bias that may dominate mean squared error. Viewing the model space as a finite population, we propose a new estimator based on a ratio of Horvitz–Thompson estimators that incorporates observed marginal likelihoods, but is approximately unbiased. This is shown to lead to a reduction in mean squared error compared to the empirical or renormalized estimators, with little increase in computational cost.},
author = {Clyde, Merlise A. and Ghosh, Joyee},
doi = {10.1093/biomet/ass040},
file = {:home/markg/Downloads/10-11.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Bayesian model averaging,Horvitz-Thompson estimator,Inclusion probability,Markov chain Monte Carlo,Median probability model,Model uncertainty,Variable selection},
number = {4},
pages = {981--988},
title = {{Finite population estimators in stochastic search variable selection}},
volume = {99},
year = {2012}
}
@article{Wand2012,
abstract = {The ag{\'{e}}d number theoretic concept of continued fractions can enhance certain Bayesian computations. The crux of this claim is due to continued fraction representations of numerically challenging special function ratios that arise in Bayesian computing. Continued fraction approximation via Lentz's Algorithm often leads to efficient and stable computation of such quantities. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
author = {Wand, Matt P. and Ormerod, John T.},
doi = {10.1002/sta4.4},
file = {:home/markg/Downloads/Wand{\_}et{\_}al-2012-Stat.pdf:pdf},
issn = {20491573},
journal = {Stat},
keywords = {Hypergeometric functions,Mean field variational Bayes,Parabolic cylinder functions,Special functions,Variable selection,Variational approximations},
number = {1},
pages = {31--41},
title = {{Continued fraction enhancement of Bayesian computing}},
volume = {1},
year = {2012}
}
@article{Nadarajah2015,
author = {Nadarajah, Saralees},
doi = {10.1080/00031305.2015.1028595},
file = {:home/markg/Downloads/on{\_}the{\_}computation{\_}of{\_}gauss{\_}hypergeometric{\_}functions.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {2},
pages = {146--148},
title = {{On the Computation of Gauss Hypergeometric Functions}},
volume = {69},
year = {2015}
}
@article{Bleier2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0412v1},
author = {Bleier, Arnim},
eprint = {arXiv:1312.0412v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bleier - 2013 - Practical Collapsed Stochastic Variational Inference for the HDP.pdf:pdf},
journal = {Proceedings of the NIPS workshop on topic models},
pages = {arXiv:1312.0412 [cs.LG]},
title = {{Practical Collapsed Stochastic Variational Inference for the HDP}},
url = {http://arxiv.org/abs/1312.0412},
year = {2013}
}
@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George andMcCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa- rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for pos- terior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probabil- ity. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I and Mcculloch, Robert E},
doi = {10.1.1.211.4871},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Mcculloch - 1997 - Approaches for bayesian variable selection.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,cal models,conjugate prior,gibbs sampling,gray code,hierarchi-,markov chain monte carlo,metropolis-hastings algorithms,mixtures,normal,normalization constant,regression,simulation},
pages = {339--373},
title = {{Approaches for Bayesian variable selection}},
volume = {7},
year = {1997}
}
@article{Fouskakis2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.2442v1},
author = {Fouskakis, D and Ntzoufras, Ioannis and Draper, David},
doi = {10.1214/14-BA887},
eprint = {arXiv:1307.2442v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fouskakis, Ntzoufras, Draper - 2015 - Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {athens,athens 15780 greece,bayes factors,bayesian variable selection,d,department of mathematics,email fouskakis,expected-posterior priors,fouskakis is with the,gaussian linear,gr,math,models,national technical university of,ntua,power-prior,pus,training samples,unit-information prior,zografou cam-},
number = {1},
pages = {75--107},
title = {{Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models}},
volume = {10},
year = {2015}
}
@article{Li2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06913v1},
author = {Li, Yingbo and Clyde, Merlise A},
eprint = {arXiv:1503.06913v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Clyde - 2015 - Mixtures of g -priors in Generalized Linear Models.pdf:pdf},
journal = {arXiv},
title = {{Mixtures of g -priors in Generalized Linear Models}},
volume = {1503.06913},
year = {2015}
}
@article{Liang2008,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, F and Paulo, R and Molina, G and Clyde, M a and Berger, J O},
doi = {Doi 10.1198/016214507000001337},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2008 - Mixtures of g priors for Bayesian variable selection.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {aic,approximations,bayesian model averaging,bic,cauchy,criterion,empirical bayes,gaussian hypergeometric functions,linear-regression,matrix,model selection,multiple-regression,zellner-siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}
@article{OHara2009,
abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo {\&} Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Je{\AE}reys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their di{\AE}erent properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the di{\AE}erent methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
author = {O'Hara, R. B. and Sillanp????, M. J.},
doi = {10.1214/09-BA403},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hara, Sillanp - 2009 - A review of bayesian variable selection methods What, how and which.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {BUGS,MCMC,Variable selection},
number = {1},
pages = {85--118},
pmid = {273483200007},
title = {{A review of Bayesian variable selection methods: What, how and which}},
volume = {4},
year = {2009}
}
@article{Nan2014,
abstract = {Many exciting results have been obtained on model selection for high-dimensional data in both efficient algorithms and theoretical developments. The powerful penalized regression methods can give sparse representations of the data even when the number of predictors is much larger than the sample size. One important question then is: How do we know when a sparse pattern identified by such a method is reliable? In this work, besides investigating instability of model selection methods in terms of variable selection, we propose variable selection deviation measures that give one a proper sense on how many predictors in the selected set are likely trustworthy in certain aspects. Simulation and a real data example demonstrate the utility of these measures for application.},
author = {Nan, Ying and Yang, Yuhong},
doi = {10.1080/10618600.2013.829780},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nan, Yang - 2014 - Variable Selection Diagnostics Measures for High-Dimensional Regression.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Model selection diagnostics,Model selection instability,Variable selection deviation},
number = {3},
pages = {636--656},
title = {{Variable Selection Diagnostics Measures for High-Dimensional Regression}},
url = {http://www.tandfonline.com/doi/full/10.1080/10618600.2013.829780},
volume = {23},
year = {2014}
}
@article{Efron2013,
abstract = {Classical statistical theory ignores model selection in assessing estimation accuracy. Here we consider bootstrap methods for computing standard errors and confidence intervals that take model selection into account. The methodology involves bagging, also known as bootstrap smoothing, to tame the erratic discontinuities of selection-based estimators. A useful new formula for the accuracy of bagging then provides standard errors for the smoothed estimators. Two examples, nonparametric and parametric, are carried through in detail: a regression model where the choice of degree (linear, quadratic, cubic,. . . ) is determined by the Cp criterion, and a Lasso-based estimation problem.},
author = {Efron, Bradley},
doi = {10.1080/01621459.2013.823775},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 2013 - Estimation and Accuracy after Model Selection.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {abc intervals,bagging,bootstrap smoothing,c p,importance sam-,lasso,model averaging},
number = {October},
pages = {130725111823001},
pmid = {25346558},
title = {{Estimation and Accuracy after Model Selection}},
volume = {1459},
year = {2013}
}
@article{Gordy1998,
abstract = {This paper introduces the "compound confluent hypergeometric" (CCH) distribution. The CCH unifies and generalizes three recently introduced generalizations of the beta distributions: the Gauss hypergeometric (GH) distribution of Armero and Bayarri (1994), the generalized beta (GB) distribution of McDonald and Xu (1995), and the confluent hypergeometric (CH) distribution of Gordy (forthcoming). In addition to greater flexibility in fitting data, the CCH offers two useful properties. Unlike the beta, GB and GH, the CCH allows for conditioning on explanatory variables in a natural and convenient way. The CCH family is conjugate for gamma distribution signals, and so may also prove useful in Bayesian analysis. Application of the CCH is demonstrated with two measures of household liquid assets. In each case, the CCH yields a statistically significant improvement in fit over the more restrictive alternatives.},
author = {Gordy, Michael B},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordy - 1998 - A generalization of generalized beta distributions.pdf:pdf},
number = {202},
pages = {1--28},
title = {{A generalization of generalized beta distributions}},
year = {1998}
}
@article{Maruyama2011,
abstract = {For the normal linear model variable selection problem, we propose selection criteria based on a fully Bayes formulation with a generalization of Zellner's g-prior which allows for p {\textgreater} n. A special case of the prior formulation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential interest.},
archivePrefix = {arXiv},
arxivId = {0801.4410},
author = {Maruyama, Yuzo and George, Edward I.},
doi = {10.1214/11-AOS917},
eprint = {0801.4410},
file = {:home/markg/Downloads/euclid.aos.1324563354.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayes factor,Model selection consistency,Ridge regression,Singular value decomposition,Variable selection},
number = {5},
pages = {2740--2765},
title = {{Fully Bayes factors with a generalized g-prior}},
volume = {39},
year = {2011}
}
@article{Wang2015,
abstract = {Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.},
archivePrefix = {arXiv},
arxivId = {1506.02222},
author = {Wang, Xiangyu and Dunson, David and Leng, Chenlei},
eprint = {1506.02222},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Dunson, Leng - 2015 - No penalty no tears Least squares in high-dimensional linear models.pdf:pdf},
pages = {1--26},
title = {{No penalty no tears: Least squares in high-dimensional linear models}},
url = {http://arxiv.org/abs/1506.02222},
year = {2015}
}
@article{Wang2015a,
abstract = {Variable selection is a challenging issue in statistical applications when the number of predictors {\$}p{\$} far exceeds the number of observations {\$}n{\$}. In this ultra-high dimensional setting, the sure independence screening (SIS) procedure was introduced to significantly reduce the dimensionality by preserving the true model with overwhelming probability, before a refined second stage analysis. However, the aforementioned sure screening property strongly relies on the assumption that the important variables in the model have large marginal correlations with the response, which rarely holds in reality. To overcome this, we propose a novel and simple screening technique called the high-dimensional ordinary least-squares projection (HOLP). We show that HOLP possesses the sure screening property and gives consistent variable selection without the strong correlation assumption, and has a low computational complexity. A ridge type HOLP procedure is also discussed. Simulation study shows that HOLP performs competitively compared to many other marginal correlation based methods. An application to a mammalian eye disease data illustrates the attractiveness of HOLP.},
archivePrefix = {arXiv},
arxivId = {1506.01782},
author = {Wang, Xiangyu and Leng, Chenlei},
eprint = {1506.01782},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Leng - 2015 - High-dimensional Ordinary Least-squares Projection for Screening Variables.pdf:pdf},
keywords = {consistency,forward regression,generalized inverse,high dimensionality,lasso,marginal correlation,moore-penrose inverse,ordinary least squares,screening,sure independent,variable selection},
pages = {1--47},
title = {{High-dimensional Ordinary Least-squares Projection for Screening Variables}},
url = {http://arxiv.org/abs/1506.01782},
year = {2015}
}
@article{Eklund2007,
abstract = {Large scale Bayesian model averaging and variable selection exercises present, despite the great increase in desktop computing power, considerable computational challenges. Due to the large scale it is impossible to evaluate all possible models and estimates of posterior probabilities are instead obtained from stochastic (MCMC) schemes designed to converge on the posterior distribution over the model space. While this frees us from the requirement of evaluating all possible models the computational effort is still substantial and efficient implementation is vital. Efficient implementation is concerned with two issues: the efficiency of the MCMC algorithm itself and efficient computation of the quantities needed to obtain a draw from the MCMC algorithm. We evaluate several different MCMC algorithms and find that relatively simple algorithms with local moves perform competitively except possibly when the data is highly collinear. For the second aspect, efficient computation within the sampler, we focus on the important case of linear models where the computations essentially reduce to least squares calculations. Least squares solvers that update a previous model estimate are appealing when the MCMC algorithm makes local moves and we find that the Cholesky update is both fast and accurate.},
author = {Eklund, Jana and Karlsson, Sune},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eklund, Karlsson - 2007 - Computational Efficiency in Bayesian Model and Variable Selection.pdf:pdf},
keywords = {Bayesian Model Averaging,Cholesky decomposition,QR decomposition,Sweep operator,Swendsen-Wang algorithm},
number = {2007:4},
title = {{Computational Efficiency in Bayesian Model and Variable Selection}},
url = {http://econpapers.repec.org/RePEc:hhs:oruesi:2007{\_}004},
year = {2007}
}
@book{Boyd2010,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:home/markg/Downloads/bv{\_}cvxbook.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
volume = {25},
year = {2010}
}
@article{Zhu2004,
abstract = {Classification of patient samples is an important aspect of cancer diagnosis and treatment. The support vector machine (SVM) has been successfully applied to microarray cancer diagnosis problems. However, one weakness of the SVM is that given a tumor sample, it only predicts a cancer class label but does not provide any estimate of the underlying probability. We propose penalized logistic regression (PLR) as an alternative to the SVM for the microarray cancer diagnosis problem. We show that when using the same set of genes, PLR and the SVM perform similarly in cancer classification, but PLR has the advantage of additionally providing an estimate of the underlying probability. Often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classification, rather than class prediction. We consider two gene selection methods in this paper, univariate ranking (UR) and recursive feature elimination (RFE). Empirical results indicate that PLR combined with RFE tends to select fewer genes than other methods and also performs well in both cross-validation and test samples. A fast algorithm for solving PLR is also described.},
author = {Zhu, Ji and Hastie, Trevor},
doi = {10.1093/biostatistics/kxg046},
file = {:home/markg/Downloads/Zhu-Biostat04.pdf:pdf},
isbn = {1465-4644 (Print)$\backslash$r1465-4644 (Linking)},
issn = {14654644},
journal = {Biostatistics},
keywords = {Cancer diagnosis,Feature selection,Logistic regression,Microarray,Support vector machines},
number = {3},
pages = {427--443},
pmid = {15208204},
title = {{Classification of gene microarrays by penalized logistic regression}},
volume = {5},
year = {2004}
}
@article{Pregibon1981,
abstract = {A maximum likelihood fit of a logistic regression model (and other similar models) is extremely sensitive to outlying responses and extreme points in the design space. We develop diagnostic measures to aid the analyst in detecting such observations and in quantifying their effect on various aspects of the maximum likelihood fit. The elements of the fitting process which constitute the usual output (parameter estimates, standard errors, residuals, etc.) will be used for this purpose. With a properly designed computing package for fitting the usual maximum-likelihood model, the diagnostics are essentially "free for the asking." In particular, good data analysis for logistic regression models need not be expensive or time-consuming.},
author = {Pregibon, Daryl},
doi = {10.1214/aos/1176345513},
file = {:home/markg/Downloads/2240841.pdf:pdf},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {4},
pages = {705--724},
title = {{Logistic Regression Diagnostics}},
volume = {9},
year = {1981}
}
@article{Bursac2008,
author = {Bursac, Zoran and Gauss, C Heath and Williams, David Keith and Hosmer, David W},
doi = {10.1186/1751-0473-3-17},
file = {:home/markg/Downloads/1751-0473-3-17.pdf:pdf},
issn = {1751-0473},
journal = {Source Code for Biology and Medicine},
number = {1},
pages = {17},
title = {{Purposeful selection of variables in logistic regression}},
url = {http://www.scfbm.org/content/3/1/17},
volume = {3},
year = {2008}
}
@article{Zellner2004,
author = {Zellner, Dietmar and Zellner, Dietmar and Keller, Frieder and Keller, Frieder and Zellner, G{\"{u}}nter E. and Zellner, G{\"{u}}nter E.},
doi = {10.1081/SAC-200033363},
file = {:home/markg/Downloads/sac-200033363.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {3},
pages = {787--805},
title = {{Variable Selection in Logistic Regression Models}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1081/SAC-200033363{\&}magic=crossref{\%}7C{\%}7CD404A21C5BB053405B1A640AFFD44AE3},
volume = {33},
year = {2004}
}
@article{Gatu2006,
abstract = {An efficient branch-and-bound algorithm for computing the best-subset regression models is proposed. The algorithm avoids the computation of the whole regression tree that generates all possible subset models. It is formally shown that if the branch-and-bound test holds, then the current subtree together with its right-hand side subtrees are cut. This reduces significantly the computational burden of the proposed algorithm when compared to an existing leaps-and-bounds method which generates two trees. Specifically, the proposed algorithm, which is based on orthogonal transformations, outperforms by O(n 3) the leaps-and-bounds strategy. The criteria used in identifying the best subsets are based on monotone functions of the residual sum of squares (RSS) such as R2, adjusted R2, mean square error of prediction, and Cp. Strategies and heuristics that improve the computational performance of the proposed algorithm are investigated. A computationally efficient heuristic version of the branch-and-bound strategy which decides to cut subtrees using a tolerance parameter is proposed. The heuristic algorithm derives models close to the best ones. However, it is shown analytically that the relative error of the RSS, and consequently the corresponding statistic, of the computed subsets is smaller than the value of the tolerance parameter which lies between zero and one. Computational results and experiments on random and real data are presented and analyzed. {\textcopyright} 2006 American Statistical Association.},
author = {Gatu, Cristian and Kontoghiorghes, Erricos John},
doi = {10.1198/106186006X100290},
file = {:home/markg/Downloads/BBASubset.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {least squares,qr decomposition,subset regression},
number = {1},
pages = {139--156},
title = {{Branch-and-Bound Algorithms for Computing the Best-Subset Regression Models}},
volume = {15},
year = {2006}
}
@article{Clausen1999,
abstract = {A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimiza- tion problems, are easy to state, and have a finite but usually very large number of feasible solutions. While some of these as e.g. the Shortest Path problem and the Minimum Spanning Tree problem have polynomial algo- ritms, the majority of the problems in addition share the property that no polynomial method for their solution is known. Examples here are vehicle routing, crew scheduling, and production planning. All of these problems are NP-hard. Branch and Bound (B{\&}B) is by far the most widely used tool for solv- ing large scale NP-hard combinatorial optimization problems. B{\&}B is, however, an algorithm paradigm, which has to be filled out for each spe- cific problem type, and numerous choices for each of the components ex- ist. Even then, principles for the design of efficient B{\&}B algorithms have emerged over the years. In this paper I review the main principles of B{\&}B and illustrate the method and the different design issues through three examples: the Sym- metric Travelling Salesman Problem, the Graph Partitioning problem, and the Quadratic Assignment problem.},
author = {Clausen, Jens},
doi = {10.1.1.5.7475},
file = {:home/markg/Downloads/b{\_}and{\_}b.pdf:pdf},
journal = {Department of Computer Science, University of {\ldots}},
pages = {1--30},
title = {{Branch and bound algorithms-principles and examples}},
url = {http://www.imada.sdu.dk/{~}jbj/heuristikker/TSPtext.pdf},
year = {1999}
}
@article{Mcleod2010,
abstract = {The function bestglm selects the best subset of inputs for the glm family. The selection methods available include a variety of information criteria as well as cross-validation. Several examples are provided to show that this approach is sometimes more accurate than using the built-in R function step. In the Gaussian case the leaps-and-bounds algorithm in leaps is used provided that there are no factor variables with more than two levels. In the non-Gaussian glm case or when there are factor variables present with three or more levels, a simple exhaustive enumeration approach is used. This vignette also explains how the applications given in our article Xu and McLeod (2010) may easily be reproduced. A separate vignette is available to provide more details about the simulation results reported in Xu and McLeod (2010, Table 2) and to explain how the results may be reproduced.},
author = {Mcleod, a I},
file = {:home/markg/Downloads/bestglm.pdf:pdf},
journal = {Prostate The},
keywords = {aic,best subset glm,bic,cross validation,extended bic},
pages = {1--39},
title = {{bestglm : Best Subset GLM}},
url = {http://brieger.esalq.usp.br/CRAN/web/packages/bestglm/vignettes/bestglm.pdf},
year = {2010}
}
@article{Hosmer1989,
abstract = {Selection of a subset of meaningful covariates for a statistical model is an important and often time-consuming task in model building. Lawless and Singhal (1978, Biometrics 34, 318-327) proposed a method for best subsets selection for nonnormal models. We develop a method for logistic regression that may be performed with any best subsets linear regression program. CR - Copyright {\&}{\#}169; 1989 International Biometric Society},
author = {Hosmer, David W and Jovanovic, Borko and Lemeshow, Stanley},
doi = {10.2307/2531779},
file = {:home/markg/Downloads/2531779.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
number = {4},
pages = {1265--1270},
title = {{Best Subsets Logistic Regression}},
url = {http://www.jstor.org/stable/2531779},
volume = {45},
year = {1989}
}
@article{Vandenberghe1996,
abstract = {In sernidefinite programming, one minimizes a linear function subject to the constraint that an affine combination ofsynunetric matrices is positive semidefinite. Such a constraint is nonlinear and nonsmooth, but convex, so semidefinite programs are convex optimization problems. Semidefinite programming unifies several standard problems (e.g., linear and quadratic programming) and finds many applications in engineering and combinatorial optimization. Although semidefinite programs are much more general than linear programs, they are not much harder to solve. Most interior-point methods for linear programming have been generalized to semidefinite programs. As in linear programming, these methods have polynomial worst-case complexity and perform very well in practice. This paper gives a survey of the theory and applications of semidefinite programs and an introduction to primal- dual interior-point methods for their solution.},
author = {Vandenberghe, Lieven and Boyd, Stephen},
doi = {10.1137/1038003},
file = {:home/markg/Downloads/semidef{\_}prog.pdf:pdf},
isbn = {10.1137/1038003},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {combinatorial optimization,convex optimization,eigenvalue optimization,interior-point methods,semidefinite programming,system and control theory},
number = {1},
pages = {49--95},
pmid = {18059682},
title = {{Semidefinite Programming}},
url = {http://epubs.siam.org/doi/abs/10.1137/1038003},
volume = {38},
year = {1996}
}
@article{Lee2015,
author = {Lee, Cathy Yuen Yi and Wand, Matt P.},
doi = {10.1002/sim.6737},
file = {:home/markg/Downloads/sim6737.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {10.1002/sim.6737 and Bayesian inference,Markov chain Monte Carlo,approximation,bayesian inference,carlo,group-specific curves,longitudinal and multilevel data,markov chain monte,mean field variational Bayes approximation,mean field variational bayes,semiparametric regression},
number = {August},
pages = {n/a--n/a},
title = {{Variational methods for fitting complex Bayesian mixed effects models to health data}},
url = {http://doi.wiley.com/10.1002/sim.6737},
year = {2015}
}
@article{Petersen2012,
abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
doi = {10.1111/j.1365-294X.2006.03161.x},
file = {:home/markg/Downloads/matrixcookbook.pdf:pdf},
isbn = {0962-1083 (Print)$\backslash$r0962-1083 (Linking)},
issn = {09621083},
journal = {Citeseer},
keywords = {acknowledgements,and suggestions,bill baxter,christian rish{\o}j,contributions,derivative of,derivative of inverse matrix,determinant,di erentiate a matrix,douglas l,esben,matrix algebra,matrix identities,matrix relations,thank the following for,theobald,we would like to},
pages = {1--66},
pmid = {17284204},
title = {{The Matrix Cookbook}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Statistical+machine+learning+for+information+retrieval{\#}5},
year = {2012}
}
@article{S.2000,
author = {S. and Wood, N},
file = {:home/markg/Downloads/mspfinal.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society Series B},
keywords = {generalized additive models,generalized cross-validation,generalized ridge re-,gression,likelihood,model selection,multiple smoothing parameters,non-linear modelling,penalized,penalized regression splines},
number = {2},
pages = {413--428},
title = {{Modelling and smoothing parameter estimation with multiple quadratic penalties}},
volume = {62},
year = {2000}
}
@article{Raiko2007,
abstract = {We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas au- tomatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords:},
author = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen, Juha},
file = {:home/markg/Downloads/raiko07a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian modelling,building blocks,graphical models,latent variable models,local computation,variational Bayesian learning},
pages = {155--201},
title = {{Building Blocks for Variational Bayesian Learning of Latent Variable Models}},
volume = {8},
year = {2007}
}
@article{Challis2011,
abstract = {Two popular approaches to forming principled bounds in approximate Bayesian inference are local variational methods and minimal Kullback-Leibler divergence methods. For a large class of models, we explicitly relate the two approaches, showing that the local variational method is equivalent to a weakened form of Kullback-Leibler Gaussian approximation. This gives a strong motivation to develop ecient methods for KL minimisation. An important and previously unproven property of the KL variational Gaussian bound is that it is a concave function in the parameters of the Gaussian for log concave sites. This observation, along with compact concave parameterisations of the covariance, enables us to develop fast scalable optimisation procedures to obtain lower bounds on the marginal likelihood in large scale Bayesian linear models.},
author = {Challis, Edward and Barber, D},
file = {:home/markg/Downloads/challis11a.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Fourteenth International {\ldots}},
number = {2009},
pages = {199--207},
title = {{Concave Gaussian variational approximations for inference in large-scale Bayesian linear models}},
volume = {15},
year = {2011}
}
@article{Garay2015,
abstract = {In recent years, there has been considerable interest in regression models based on zero-inflated distribu-tions. These models are commonly encountered in many disciplines, such as medicine, public health, and environmental sciences, among others. The zero-inflated Poisson (ZIP) model has been typically consid-ered for these types of problems. However, the ZIP model can fail if the non-zero counts are overdispersed in relation to the Poisson distribution, hence the zero-inflated negative binomial (ZINB) model may be more appropriate. In this paper, we present a Bayesian approach for fitting the ZINB regression model. This model considers that an observed zero may come from a point mass distribution at zero or from the negative binomial model. The likelihood function is utilized to compute not only some Bayesian model selection measures, but also to develop Bayesian case-deletion influence diagnostics based on q-divergence measures. The approach can be easily implemented using standard Bayesian software, such as WinBUGS. The performance of the proposed method is evaluated with a simulation study. Further, a real data set is analyzed, where we show that ZINB regression models seems to fit the data better than the Poisson counterpart.},
author = {Garay, Aldo M and Lachos, Victor H and Bolfarine, Heleno and Paulo, S{\~{a}}o},
doi = {10.1080/02664763.2014.995610},
file = {:home/markg/Downloads/ZINB{\_}BayesianoRES.pdf:pdf},
issn = {0266-4763},
journal = {Journal of Applied Statistics},
keywords = {Bayesian inference,MCMC,binomial negative distribution,q-divergence measures,zero-inflated models},
number = {6},
pages = {1148--1165},
title = {{Bayesian estimation and case influence diagnostics for the zero-inflated negative binomial regression model}},
url = {http://www.tandfonline.com/loi/cjas20{\%}5Cnhttp://dx.doi.org/10.1080/02664763.2014.995610{\%}5Cnhttp://www.tandfonline.com/page/terms-and-conditions},
volume = {42},
year = {2015}
}
@article{Barber1998,
abstract = {Bayesian treatments of learning in neural networks$\backslash$nare typically based either on local Gaussian$\backslash$napproximations to a mode of the posterior weight$\backslash$ndistribution, or on Markov chain Monte Carlo$\backslash$nsimulations. A third approach, called ensemble$\backslash$nlearning, was introduced by Hinton and van Camp$\backslash$n(1993). It aims to approximate the posterior$\backslash$ndistribution by minimizing the Kullback-Leibler$\backslash$ndivergence between the true posterior and a$\backslash$nparametric approximating distribution. However, the$\backslash$nderivation of a deterministic algorithm relied on$\backslash$nthe use of a Gaussian approximating distribution$\backslash$nwith a diagonal covariance matrix and so was unable$\backslash$nto capture the posterior correlations between$\backslash$nparameters. In this paper, we show how the ensemble$\backslash$nlearning approach can be extended to$\backslash$nfull-covariance Gaussian distributions while$\backslash$nremaining computationally tractable. We also extend$\backslash$nthe framework to deal with hyperparameters, leading$\backslash$nto a simple re-estimation procedure. Initial$\backslash$nresults from a standard benchmark problem are$\backslash$nencouraging.},
author = {Barber, D and Bishop, C M},
file = {:home/markg/Downloads/1480-ensemble-learning-for-multi-layer-networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {395--401},
title = {{Ensemble learning for multi-layer networks}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=M55BL-GvQ8IC{\&}oi=fnd{\&}pg=PA395{\&}dq=Ensemble+learning+for+multi-layer+networks{\&}ots=Fsyx0wyKmU{\&}sig=WqiSbpJZBdFfuMuIEHrWqGqO9Sg},
year = {1998}
}
@article{Knowles2011,
abstract = {Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.},
author = {Knowles, David and Minka, Thomas P.},
file = {:home/markg/Downloads/KnoMin11.pdf:pdf},
isbn = {9781618395993},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {1--9},
title = {{Non-conjugate variational message passing for multinomial and binary regression}},
url = {http://eprints.pascal-network.org/archive/00008459/},
year = {2011}
}
@article{Wand2014,
abstract = {Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very efficiently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit.},
author = {Wand, Matt P.},
file = {:home/markg/Downloads/wand14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {bayesian computing,field variational bayes,graphical models,matrix differential calculus,mean,variational approximation},
pages = {1351--1369},
title = {{Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing}},
url = {http://jmlr.org/papers/v15/wand14a.html},
volume = {15},
year = {2014}
}
@article{Chib1995,
author = {Chib, Siddhartha and Greenberg, Edward},
file = {:home/markg/Downloads/chib{\_}1995.pdf:pdf},
journal = {American Statistician},
number = {4},
pages = {327--335},
title = {{Understanding the metropolis-hastings algorithm}},
volume = {49},
year = {1995}
}
@article{Opper2009,
abstract = {The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an Omicron(N)(2) number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually Omicron(N). The approach is applied to gaussian process regression with nongaussian likelihoods.},
author = {Opper, Manfred and Archambeau, C{\'{e}}dric},
doi = {10.1162/neco.2008.08-07-592},
file = {:home/markg/Downloads/neco{\_}mo09{\_}web.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
number = {3},
pages = {786--792},
pmid = {18785854},
title = {{The variational gaussian approximation revisited.}},
volume = {21},
year = {2009}
}
@article{Saul1996,
abstract = {We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition--the classification of handwritten digits.},
archivePrefix = {arXiv},
arxivId = {http://arxiv.org/pdf/cs/9603102.pdf},
author = {Saul, L K and Jaakkola, T and Jordan, M I},
doi = {10.1.1.54.4128},
eprint = {/arxiv.org/pdf/cs/9603102.pdf},
file = {:home/markg/Downloads/live-251-1520-jair.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
number = {1570},
pages = {61--76},
primaryClass = {http:},
title = {{Mean Field Theory for Sigmoid Belief Networks}},
url = {http://arxiv.org/abs/cs/9603102},
volume = {4},
year = {1996}
}
@article{Nickisch2008,
abstract = {We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.},
author = {Nickisch, Hannes and Rasmussen, Carl Edward},
file = {:home/markg/Downloads/nickisch08a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with Statistics,Theory {\&} Algorithms},
pages = {2035--2078},
title = {{Approximations for Binary Gaussian Process Classification}},
url = {http://eprints.pascal-network.org/archive/00005312/},
volume = {9},
year = {2008}
}
@article{Seeger1999,
author = {Seeger, M},
file = {:home/markg/Downloads/1722-bayesian-model-selection-for-support-vector-machines-gaussian-processes-and-other-kernel-classifiers.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 12},
title = {{Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers}},
year = {1999}
}
@article{Carlo1990,
author = {Carlo, Monte},
file = {:home/markg/Downloads/p5-hinton.pdf:pdf},
keywords = {gaussian quadrature,importance sampling,laplacian approximation,maximum likelihood estimation,nonlinear mixed effects models},
title = {{of the Weights P ( d ;}},
year = {1990}
}
@article{Wood2001,
abstract = {Objective functions that arise when fitting nonlinear models often contain local minima that are of little significance except for their propensity to trap minimization algorithms. The standard methods for attempting to deal with this problem treat the objective function as fixed and employ stochastic minimization approaches in the hope of randomly jumping out of local minima. This article suggests a simple trick for performing such minimizations that can be employed in conjunction with most conventional nonstochastic fitting methods. The trick is to stochastically perturb the objective function by bootstrapping the data to be fit. Each bootstrap objective shares the large-scale structure of the original objective but has different small-scale structure. Minimizations of bootstrap objective functions are alternated with minimizations of the original objective function starting from the parameter values with which minimization of the previous bootstrap objective terminated. An example is presented, fitting a nonlinear population dynamic model to population dynamic data and including a comparison of the suggested method with simulated annealing. Convergence diagnostics are discussed.},
author = {Wood, S N},
doi = {10.1111/j.0006-341X.2001.00240.x},
file = {:home/markg/Downloads/j.0006-341X.2001.00240.x.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
keywords = {ecological model,fitting,global optimization,nonlinear model fitting,population dynamic model,simulated annealing,stochastic optimization},
number = {1},
pages = {240--244},
pmid = {11252605},
title = {{Minimizing model fitting objectives that contain spurious local minima by bootstrap restarting.}},
volume = {57},
year = {2001}
}
@article{Wood2010,
abstract = {Generalized additive models (GAMs) have been popularized by the work of Hastie and Tibshirani (Generalized Additive Models (1990)) and the availability of user friendly gam software in Splus. However, whilst it is flexible and efficient, the gam framework based on backfitting with linear smoothers presents some difficulties when it comes to model selection and inference. On the other hand, the mathematically elegant work of Wahba (Spline Models for Observational Data (1990)) and co-workers on Generalized Spline Smoothing (GSS) provides a rigorous framework for model selection (SIAM J. Sci. Statist. Comput. 12 (1991) 383) and inference with GAMs constructed from smoothing splines: but unfortunately these models are computationally very expensive with operations counts that are of cubic order in the number of data. A `middle way' between these approaches is to construct GAMs using penalized regression splines (e.g. Marx and Eilers, Comput. Statist. Data Anal. (1998)). In this paper, we develop this idea further and show how GAMs constructed using penalized regression splines can be used to get most of the practical benefits of GSS models, including well founded model selection and multi-dimensional smooth terms, with the ease of use and low computational cost of backfit GAMs. Inference with the resulting methods also requires slightly fewer approximations than are employed in the GAM modelling software provided in Splus. This paper presents the basic mathematical and numerical approach to GAMs implemented in the package mgcv, and includes two environmental examples using the methods as implemented in the package.},
author = {Wood, S N and Augustin, N H},
doi = {10.1016/S0304-3800(02)00193-X},
file = {:home/markg/Downloads/wagam.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {gam,gcv,penalized regression spline},
number = {2-3},
pages = {157--177},
title = {{GAMs with integrated model selection using penalized regression splines and applications to environmental modelling}},
url = {http://opus.bath.ac.uk/7362/},
volume = {157},
year = {2010}
}
@article{Wood2004,
author = {Wood, Simon N},
doi = {10.1198/016214504000000980},
file = {:home/markg/Downloads/016214504000000980.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {generalized additive mixed model,generalized cross-validation,penalized quasi-likelihood,regularization,reml,ridge regression,smoothing spline analysis of,spline,stable computation,variance},
number = {467},
pages = {673--686},
title = {{Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000980},
volume = {99},
year = {2004}
}
@article{Ood2006,
author = {Ood, S Imon N W},
doi = {10.1111/j.1467-842X.2006.00450.x},
file = {:home/markg/Downloads/j.1467-842X.2006.00450.x.pdf:pdf},
issn = {1369-1473},
keywords = {bayesian confidence interval,gam,gcv,generalized additive model,generalized cross,multiple smoothing parameters,penalized regression spline,validation},
number = {4},
pages = {445--464},
title = {{ON CONFIDENCE INTERVALS FOR GENERALIZED ADDITIVE MODELS BASED ON PENALIZED REGRESSION SPLINES University of Bath}},
volume = {48},
year = {2006}
}
@article{Demyanov2006,
abstract = {Population dynamic modelling often entails parameterizing quite sophisticated biological and ecological mechanisms. For models of moderate mechanistic complexity, this has traditionally been done in an ad hoc manner, with different parameters being estimated independently. The point estimates so obtained are then used for model simulation, perhaps with some further ad hoc adjustment based on comparison with any available data on population dynamics.Quantitative assessments of model adequacy and prediction uncertainty are not easily made using this approach. As an alternative, the paper investigates the practical feasibility of fitting a moderately complex population dynamic model directly and simultaneously to all the data available for parameterization of the model, and to all available data on the population dynamics of the target animal. This alternative approach allows us to combine all available quantitative information on the target species, to assess the viability of the model, the mutual consistency of model and different sources of data and to estimate the uncertainties that are associated with model-based predictions. The target organism in this study is the freshwater amphipod Gammarus pulex (L.), which we model using a stage-structured population dynamic model, implemented via a set of delay differential equations describing the basic demography of the population. Target data include population dynamic data from two sites, information on basic physiological relationships and environmental temperature data. Fitting is performed by using a non-linear least squares approach supplemented with a bootstrapping method for avoiding small scale local minima in the least squares objective function. Variance estimation is performed by further bootstrapping. Interest in Gammarus pulex population dynamics in this case is primarily related to likely population level responses to chemical stressors, and for this we examine predicted ‘recovery times' following exposure to a known toxicant.},
author = {Demyanov, V. and Wood, S. N. and Kedwards, T. J. and Dernyanov, V},
doi = {10.1111/j.1467-9876.2005.00527.x},
file = {:home/markg/Downloads/j.1467-9876.2005.00527.x.pdf:pdf},
issn = {0035-9254},
journal = {Applied Statistics},
keywords = {differential equation model,ecological prediction,ecological risk assessment,population dynamic model},
number = {1},
pages = {41--62},
title = {{Improving ecological impact assessment by statistical data synthesis using process-based models}},
url = {http://doi.wiley.com/10.1111/j.1467-9876.2005.00527.x},
volume = {55},
year = {2006}
}
@article{Wood2003,
abstract = {discuss the production of low rank smoothers for d greater than or equal to 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with 'knot placement' that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics},
author = {Wood, Simon N.},
doi = {10.1111/1467-9868.00374},
file = {:home/markg/Downloads/1467-9868.00374.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Generalized additive model,Regression spline,Thin plate spline},
pages = {95--114},
title = {{Thin plate regression splines}},
volume = {65},
year = {2003}
}
@article{Wood2008,
abstract = {Existing computationally efficient methods for penalized likelihood generalized additive model fitting employ iterative smoothness selection on working linear models (or working mixed models). Such schemes fail to converge for a non-negligible proportion of models, with failure being particularly frequent in the presence of concurvity. If smoothness selection is performed by optimizing ‘whole model' criteria these problems disappear, but until now attempts to do this have employed finite-difference-based optimization schemes which are computationally inefficient and can suffer from false convergence. The paper develops the first computationally efficient method for direct generalized additive model smoothness selection. It is highly stable, but by careful structuring achieves a computational efficiency that leads, in simulations, to lower mean computation times than the schemes that are based on working model smoothness selection. The method also offers a reliable way of fitting generalized additive mixed models.},
archivePrefix = {arXiv},
arxivId = {0709.3906},
author = {Wood, Simon N.},
doi = {10.1111/j.1467-9868.2007.00646.x},
eprint = {0709.3906},
file = {:home/markg/Downloads/j.1467-9868.2007.00646.x.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Akaike's information criterion,Generalized additive mixed models,Generalized additive models,Generalized approximate cross-validation,Generalized cross-validation,Penalized likelihood,Penalized regression splines,Stable computation},
number = {3},
pages = {495--518},
title = {{Fast stable direct fitting and smoothness selection for generalized additive models}},
volume = {70},
year = {2008}
}
@article{Wood2008a,
abstract = {Conventional smoothing methods sometimes perform badly when used to smooth data over complex domains, by smoothing inappropriately across boundary features, such as peninsulas. Solutions to this smoothing problem tend to be computationally complex, and not to provide model smooth functions which are appropriate for incorporating as components of other models, such as generalized additive models or mixed additive models. We propose a class of smoothers that are appropriate for smoothing over difficult regions of R2 which can be represented in terms of a low rank basis and one or two quadratic penalties. The key features of these smoothers are that they do not `smooth across' boundary features, that their representation in terms of a basis and penalties allows straightforward incorporation as components of generalized additive models, mixed models and other non-standard models, that smoothness selection for these model components is straightforward to accomplish in a computationally efficient manner via generalized cross-validation, Akaike's information criterion or restricted maximum likelihood, for example, and that their low rank means that their use is computationally efficient.},
author = {Wood, Simon N. and Bravington, Mark V. and Hedley, Sharon L.},
doi = {10.1111/j.1467-9868.2008.00665.x},
file = {:home/markg/Downloads/j.1467-9868.2008.00665.x.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Basis penalty smooth,Differential equation smoothing,FELSPLINE,Finite window smoothing,Known boundary smoothing,Spline},
number = {5},
pages = {931--955},
title = {{Soap film smoothing}},
volume = {70},
year = {2008}
}
@article{Clarke2006,
abstract = {The distribution and biomass of phytoplankton in the upper layers of the ocean are important indicators of productivity and carbon cycling. Large scale perturbations in phytoplankton are linked to global climate change, so accurate monitoring is increasingly important. The chlorophyll-a pigment concentration in the water is routinely measured as an index of algal biomass. Direct water sampling from ships and moorings provides accurate data, but woefully poor spatial and temporal coverage of the oceans. In contrast, multispectral sea surface reflectance data from orbiting satellite-borne sensors, which in principle can be used to derive pigment concentration, give the prospect of globally detailed spatial and temporal coverage. Unfortunately, there are some locally variable confounding factors, which the algorithms for converting reflectance data to ocean chlorophyll-a concentration do not take into account. Hence, statistical methods are needed to obtain accurate predictions of chlorophyll-a. concentration by using data from both these sources. We use penalized regression splines to model water sample data as a three-dimensional function of satellite measurements, seabed depth and time of year. The models are effectively complex calibrations of the satellite data against the bottle data. We compare the results by using thin plate regression splines and tensor product splines using generalized cross-validation to choose the relative amounts of smoothing for each of the covariates. Since the thin plate spline penalty functional is isotropic, this requires the introduction of two scaling parameters, which are also chosen by generalized cross-validation, to scale the covariates relatively to one another. The tensor product spline smooths each covariate appropriately by use of separate smoothing parameters for each covariate. The models are tested by application to data from the north-east Atlantic, first randomly subsampling the data to achieve even coverage over the entire region. Both approaches perform equally well, achieving R-2 approximate to 65{\%}, both for the data that are used to fit the model and for a validation data set. Of particular concern in this application is that monthly predictions from the models should be biologically plausible over the whole region, describing the broad regional features that are apparent in the satellite data and extrapolating sensibly where satellite data are not available. To achieve this, the satellite data must be one of the covariates in the model; spatiotemporal covariates alone are not sufficient to extrapolate sensibly into areas where no data are available.},
author = {Clarke, E. D. and Speirs, D. C. and Heath, M. R. and Wood, S. N. and Gurney, W. S. C. and Holmes, S. J.},
doi = {10.1111/j.1467-9876.2006.00540.x},
file = {:home/markg/Downloads/j.1467-9876.2006.00540.x.pdf:pdf},
issn = {0035-9254},
journal = {Journal of the Royal Statistical Society Series C-Applied Statistics},
pages = {331--353},
title = {{Calibrating remotely sensed chlorophyll-a data by using penalized regression splines}},
volume = {55},
year = {2006}
}
@article{Wood2006,
abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
author = {Wood, Simon N.},
doi = {10.1111/j.1541-0420.2006.00574.x},
file = {:home/markg/Downloads/j.1541-0420.2006.00574.x.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
keywords = {Computationally efficient,Generalized additive mixed model (GAMM),Mixed effect variable coefficient model,Multiple penalties,Penalized regression,SS-ANOVA,Scale invariant,Smooth interaction,Smoothing penalty,Spline,Tensor product smooth},
number = {4},
pages = {392},
pmid = {17156276},
title = {{Low-rank scale-invariant tensor product smooths for generalized additive mixed models}},
volume = {62},
year = {2006}
}
@article{Lambert1992,
abstract = {Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 - p, a Poisson(lambda) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(lambda) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects-lambda in the imperfect state-may depend on covariates. Sometimes p and lambda are unrelated; other times p is a simple function of lambda such as p = 1/(1 + lambda(tau)) for an unknown constant-tau. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.},
author = {Lambert, Diane},
file = {:home/markg/Downloads/1269547.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {EM ALGORITHM,NEGATIVE BINOMIAL,OVERDISPERSION,P},
number = {1},
pages = {1--14},
title = {{Zero-Inflated Poisson Regression, With an Application To Defects in Manufacturing}},
volume = {34},
year = {1992}
}
@article{Honkela2010,
abstract = {Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.},
author = {Honkela, Antti and Raiko, Tapani and Kuusela, Mikael and Tornio, Matti and Karhunen, Juha},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Honkela et al. - 2010 - Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {5},
pages = {3235--3268},
title = {{Approximate Riemannian conjugate gradient learning for fixed-form Variational Bayes}},
url = {http://eprints.pascal-network.org/archive/00007751/},
volume = {11},
year = {2010}
}
@article{Amari1998,
abstract = {Gradient adaptation is a useful technique for adjusting a set of parameters to minimize a cost function. While often easy to implement, the convergence speed of gradient adaptation can be slow when the slope of the cost function varies widely for small changes in the parameters. In this paper, we outline an alternative technique, termed natural gradient adaptation, that overcomes the poor convergence properties of gradient adaptation in many cases. The natural gradient is based on differential geometry and employs knowledge of the Riemannian structure of the parameter space to adjust the gradient search direction. Unlike Newton's method, natural gradient adaptation does not assume a locally-quadratic cost function. Moreover, for maximum likelihood estimation tasks, natural gradient adaptation is asymptotically Fisher-efficient. A simple example illustrates the desirable properties of natural gradient adaptation.},
author = {Amari, S and Douglas, Sc},
doi = {10.1109/ICASSP.1998.675489},
isbn = {0-7803-4428-6},
issn = {1520-6149},
journal = {{\ldots} , 1998. Proceedings of the 1998 IEEE {\ldots}},
pages = {1213--1216},
title = {{Why natural gradient?}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=675489},
volume = {9},
year = {1998}
}
@article{Rohde2015,
author = {Rohde, D and Wand, M. P.},
file = {:home/markg/Downloads/RohdeWand.pdf:pdf},
keywords = {bayesian computing,conjugate variational message passing,fixed-form variational bayes,fixed-point iteration,non-,nonlinear conjugate gradient method},
number = {January},
pages = {1--41},
title = {{Semiparametric Mean Field Variational Bayes : General Principles and Numerical Issues Semiparametric Mean Field Variational Bayes :}},
year = {2015}
}
@article{Ormerod2012,
abstract = {Variational approximation methods have become a mainstay of contemporary machine learning methodology, but currently have little presence in statistics. We devise an effective variational approximation strategy for fitting generalized linear mixed models (GLMMs) appropriate for grouped data. It involves Gaussian approximation to the distributions of random effects vectors, conditional on the responses. We show that Gaussian variational approximation is a relatively simple and natural alternative to Laplace approximation for fast, non-Monte Carlo, GLMM analysis. Numerical studies show Gaussian variational approximation to be very accurate in grouped data GLMM contexts. Finally, we point to some recent theory on consistency of Gaussian variational approximation in this context. Supplemental materials are available online.},
author = {Ormerod, J. T. and Wand, M. P.},
doi = {10.1198/jcgs.2011.09118},
file = {:home/markg/Downloads/Ormerod11.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {best prediction,likelihood-based inference,longitudinal data analysis,machine learning,variance components},
number = {1},
pages = {2--17},
title = {{Gaussian Variational approximate inference for Generalized Linear Mixed Models}},
volume = {21},
year = {2012}
}
@article{Wand2002,
abstract = {Many statisitcal operations benefit from differential calculus. Examples include optimization of likelihood functions and calculation of information matrices. For multiparameter models differential calculuc suited to vector argument functions is usually the most efficient means of performing the required calculations. We present a primer on vector differential calculus and demonstrate its application to statistics through several worked examples.},
author = {Wand, M. P},
doi = {10.1198/000313002753631376},
file = {:home/markg/Downloads/Wand02.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {best linear prediction,generalized linear,generalized linear mixed model,information matrix,matrix differential calculus,maximum likelihood estimation,model,penalized quasi-likelihood,score equation},
number = {1},
pages = {55--62},
title = {{Vector Differential Calculus in Statistics}},
volume = {56},
year = {2002}
}
@article{Vatsa2014,
author = {Vatsa, Richa and Wilson, Simon},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vatsa, Wilson - 2014 - Variational Bayes Approximation for Inverse Non-Linear Regression.pdf:pdf},
issn = {2278-2273},
keywords = {1,2278-2273,2348-7909,e-issn,earch,inverse non-linear,journal of statistics,p-issn,reviews,special issue on recent,statistical methodologies and applications,variational bayes approximation for,vol},
number = {JANUARY 2014},
pages = {76--84},
title = {{Variational Bayes approximation for inverse non-linear regression}},
year = {2014}
}
@book{Agresti2002,
abstract = {Cap{\'{i}}tulo 4: Introducci{\'{o}}n a los GLM. La secci{\'{o}}n 4.3 est{\'{a}} dedicada a datos de conteo. Es una buena introducci{\'{o}}n, con un poco m{\'{a}}s de profundidad en cada tema. Todo el libro es muy bueno.},
author = {Agresti, Alan},
booktitle = {Statistical methodology in the pharmaceutical sciences},
doi = {10.1002/0471249688},
isbn = {0471360937},
issn = {0949-1775},
pages = {1--17},
pmid = {15003161},
title = {{Categorical Data Analysis}},
volume = {13},
year = {2002}
}
@book{Gelman2007,
abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/{\~{}}gelman/arm/},
author = {Gelman, Andrew and Hill, Jennifer},
booktitle = {Policy Analysis},
doi = {10.2277/0521867061},
isbn = {052168689X},
issn = {0022-0655},
pages = {625},
pmid = {14341096},
title = {{Data analysis using regression and multilevel/hierarchical models}},
volume = {625},
year = {2007}
}
@article{Luts2013,
abstract = {A mean field variational Bayes approach to support vector machines (SVMs) using the latent variable representation on Polson {\&} Scott (2012) is presented. This representation allows circumvention of many of the shortcomings associated with classical SVMs including automatic penalty parameter selection, the ability to handle dependent samples, missing data and variable selection. We demonstrate on simulated and real datasets that our approach is easily extendable to non-standard situations and outperforms the classical SVM approach whilst remaining computationally efficient.},
archivePrefix = {arXiv},
arxivId = {1305.2667},
author = {Luts, Jan and Ormerod, John T.},
doi = {10.1016/j.csda.2013.10.030},
eprint = {1305.2667},
file = {:home/markg/Downloads/1305.2667v1.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {approximate bayesian inference},
number = {2000},
pages = {18},
title = {{Mean field Variational Bayesian inference for support vector machine classification}},
url = {http://arxiv.org/abs/1305.2667},
volume = {73},
year = {2013}
}
@article{LeeWangScottYauMcLachlan2006,
abstract = {Count data with excess zeros relative to a Poisson distribution are common in many biomedical applications. A popular approach to the analysis of such data is to use a zero-inflated Poisson (ZIP) regression model. Often, because of the hierarchical study design or the data collection procedure, zero-inflation and lack of independence may occur simultaneously, which render the standard ZIP model inadequate. To account for the preponderance of zero counts and the inherent correlation of observations, a class of multi-level ZIP regression model with random effects is presented. Model fitting is facilitated using an expectation-maximization algorithm, whereas variance components are estimated via residual maximum likelihood estimating equations. A score test for zero-inflation is also presented. The multi-level ZIP model is then generalized to cope with a more complex correlation structure. Application to the analysis of correlated count data from a longitudinal infant feeding study illustrates the usefulness of the approach.},
annote = {Copyright - {\textcopyright} 2006 Arnold; Last updated - 2014-04-29},
author = {Lee, Andy H and Wang, Kui and Scott, Jane A and Yau, Kelvin K W and McLachlan, Geoffrey J},
file = {:home/markg/Downloads/lwsym{\_}smmr06.pdf:pdf},
isbn = {09622802},
journal = {Statistical methods in medical research},
keywords = {Breast Feeding -- statis,Female,Infant,Longitudinal Studies,Male,Medical Sciences,Models,Newborn,Statistical},
number = {1},
pages = {47--61},
title = {{Multi-level zero-inflated Poisson regression modelling of correlated count data with excess zeros}},
volume = {15},
year = {2006}
}
@article{BIMJ:BIMJ200390024,
author = {Yau, Kelvin K W and Wang, Kui and Lee, Andy H},
doi = {10.1002/bimj.200390024},
issn = {1521-4036},
journal = {Biometrical Journal},
keywords = {Count data,Generalised linear mixed models,Negative binomial,Poisson regression,Random effects,Zero-inflation},
number = {4},
pages = {437--452},
publisher = {WILEY-VCH Verlag},
title = {{Zero-Inflated Negative Binomial Mixed Regression Modeling of Over-Dispersed Count Data with Extra Zeros}},
volume = {45},
year = {2003}
}
@book{Nocedal2006,
author = {Nocedal, Jorge and Wright, Stephen},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nocedal, Wright - 2006 - Numerical optimization.pdf:pdf},
publisher = {Springer Science {\&} Business Media},
title = {{Numerical optimization}},
year = {2006}
}
@article{Wand2008,
author = {Wand, M P and Ormerod, J T},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wand, Ormerod - 2008 - ON SEMIPARAMETRIC REGRESSION WITH O'SULLIVAN PENALIZED SPLINES.pdf:pdf},
journal = {Australian {\&} New Zealand Journal of Statistics},
number = {2},
pages = {179--198},
publisher = {Wiley Online Library},
title = {{On semiparametric regression with O'Sullivan penalized splines}},
volume = {50},
year = {2008}
}
@book{Demmel1997,
author = {Demmel, James W},
publisher = {Siam},
title = {{Applied numerical linear algebra}},
year = {1997}
}
@article{JOFP:rethink,
author = {{Atkins David C.; Gallop}, Robert J},
doi = {10.1037/0893-3200.21.4.726},
journal = {Journal of Family Psychology},
keywords = {Poisson regression,count models,zero-inflated models},
number = {4},
pages = {726--735},
publisher = {American Psychological Association},
title = {{Rethinking How Family Researchers Model Infrequent Outcomes: A Tutorial on Count Regression and Zero-Inflated Models.}},
volume = {21},
year = {2007}
}
@article{broydon1970,
author = {Broydon, C G},
journal = {Journal of the Institute of Mathematics and its Applications},
title = {{The convergence of a class of double rank minimisation algorithms: 1. General considerations}},
volume = {6},
year = {1970}
}
@article{Shankar1997829,
abstract = {This paper presents an empirical inquiry into the applicability of zero-altered counting processes to roadway section accident frequencies. The intent of such a counting process is to distinguish sections of roadway that are truly safe (near zero-accident likelihood) from those that are unsafe but happen to have zero accidents observed during the period of observation (e.g. one year). Traditional applications of Poisson and negative binomial accident frequency models do not account for this distinction and thus can produce biased coefficient estimates because of the preponderance of zero-accident observations. Zero-altered probability processes such as the zero-inflated Poisson (ZIP) and zero-inflated negative binomial (ZINB) distributions are examined and proposed for accident frequencies by roadway functional class and geographic location. The findings show that the {\{}ZIP{\}} structure models are promising and have great flexibility in uncovering processes affecting accident frequencies on roadway sections observed with zero accidents and those with observed accident occurrences. This flexibility allows highway engineers to better isolate design factors that contribute to accident occurrence and also provides additional insight into variables that determine the relative accident likelihoods of safe versus unsafe roadways. The generic nature of the models and the relatively good power of the Vuong specification test used in the non-nested hypotheses of model specifications offers roadway designers the potential to develop a global family of models for accident frequency prediction that can be embedded in a larger safety management system.},
author = {Shankar, V and Milton, J and Mannering, F},
doi = {http://dx.doi.org/10.1016/S0001-4575(97)00052-3},
issn = {0001-4575},
journal = {Accident Analysis and Prevention},
keywords = {Accident frequency,Poisson regression,Zero-inflated count models},
number = {6},
pages = {829--837},
title = {{Modeling accident frequencies as zero-altered probability processes: An empirical inquiry}},
volume = {29},
year = {1997}
}
@article{BIOM:BIOM1030,
author = {Hall, Daniel B},
doi = {10.1111/j.0006-341X.2000.01030.x},
issn = {1541-0420},
journal = {Biometrics},
keywords = {EM algorithm,Excess zeros,Generalized linear mixed model,Heterogeneity,Mixed effects,Overdispersion,Repeated measures},
number = {4},
pages = {1030--1039},
publisher = {Blackwell Publishing Ltd},
title = {{Zero-Inflated Poisson and Binomial Regression with Random Effects: A Case Study}},
volume = {56},
year = {2000}
}
@misc{stan-manual:2015,
title = {{No Title}}
}
@article{Min01042005,
abstract = {For count responses, the situation of excess zeros (relative to what standard models allow) often occurs in biomedical and sociological applications. Modeling repeated measures of zero-inflated count data presents special challenges. This is because in addition to the problem of extra zeros, the correlation between measurements upon the same subject at different occasions needs to be taken into account. This article discusses random effect models for repeated measurements on this type of response variable. A useful model is the hurdle model with random effects, which separately handles the zero observations and the positive counts. In maximum likelihood model fitting, we consider both a normal distribution and a nonparametric approach for the random effects. A special case of the hurdle model can be used to test for zero inflation. Random effects can also be introduced in a zero-inflated Poisson or negative binomial model, but such a model may encounter fitting problems if there is zero deflation at any settings of the explanatory variables. A simple alternative approach adapts the cumulative logit model with random effects, which has a single set of parameters for describing effects. We illustrate the proposed methods with examples.},
author = {Min, Yongyi and Agresti, Alan},
doi = {10.1191/1471082X05st084oa},
journal = {Statistical Modelling},
number = {1},
pages = {1--19},
title = {{Random effect models for repeated measures of zero-inflated count data}},
volume = {5},
year = {2005}
}
@misc{rstan-software:2015,
title = {{No Title}}
}
@article{Liu1989,
author = {Liu, Dong C and Nocedal, Jorge},
journal = {Mathematical programming},
number = {1-3},
pages = {503--528},
publisher = {Springer},
title = {{On the limited memory BFGS method for large scale optimization}},
volume = {45},
year = {1989}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Jaakola{\_}Jordan{\_}2000.ps:ps},
title = {{Jaakola{\_}Jordan{\_}2000}}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - WandOrmerod08.rs:rs},
title = {{WandOrmerod08}}
}
@article{Ormerod2010,
abstract = {The American Statistician, Vol.64, No.2, 2010, 140-153},
author = {Ormerod, J T and Wand, M P},
doi = {10.1198/tast.2010.09058},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ormerod, Wand - 2010 - Explaining Variational Approximations.pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bayesian inference,Bayesian networks,Directed ac,back,bayesian inference,bayesian networks,di-,generalized linear mixed models,kull-,leibler divergence,linear mixed models,rected acyclic graphs},
number = {2},
pages = {140--153},
title = {{Explaining variational approximations}},
volume = {64},
year = {2010}
}
@article{Pham,
abstract = {A fast mean field variational Bayes (MFVB) approach to nonparametric regression when the predictors are subject to classical measurement error is investigated. It is shown that the use of such technology to the measurement error setting achieves reasonable accuracy. In tandem with the methodological development, a customized Markov chain Monte Carlo method is developed to facilitate the evaluation of accuracy of the MFVB method.},
author = {Pham, Tung H and Ormerod, John T and Wand, M P},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham, Ormerod, Wand - Unknown - Mean Field Variational Bayesian Inference for Nonparametric Regression with Measurement Error.pdf:pdf},
keywords = {Markov chain Monte Carlo,Penalized splines,classical measurement error,variational approximations},
title = {{Mean Field Variational Bayesian Inference for Nonparametric Regression with Measurement Error}}
}
@article{Zhao2006,
abstract = {Linear mixed models are able to handle an extraordinary range of complications in regression-type analyses. Their most common use is to account for within-subject correlation in longitudinal data analysis. They are also the standard vehicle for smoothing spatial count data. However, when treated in full generality, mixed models can also handle spline-type smoothing and closely approximate kriging. This allows for nonparametric regression models (e.g., additive models and varying coefficient models) to be handled within the mixed model framework. The key is to allow the ran-dom effects design matrix to have general structure; hence our label general design. For continuous response data, particularly when Gaussianity of the response is reasonably assumed, computation is now quite mature and sup-ported by the R, SAS and S-PLUS packages. Such is not the case for bi-nary and count responses, where generalized linear mixed models (GLMMs) are required, but are hindered by the presence of intractable multivariate in-tegrals. Software known to us supports special cases of the GLMM (e.g., PROC NLMIXED in SAS or glmmML in R) or relies on the sometimes crude Laplace-type approximation of integrals (e.g., the SAS macro glimmix or glmmPQL in R). This paper describes the fitting of general design general-ized linear mixed models. A Bayesian approach is taken and Markov chain Monte Carlo (MCMC) is used for estimation and inference. In this gener-alized setting, MCMC requires sampling from nonstandard distributions. In this article, we demonstrate that the MCMC package WinBUGS facilitates sound fitting of general design Bayesian generalized linear mixed models in practice.},
author = {Zhao, Y and Staudenmayer, J and Coull, B A and Wand, M P},
doi = {10.1214/088342306000000015},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2006 - General Design Bayesian Generalized Linear Mixed Models.pdf:pdf},
journal = {Statistical Science},
keywords = {Generalized additive models,Markov chain Monte Carlo,WinBUGS,and phrases,hierarchical center-ing,kriging,nonparametric regression,penalized splines,spatial count data},
number = {1},
pages = {35--51},
title = {{General Design Bayesian Generalized Linear Mixed Models}},
volume = {21},
year = {2006}
}
@article{Welham2007,
abstract = {Three types of polynomial mixed model splines have been proposed: smoothing splines, P-splines and penalized splines using a truncated power function basis. The close connections between these models are demonstrated, showing that the default cubic form of the splines differs only in the penalty used. A general definition of the mixed model spline is given that includes general constraints and can be used to produce natural or periodic splines. The impact of different penalties is demonstrated by evaluation across a set of functions with specific features, and shows that the best penalty in terms of mean squared error of prediction depends on both the form of the underlying function and the signal:noise ratio.},
author = {Welham, Sue J and Cullis, Brian R and Kenward, Michael G and Thompson, Robin},
doi = {10.1111/j.1467-842X.2006.00454.x},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Welham et al. - 2007 - A COMPARISON OF MIXED MODEL SPLINES FOR CURVE FITTING.pdf:pdf},
journal = {Aust. N. Z. J. Stat},
keywords = {P-splines,best linear unbiased prediction,mixed models,penalized splines,residual maximum likelihood,smoothing splines},
number = {1},
pages = {1--23},
title = {{A COMPARISON OF MIXED MODEL SPLINES FOR CURVE FITTING}},
volume = {49},
year = {2007}
}
@article{Albert1993,
author = {Albert, James H and Chib, Siddhartha},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Albert, Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data.pdf:pdf},
journal = {Source Journal of the American Statistical Association},
number = {422},
pages = {669--679},
publisher = {American Statistical Association},
title = {{Bayesian Analysis of Binary and Polychotomous Response Data}},
url = {http://www.jstor.org/stable/2290350 http://www.jstor.org/ http://www.jstor.org/action/showPublisher?publisherCode=astata.},
volume = {88},
year = {1993}
}
@article{Cibulskis2013,
abstract = {Detection of somatic point substitutions is a key step in characterizing the cancer genome. However, existing methods typically miss low-allelic-fraction mutations that occur in only a subset of the sequenced cells owing to either tumor heterogeneity or contamination by normal cells. Here we present MuTect, a method that applies a Bayesian classifier to detect somatic mutations with very low allele fractions, requiring only a few supporting reads, followed by carefully tuned filters that ensure high specificity. We also describe benchmarking approaches that use real, rather than simulated, sequencing data to evaluate the sensitivity and specificity as a function of sequencing depth, base quality and allelic fraction. Compared with other methods, MuTect has higher sensitivity with similar specificity, especially for mutations with allelic fractions as low as 0.1 and below, making MuTect particularly useful for studying cancer subclones and their evolution in standard exome and genome sequencing data. Somatic single-nucleotide substitutions are an important and common mechanism for altering gene function in cancer. Yet they are difficult to identify. First, they occur at a very low frequency in the genome, ranging from 0.1 to 100 mutations per megabase (Mb), depending on tumor type 1–7 . Second, the alterations may be present only in a small fraction of the DNA molecules originating from the specific genomic locus for reasons including contaminating normal cells in the ana-lyzed sample, local copy-number variation in the cancer genome and presence of a mutation only in a subpopulation of the tumor cells 8–11 ('subclonality'). The fraction of DNA molecules harboring an altera-tion ('allelic fraction') has been reported to be as low as 0.05 for highly impure tumors 8 . The study of the subclonal structure of tumors is not only critical to understanding tumor evolution both in disease pro-gression and response to treatment 12 but also for developing reliable clinical diagnostic tools for personalized cancer therapy 13 . Recent reports on subclonal events in cancer have used three dif-ferent nonstandard experimental strategies: (i) analysis of clonal mutations present in several, but not all, of the metastases from the same patient, which suggested that these mutations were subclonal in the primary tumor 14 ; (ii) detection of subclonal mutations by ultra-deep sequencing 11 ; or (iii) sequencing of very small numbers of single cells 15–17 . In contrast, tens of thousands of tumors are being sequenced at standard depths of 100–150× for exomes and 30–60× for whole genomes as part of large-scale cancer genome projects, such as The Cancer Genome Atlas 1,2,7 and the International Cancer Genome Consortium 18 . To detect clonal and subclonal mutations present in these samples, one needs a highly sensitive and specific mutation-calling method. Although specificity can be controlled through subsequent experimental validation, this is an expensive and time-consuming step that is impractical for general application. The sensitivity and specificity of any somatic mutation–calling method varies along the genome and depends on several fac-tors, including the depth of sequence coverage in the tumor and a patient-matched normal sample, the local sequencing error rate, the allelic fraction of the mutation and the evidence thresholds used to declare a mutation. Characterizing how sensitivity and specificity depend on these factors is necessary for designing experiments with adequate power to detect mutations at a given allelic fraction, as well as for inferring the mutation frequency along the genome, which is a key parameter for understanding mutational processes and significance analysis 19,20 . To meet these critical needs of high sensitivity and specificity, which are not adequately addressed by available methods 21–23 , we developed a caller of somatic point mutations, MuTect. During its development, MuTect was used in many collaborative studies 1–4,7,19,24–35 . Here we describe the publicly available version of MuTect, including the ration-ale behind its different components. We also estimate its perform-ance as a function of the aforementioned factors using benchmarking approaches that, to our knowledge, have not been described before. The performance of the method is also supported by independent experi-mental validation in previous studies 3,4,7,19,24–30 as well as by its appli-cation to data sets analyzed in other publications 36,37 . We demonstrate that our method is several times more sensitive than other methods for low-allelic-fraction events while remaining highly specific, allowing for deeper exploration of the mutational landscape of highly impure tumor samples and of the subclonal evolution of tumors. MuTect is freely available for noncommercial use at http://www. broadinstitute.org/cancer/cga/mutect (Supplementary Data).},
author = {Cibulskis, Kristian and Lawrence, Michael S and Carter, Scott L and Sivachenko, Andrey and Jaffe, David and Sougnez, Carrie and Gabriel, Stacey and Meyerson, Matthew and Lander, Eric S and Getz, Gad},
doi = {10.1038/nbt.2514},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cibulskis et al. - 2013 - Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples.pdf:pdf},
journal = {Nature Biotechnology},
title = {{Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples}},
volume = {31},
year = {2013}
}
@article{Braun2008,
abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate infer-ence possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demon-strate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
author = {Braun, Michael and Mcauliffe, Jon},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braun, Mcauliffe - 2008 - Variational inference for large-scale models of discrete choice.2526:2526},
title = {{Variational inference for large-scale models of discrete choice}},
year = {2008}
}
@article{Lawrence2013,
abstract = {Major international projects are underway that are aimed at creating a comprehensive catalogue of all the genes responsible for the ini-tiation and progression of cancer 1–9 . These studies involve the sequencing of matched tumour–normal samples followed by math-ematical analysis to identify those genes in which mutations occur more frequently than expected by random chance. Here we describe a fundamental problem with cancer genome studies: as the sample size increases, the list of putatively significant genes produced by current analytical methods burgeons into the hundreds. The list includes many implausible genes (such as those encoding olfactory receptors and the muscle protein titin), suggesting extensive false-positive findings that overshadow true driver events. We show that this problem stems largely from mutational heterogeneity and provide a novel analytical methodology, MutSigCV, for resolving the problem. We apply MutSigCV to exome sequences from 3,083 tumour–normal pairs and discover extraordinary variation in mutation frequency and spectrum within cancer types, which sheds light on mutational processes and disease aetiology, and in mutation frequency across the genome, which is strongly correlated with DNA replication timing and also with transcriptional activity. By incorporating mutational heterogeneity into the analyses, MutSigCV is able to eliminate most of the apparent artefactual findings and enable the identification of genes truly associated with cancer.},
author = {Lawrence, Michael S and Stojanov, Petar and Polak, Paz and Kryukov, Gregory V and Cibulskis, Kristian and Sivachenko, Andrey and Carter, Scott L and Stewart, Chip and Mermel, Craig H and Roberts, Steven A and Kiezun, Adam and Hammerman, Peter S and Mckenna, Aaron and Drier, Yotam and Zou, Lihua and Ramos, Alex H and Pugh, Trevor J and Stransky, Nicolas and Helman, Elena and Kim, Jaegil and Sougnez, Carrie and Ambrogio, Lauren and Nickerson, Elizabeth and Shefler, Erica and Cort{\'{e}}s, Maria L and Auclair, Daniel and Saksena, Gordon and Voet, Douglas and Noble, Michael and Dicara, Daniel and Lin, Pei and Lichtenstein, Lee and Heiman, David I and Fennell, Timothy and Imielinski, Marcin and Hernandez, Bryan and Hodis, Eran and Baca, Sylvan and Dulak, Austin M and Lohr, Jens and Landau, Dan-Avi and Wu, Catherine J and Melendez-Zajgla, Jorge and Hidalgo-Miranda, Alfredo and Koren, Amnon and Mccarroll, Steven A and Mora, Jaume and Lee, Ryan S and Crompton, Brian and Onofrio, Robert and Parkin, Melissa and Winckler, Wendy and Ardlie, Kristin and Gabriel, Stacey B and Roberts, Charles W M and Biegel, Jaclyn A and Stegmaier, Kimberly and Bass, Adam J and Garraway, Levi A and Meyerson, Matthew and Golub, Todd R and Gordenin, Dmitry A and Sunyaev, Shamil and Lander, Eric S and Getz, Gad},
doi = {10.1038/nature12213},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lawrence et al. - 2013 - Mutational heterogeneity in cancer and the search for new cancer-associated genes.pdf:pdf},
journal = {Nature},
title = {{Mutational heterogeneity in cancer and the search for new cancer-associated genes}},
volume = {499},
year = {2013}
}
@article{Hodis2012,
abstract = {SUMMARY Despite recent insights into melanoma genetics, systematic surveys for driver mutations are chal-lenged by an abundance of passenger mutations caused by carcinogenic UV light exposure. We devel-oped a permutation-based framework to address this challenge, employing mutation data from in-tronic sequences to control for passenger mutational load on a per gene basis. Analysis of large-scale melanoma exome data by this approach discovered six novel melanoma genes (PPP6C, RAC1, SNX31, TACC1, STK19, and ARID2), three of which—RAC1, PPP6C, and STK19—harbored recurrent and poten-tially targetable mutations. Integration with chromo-somal copy number data contextualized the land-scape of driver mutations, providing oncogenic insights in BRAF-and NRAS-driven melanoma as well as those without known NRAS/BRAF mutations. The landscape also clarified a mutational basis for RB and p53 pathway deregulation in this malignancy. Finally, the spectrum of driver mutations provided unequivocal genomic evidence for a direct muta-genic role of UV light in melanoma pathogenesis.},
author = {Hodis, Eran and Watson, Ian R and Kryukov, Gregory V and Arold, Stefan T and Imielinski, Marcin and Theurillat, Jean-Philippe and Nickerson, Elizabeth and Auclair, Daniel and Li, Liren and Place, Chelsea and Dicara, Daniel and Ramos, Alex H and Lawrence, Michael S and Cibulskis, Kristian and Sivachenko, Andrey and Voet, Douglas and Saksena, Gordon and Stransky, Nicolas and Onofrio, Robert C and Winckler, Wendy and Ardlie, Kristin and Wagle, Nikhil and Wargo, Jennifer and Chong, Kelly and Morton, Donald L and Stemke-Hale, Katherine and Chen, Guo and Noble, Michael and Meyerson, Matthew and Ladbury, John E and Davies, Michael A and Gershenwald, Jeffrey E and Wagner, Stephan N and Hoon, Dave S B and Schadendorf, Dirk and Lander, Eric S and Gabriel, Stacey B and Getz, Gad and Garraway, Levi A},
doi = {10.1016/j.cell.2012.06.024},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodis et al. - 2012 - A Landscape of Driver Mutations in Melanoma.pdf:pdf},
journal = {Cell},
pages = {251--263},
title = {{A Landscape of Driver Mutations in Melanoma}},
url = {http://dx.doi.org/10.1016/j.cell.2012.06.024},
volume = {150},
year = {2012}
}
@article{Challis2013,
abstract = {We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the fol-lowing novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate in-ference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.},
author = {Challis, Edward and Barber, David},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Challis, Barber DBARBER - 2013 - Gaussian Kullback-Leibler Approximate Inference.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Gaussian processes,active learning,experimental design,generalised linear models,large scale inference,latent linear models,sparse learning,variational approximate inference},
pages = {2239--2286},
title = {{Gaussian Kullback-Leibler Approximate Inference}},
volume = {14},
year = {2013}
}
@article{Ghosh2006,
abstract = {In modeling defect counts collected from an established manufacturing processes, there are usually a relatively large number of zeros (non-defects). The commonly used models such as Poisson or Geometric distributions can underestimate the zero-defect probability and hence make it difficult to identify significant covariate effects to improve production quality. This article introduces a flexible class of zero inflated models which includes other familiar models such as the Zero Inflated Poisson (ZIP) models, as special cases. A Bayesian estimation method is developed as an alternative to tra-ditionally used maximum likelihood based methods to analyze such data. Simulation studies show that the proposed method has better finite sample performance than the classical method with tighter interval estimates and better coverage probabilities. A real-life data set is analyzed to illustrate the practicability of the proposed method easily implemented using WinBUGS.},
author = {Ghosh, Sujit K and Mukhopadhyay, Pabak and Lu, Jye-Chyi},
doi = {10.1016/j.jspi.2004.10.008},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosh, Mukhopadhyay, Lu - 2006 - Bayesian analysis of zero-inflated regression models.pdf:pdf},
journal = {Journal of Statistical Planning and Inference},
keywords = {62E10,62F15,62P30 Keywords,Bayesian inference,Data augmentation,Gibbs sampling,MSC,Markov chain Monte Carlo,WinBUGS,Zero-inflated power series models},
pages = {1360--1375},
title = {{Bayesian analysis of zero-inflated regression models}},
volume = {136},
year = {2006}
}
@article{Ruli2013,
author = {Ruli, Erlis and Ventura, Laura},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruli, Ventura - 2013 - Modern Bayesian Inference in Zero-Inflated Poisson Models.pdf:pdf},
keywords = {asymptotic expansions,count data,likelihood,matching prior,modified profile,nuisance parameter,tail area probability,zip regression},
title = {{Modern Bayesian Inference in Zero-Inflated Poisson Models}},
year = {2013}
}
