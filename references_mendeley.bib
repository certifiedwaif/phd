@article{Albert1993,
author = {Albert, James H and Chib, Siddhartha},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Albert, Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data.pdf:pdf},
journal = {Source Journal of the American Statistical Association},
number = {422},
pages = {669--679},
publisher = {American Statistical Association},
title = {{Bayesian Analysis of Binary and Polychotomous Response Data}},
volume = {88},
year = {1993}
}

@book{Abramowitz1972,
	author = {Abramowitz, M. and Stegun, I. A.},
	title = {{Handbook of Mathematical Functions with Formulas,
	Graphs, and Mathematical Tables}},
	publisher = {New York: Dover Publications},
	year = {1972}
}



@article{Mitchell1988,
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	journal = {Journal of the American Statistical Association},
	pages = {1023--1032},
	title = {{Bayesian variable selection in linear regression}},
	volume = {83},
	year = {1988}
}

 
@ARTICLE{OrmerodEtal2017,
	author = {{Ormerod}, J.~T. and {Stewart}, M. and {Yu}, W. and {Romanes}, S.~E.
	},
	title = "{Bayesian hypothesis tests with diffuse priors: Can we have our cake and eat it too?}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1710.09146},
	primaryClass = "math.ST",
	keywords = {Mathematics - Statistics Theory},
	year = 2017,
	month = oct,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Cibulskis2013,
abstract = {Detection of somatic point substitutions is a key step in characterizing the cancer genome. However, existing methods typically miss low-allelic-fraction mutations that occur in only a subset of the sequenced cells owing to either tumor heterogeneity or contamination by normal cells. Here we present MuTect, a method that applies a Bayesian classifier to detect somatic mutations with very low allele fractions, requiring only a few supporting reads, followed by carefully tuned filters that ensure high specificity. We also describe benchmarking approaches that use real, rather than simulated, sequencing data to evaluate the sensitivity and specificity as a function of sequencing depth, base quality and allelic fraction. Compared with other methods, MuTect has higher sensitivity with similar specificity, especially for mutations with allelic fractions as low as 0.1 and below, making MuTect particularly useful for studying cancer subclones and their evolution in standard exome and genome sequencing data. Somatic single-nucleotide substitutions are an important and common mechanism for altering gene function in cancer. Yet they are difficult to identify. First, they occur at a very low frequency in the genome, ranging from 0.1 to 100 mutations per megabase (Mb), depending on tumor type 1–7 . Second, the alterations may be present only in a small fraction of the DNA molecules originating from the specific genomic locus for reasons including contaminating normal cells in the ana-lyzed sample, local copy-number variation in the cancer genome and presence of a mutation only in a subpopulation of the tumor cells 8–11 ('subclonality'). The fraction of DNA molecules harboring an altera-tion ('allelic fraction') has been reported to be as low as 0.05 for highly impure tumors 8 . The study of the subclonal structure of tumors is not only critical to understanding tumor evolution both in disease pro-gression and response to treatment 12 but also for developing reliable clinical diagnostic tools for personalized cancer therapy 13 . Recent reports on subclonal events in cancer have used three dif-ferent nonstandard experimental strategies: (i) analysis of clonal mutations present in several, but not all, of the metastases from the same patient, which suggested that these mutations were subclonal in the primary tumor 14 ; (ii) detection of subclonal mutations by ultra-deep sequencing 11 ; or (iii) sequencing of very small numbers of single cells 15–17 . In contrast, tens of thousands of tumors are being sequenced at standard depths of 100–150× for exomes and 30–60× for whole genomes as part of large-scale cancer genome projects, such as The Cancer Genome Atlas 1,2,7 and the International Cancer Genome Consortium 18 . To detect clonal and subclonal mutations present in these samples, one needs a highly sensitive and specific mutation-calling method. Although specificity can be controlled through subsequent experimental validation, this is an expensive and time-consuming step that is impractical for general application. The sensitivity and specificity of any somatic mutation–calling method varies along the genome and depends on several fac-tors, including the depth of sequence coverage in the tumor and a patient-matched normal sample, the local sequencing error rate, the allelic fraction of the mutation and the evidence thresholds used to declare a mutation. Characterizing how sensitivity and specificity depend on these factors is necessary for designing experiments with adequate power to detect mutations at a given allelic fraction, as well as for inferring the mutation frequency along the genome, which is a key parameter for understanding mutational processes and significance analysis 19,20 . To meet these critical needs of high sensitivity and specificity, which are not adequately addressed by available methods 21–23 , we developed a caller of somatic point mutations, MuTect. During its development, MuTect was used in many collaborative studies 1–4,7,19,24–35 . Here we describe the publicly available version of MuTect, including the ration-ale behind its different components. We also estimate its perform-ance as a function of the aforementioned factors using benchmarking approaches that, to our knowledge, have not been described before. The performance of the method is also supported by independent experi-mental validation in previous studies 3,4,7,19,24–30 as well as by its appli-cation to data sets analyzed in other publications 36,37 . We demonstrate that our method is several times more sensitive than other methods for low-allelic-fraction events while remaining highly specific, allowing for deeper exploration of the mutational landscape of highly impure tumor samples and of the subclonal evolution of tumors. MuTect is freely available for noncommercial use at http://www. broadinstitute.org/cancer/cga/mutect (Supplementary Data).},
author = {Cibulskis, Kristian and Lawrence, Michael S and Carter, Scott L and Sivachenko, Andrey and Jaffe, David and Sougnez, Carrie and Gabriel, Stacey and Meyerson, Matthew and Lander, Eric S and Getz, Gad},
doi = {10.1038/nbt.2514},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cibulskis et al. - 2013 - Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples.pdf:pdf},
journal = {Nature Biotechnology},
title = {{Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples}},
volume = {31},
year = {2013}
}

 

@book{Johnson1995,
	author = {Johnson, N. L. and Kotz, S. and Balakrishnan, N.},
	title = {Continuous Univariate Distributions, Volume 2 (2nd Edition)},
	year = {1995},
	publisher = {Wiley}
}


@Article{Hankin2006,
	title = {{Special functions in R: introducing the gsl package}},
	author = {Robin K. S. Hankin},
	journal = {R News},
	year = {2006},
	month = {October},
	volume = {6},
	issue = {4},
}

@Manual{Bove2013,
	title = {appell: Compute Appell's F1 hypergeometric function},
	author = {Bov\'e, D. S. and Colavecchia, F. D. and Forrey, R. C. and Gasaneo, G. and Michel, N. L. J.  and Shampine, L. F.  and Stoitsov, M. V.  and Watts, H. A.},
	year = {2013},
	note = {R package version 0.0-4},
	url = {https://CRAN.R-project.org/package=appell},
}


@Article{Young2010,
	title = {{tolerance}: An {R} Package for Estimating Tolerance Intervals},
	author = {Derek S. Young},
	journal = {Journal of Statistical Software},
	year = {2010},
	volume = {36},
	number = {5},
	pages = {1--39},
	url = {http://www.jstatsoft.org/v36/i05/},
}

@Manual{Clyde2017,
	title = {BAS: Bayesian Adaptive Sampling for Bayesian
	Model Averaging},
	author = {Merlise Clyde},
	year = {2017},
	note = {R package version 1.4.4},
}


@Manual{Garcia-Donato2016,
	title = {BayesVarSel: Bayes Factors, Model Choice and Variable Selection in Linear
	Models},
	author = {Gonzalo Garc\'{i}a-Donato and Anabel Forte},
	year = {2016},
	note = {R package version 1.7.0},
	url = {https://CRAN.R-project.org/package=BayesVarSel},
}

@Book{Venables2002,
	title = {Modern Applied Statistics with S},
	author = {W. N. Venables and B. D. Ripley},
	publisher = {Springer},
	edition = {Fourth},
	address = {New York},
	year = {2002}
}


@article {Bottolo2010,
	AUTHOR = {Bottolo, L. and Richardson, S.},
	TITLE = {Evolutionary stochastic search for {B}ayesian model exploration},
	JOURNAL = {Bayesian Analysis},
	VOLUME = {5},
	YEAR = {2010},
	PAGES = {583--618},
	NUMBER = {3}
}

@Article{Schafer2013,
	author="Sch{\"a}fer, Christian
	and Chopin, Nicolas",
	title="Sequential Monte Carlo on large binary sampling spaces",
	journal="Statistics and Computing",
	year="2013",
	volume="23",
	number="2",
	pages="163--184"
}

@article{Shi2011,
	title = "Bayesian variable selection via particle stochastic search",
	journal = "Statistics \& Probability Letters",
	volume = "81",
	number = "2",
	pages = "283--291",
	year = "2011",
	author = "Minghui Shi and David B. Dunson",
}

@article {Hans2007,
	AUTHOR = {Hans, C. and Dobra, A. and West, M.},
	TITLE = {Shotgun stochastic search for ``large {$p$}'' regression},
	JOURNAL = {Journal of the American Statistical Association},
	VOLUME = {102},
	YEAR = {2007},
	PAGES = {507--516},
	NUMBER = {478}   
}

@Article{Jasra2007,
	author="Jasra, Ajay
	and Stephens, David A.
	and Holmes, Christopher C.",
	title="On population-based simulation for static inference",
	journal="Statistics and Computing",
	year="2007",
	month="Sep",
	day="01",
	volume="17",
	number="3",
	pages="263--279"
}


@Manual{Croissant2016,
	title = {Ecdat: Data Sets for Econometrics},
	author = {Yves Croissant},
	year = {2016},
	note = {R package version 0.3-1},
	url = {https://CRAN.R-project.org/package=Ecdat},
}

 

@book{Erdelyi1953,
	author = {Erd\'{e}lyi, A. and  Magnus, W. and  Oberhettinger, F. and  Tricomi, F.G.},
	title = {Higher Transcendental Functions Volume I},
	year = {1953},
	publisher = {McGraw-Hill}, 
	address = {New York}
}



@Article{Zeugner2015,
	title = {Bayesian Model Averaging Employing Fixed and Flexible Priors: The {BMS} Package for {R}},
	author = {Stefan Zeugner and Martin Feldkircher},
	journal = {Journal of Statistical Software},
	year = {2015},
	volume = {68},
	number = {4},
	pages = {1--37},
	doi = {10.18637/jss.v068.i04},
}

@article{Tran2015,
abstract = {Variational Bayes (VB) is rapidly becoming a popular tool for Bayesian inference in statistical modeling. However, the existing VB algorithms are restricted to cases where the likelihood is tractable, which precludes the use of VB in many interesting models such as in state space models and in approximate Bayesian computation (ABC), where application of VB methods was previously impossible. This paper extends the scope of application of VB to cases where the likelihood is intractable, but can be estimated unbiasedly. The proposed VB method therefore makes it possible to carry out Bayesian inference in many statistical models, including state space models and ABC. The method is generic in the sense that it can be applied to almost all statistical models without requiring a model-based derivation, which is a drawback of many existing VB algorithms. We also show how the proposed method can be used to obtain highly accurate VB approximations of marginal posterior distributions.},
archivePrefix = {arXiv},
arxivId = {1503.08621},
author = {Tran, Minh-Ngoc and Nott, David J. and Kohn, Robert},
eprint = {1503.08621},
file = {:home/markg/Downloads/1503.08621.pdf:pdf},
keywords = {approximate bayesian computation,marginal likelihood,natural gradient,state space models,stochastic optimization},
pages = {28},
title = {{Variational Bayes with Intractable Likelihood}},
volume = {117546},
year = {2015}
}
@misc{,
file = {:home/markg/Documents/kass1995.pdf:pdf},
title = {{kass1995BayesFactors}}
}
@article{Pham,
abstract = {A fast mean field variational Bayes (MFVB) approach to nonparametric regression when the predictors are subject to classical measurement error is investigated. It is shown that the use of such technology to the measurement error setting achieves reasonable accuracy. In tandem with the methodological development, a customized Markov chain Monte Carlo method is developed to facilitate the evaluation of accuracy of the MFVB method.},
author = {Pham, Tung H and Ormerod, John T and Wand, M P},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham, Ormerod, Wand - Unknown - Mean Field Variational Bayesian Inference for Nonparametric Regression with Measurement Error.pdf:pdf},
keywords = {Markov chain Monte Carlo,Penalized splines,classical measurement error,variational approximations},
title = {{Mean Field Variational Bayesian Inference for Nonparametric Regression with Measurement Error}}
}
 
@article{BIMJ:BIMJ200390024,
author = {Yau, Kelvin K W and Wang, Kui and Lee, Andy H},
doi = {10.1002/bimj.200390024},
issn = {1521-4036},
journal = {Biometrical Journal},
keywords = {Count data,Generalised linear mixed models,Negative binomial,Poisson regression,Random effects,Zero-inflation},
number = {4},
pages = {437--452},
publisher = {WILEY-VCH Verlag},
title = {{Zero-Inflated Negative Binomial Mixed Regression Modeling of Over-Dispersed Count Data with Extra Zeros}},
volume = {45},
year = {2003}
}
@article{Wand2012,
abstract = {The ag{\'{e}}d number theoretic concept of continued fractions can enhance certain Bayesian computations. The crux of this claim is due to continued fraction representations of numerically challenging special function ratios that arise in Bayesian computing. Continued fraction approximation via Lentz's Algorithm often leads to efficient and stable computation of such quantities. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
author = {Wand, Matt P. and Ormerod, John T.},
doi = {10.1002/sta4.4},
file = {:home/markg/Downloads/Wand{\_}et{\_}al-2012-Stat.pdf:pdf},
issn = {20491573},
journal = {Stat},
keywords = {Hypergeometric functions,Mean field variational Bayes,Parabolic cylinder functions,Special functions,Variable selection,Variational approximations},
number = {1},
pages = {31--41},
title = {{Continued fraction enhancement of Bayesian computing}},
volume = {1},
year = {2012}
}

 

@article{Tan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1205.3906v3},
author = {Tan, Linda S. L. and Nott, David J.},
doi = {10.1214/13-STS418},
eprint = {arXiv:1205.3906v3},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2013 - Variational Inference for Generalized Linear Mixed Models Using Partially Noncentered Parametrizations.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Variational Bayes,and phrases,hierarchical centering,longitudinal data analysis,nonconjugate models,tional message passing,varia-,variati,variational bayes},
number = {2},
pages = {168--188},
title = {{Variational Inference for Generalized Linear Mixed Models Using Partially Noncentered Parametrizations}},
volume = {28},
year = {2013}
}
@article{Clausen1999,
abstract = {A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimiza- tion problems, are easy to state, and have a finite but usually very large number of feasible solutions. While some of these as e.g. the Shortest Path problem and the Minimum Spanning Tree problem have polynomial algo- ritms, the majority of the problems in addition share the property that no polynomial method for their solution is known. Examples here are vehicle routing, crew scheduling, and production planning. All of these problems are NP-hard. Branch and Bound (B{\&}B) is by far the most widely used tool for solv- ing large scale NP-hard combinatorial optimization problems. B{\&}B is, however, an algorithm paradigm, which has to be filled out for each spe- cific problem type, and numerous choices for each of the components ex- ist. Even then, principles for the design of efficient B{\&}B algorithms have emerged over the years. In this paper I review the main principles of B{\&}B and illustrate the method and the different design issues through three examples: the Sym- metric Travelling Salesman Problem, the Graph Partitioning problem, and the Quadratic Assignment problem.},
author = {Clausen, Jens},
doi = {10.1.1.5.7475},
file = {:home/markg/Downloads/b{\_}and{\_}b.pdf:pdf},
journal = {Department of Computer Science, University of {\ldots}},
pages = {1--30},
title = {{Branch and bound algorithms-principles and examples}},
year = {1999}
}
@article{Akaike1974,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/Documents/01100705.pdf:pdf},
isbn = {0018-9286 VO - 19},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
pmid = {1100705},
title = {{A New Look at the Statistical Model Identification}},
volume = {19},
year = {1974}
}
@misc{Tibshirani2001,
abstract = {We propose a method (the ‘gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
author = {Tibshirani, R and Walther, G and Hastie, T},
booktitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
doi = {10.1111/1467-9868.00293},
file = {:home/markg/Dropbox/Downloads/gap.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
pages = {411--423},
pmid = {306526},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
volume = {63},
year = {2001}
}
@article{Efron2013,
abstract = {Classical statistical theory ignores model selection in assessing estimation accuracy. Here we consider bootstrap methods for computing standard errors and confidence intervals that take model selection into account. The methodology involves bagging, also known as bootstrap smoothing, to tame the erratic discontinuities of selection-based estimators. A useful new formula for the accuracy of bagging then provides standard errors for the smoothed estimators. Two examples, nonparametric and parametric, are carried through in detail: a regression model where the choice of degree (linear, quadratic, cubic,. . . ) is determined by the Cp criterion, and a Lasso-based estimation problem.},
author = {Efron, Bradley},
doi = {10.1080/01621459.2013.823775},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 2013 - Estimation and Accuracy after Model Selection.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {abc intervals,bagging,bootstrap smoothing,c p,importance sam-,lasso,model averaging},
number = {October},
pages = {130725111823001},
pmid = {25346558},
title = {{Estimation and Accuracy after Model Selection}},
volume = {1459},
year = {2013}
}
@article{Bertsimas2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.03133v1},
author = {Bertsimas, Dimitris and King, Angela},
doi = {10.1214/15-AOS1388},
eprint = {arXiv:1507.03133v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsimas, King - 2014 - Best Subset Selection via a Modern Optimization Lens.pdf:pdf},
isbn = {0001415123},
pages = {1--63},
title = {{Best Subset Selection via a Modern Optimization Lens}},
year = {2014}
}
@article{Volinsky2000,
abstract = {We investigate the Bayesian Information Criterion (BIC) for variable selection in models for censored survival data. Kass and Wasserman (1995, Journal of the American Statistical Association 90, 928-934) showed that BIC provides a close approximation to the Bayes factor when a unit-information prior on the parameter space is used. We propose a revision of the penalty term in BIC so that it is defined in terms of the number of uncensored events instead of the number of observations. For a simple censored data model, this revision results in a better approximation to the exact Bayes factor based on a conjugate unit-information prior. In the Cox proportional hazards regression model, we propose defining BIC in terms of the maximized partial likelihood. Using the number of deaths rather than the number of individuals in the BIC penalty term corresponds to a more realistic prior on the parameter space and is shown to improve predictive performance for assessing stroke risk in the Cardiovascular Health Study.},
author = {Volinsky, C T and Raftery, a E},
doi = {10.1111/j.0006-341X.2000.00256.x},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Volinsky, Raftery - 2000 - Bayesian information criterion for censored survival models.pdf:pdf},
isbn = {0006-341X (Print)},
issn = {0006-341X},
journal = {Biometrics},
keywords = {bayes factor,cox proportional hazards model,exponential distribution,partial likelihood},
number = {1},
pages = {256--262},
pmid = {10783804},
title = {{Bayesian information criterion for censored survival models.}},
volume = {56},
year = {2000}
}
@article{Fouskakis2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.2442v1},
author = {Fouskakis, D and Ntzoufras, Ioannis and Draper, David},
doi = {10.1214/14-BA887},
eprint = {arXiv:1307.2442v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fouskakis, Ntzoufras, Draper - 2015 - Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {athens,athens 15780 greece,bayes factors,bayesian variable selection,d,department of mathematics,email fouskakis,expected-posterior priors,fouskakis is with the,gaussian linear,gr,math,models,national technical university of,ntua,power-prior,pus,training samples,unit-information prior,zografou cam-},
number = {1},
pages = {75--107},
title = {{Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models}},
volume = {10},
year = {2015}
}
@article{Johnstone2005,
abstract = {This paper explores a class of empirical Bayes methods for level-dependent threshold selection in wavelet shrinkage. The prior considered for each wavelet coefficient is a mixture of an atom of probability at zero and a heavy-tailed density. The mixing weight, or sparsity parameter, for each level of the transform is chosen by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold. Details of the calculations needed for implementing the procedure are included. In practice, the estimates are quick to compute and there is software available. Simulations on the standard model functions show excellent performance, and applications to data drawn from various fields of application are used to explore the practical performance of the approach. By using a general result on the risk of the corresponding marginal maximum likelihood approach for a single sequence, overall bounds on the risk of the method are found subject to membership of the unknown function in one of a wide range of Besov classes, covering also the case of f of bounded variation. The rates obtained are optimal for any value of the parameter p in (0,$\backslash$infty], simultaneously for a wide range of loss functions, each dominating the L{\_}q norm of the $\backslash$sigmath derivative, with $\backslash$sigma$\backslash$ge0 and 0{\textless}q$\backslash$le2.},
archivePrefix = {arXiv},
arxivId = {math/0508281},
author = {Johnstone, Iain M. and Silverman, Bernard W.},
doi = {10.1214/009053605000000345},
eprint = {0508281},
file = {:home/markg/Documents/0508281.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Adaptivity,Bayesian inference,Nonparametric regression,Smoothing,Sparsity},
number = {4},
pages = {1700--1752},
primaryClass = {math},
title = {{Empirical bayes selection of wavelet thresholds}},
volume = {33},
year = {2005}
}
@article{Wand2014,
abstract = {Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very efficiently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit.},
author = {Wand, Matt P.},
file = {:home/markg/Downloads/wand14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {bayesian computing,field variational bayes,graphical models,matrix differential calculus,mean,variational approximation},
pages = {1351--1369},
title = {{Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing}},
volume = {15},
year = {2014}
}
@article{Society2016,
author = {Jeffreys, Harold},
file = {:home/markg/Downloads/97883.pdf:pdf},
number = {1670},
pages = {397--417},
title = {{Spectroscopic Mode Gr{\"{u}}neisen Parameters for Diamond Author ( s ): B . J . Parsons Source : Proceedings of the Royal Society of London . Series A , Mathematical and Physical Published by : Royal Society Stable}},
volume = {352},
year = {1946}
}
@article{Hoeting1999,
abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of- sample predictive performance. We also provide a catalogue of currently available BMA software.},
author = {Hoeting, Jennifer a and Madigan, David and Raftery, Adrian E and Volinsky, C T},
doi = {10.2307/2676803},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoeting et al. - 1999 - Bayesian model averaging a tutorial.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayesian graphical models,Bayesian model averaging,Markov chain Monte Carlo.,learning,model uncertainty},
number = {4},
pages = {382--417},
title = {{Bayesian model averaging: a tutorial}},
volume = {14},
year = {1999}
}
@article{Braun2008,
abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate infer-ence possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demon-strate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
author = {Braun, Michael and Mcauliffe, Jon},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braun, Mcauliffe - 2008 - Variational inference for large-scale models of discrete choice.2526:2526},
title = {{Variational inference for large-scale models of discrete choice}},
year = {2008}
}
@article{Jang2006,
abstract = {The penalized quasi-likelihood (PQL) approach is the most common estimation procedure for the generalized linear mixed model (GLMM). However, it has been noticed that the PQL tends to underestimate variance components as well as regression coefficients in the previous literature. In this paper, we numerically show that the biases of variance component estimates by PQL are systematically related to the biases of regression coefficient estimates by PQL, and also show that the biases of variance component estimates by PQL increase as random effects become more heterogeneous.},
author = {Jang, W. and Lim, J.},
file = {:home/markg/Dropbox/Downloads/05-21.pdf:pdf},
keywords = {and phrases,generalized linear mixed models,heterogeneity,penalized quasi-likelihood estimator,variance components},
pages = {05--21},
title = {{PQL Estimation Biases in Generalized Linear Mixed Models}},
year = {2006}
}
@article{Honkela2010,
abstract = {Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.},
author = {Honkela, Antti and Raiko, Tapani and Kuusela, Mikael and Tornio, Matti and Karhunen, Juha},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Honkela et al. - 2010 - Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {5},
pages = {3235--3268},
title = {{Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes}},
volume = {11},
year = {2010}
}
@article{Ood2006,
author = {Ood, S Imon N W},
doi = {10.1111/j.1467-842X.2006.00450.x},
file = {:home/markg/Downloads/j.1467-842X.2006.00450.x.pdf:pdf},
issn = {1369-1473},
keywords = {bayesian confidence interval,gam,gcv,generalized additive model,generalized cross,multiple smoothing parameters,penalized regression spline,validation},
number = {4},
pages = {445--464},
title = {{ON CONFIDENCE INTERVALS FOR GENERALIZED ADDITIVE MODELS BASED ON PENALIZED REGRESSION SPLINES University of Bath}},
volume = {48},
year = {2006}
}
@article{Muller2016,
author = {M{\"{u}}ller, Samuel and Welsh, A H},
doi = {10.1198/016214505000000529},
file = {:home/markg/Documents/27590673.pdf:pdf},
keywords = {bootstrap model selection,outlier,robust model selection,schwarz bayesian information criterion,stratified bootstrap},
number = {March},
pages = {1297--1310},
title = {{Outlier Robust Model Selection in Linear Regression Outlier Robust Model Selection in Linear Regression}},
volume = {1459},
year = {2016}
}
@article{Knowles2011,
abstract = {Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.},
author = {Knowles, David and Minka, Thomas P.},
file = {:home/markg/Downloads/KnoMin11.pdf:pdf},
isbn = {9781618395993},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {1--9},
title = {{Non-conjugate variational message passing for multinomial and binary regression}},
year = {2011}
}
@article{Congress,
author = {Congress, International and Lindley, D V},
file = {:home/markg/Documents/44-3-4-533.pdf:pdf},
journal = {Methods},
number = {3},
pages = {533--534},
title = {{Miscellanea 533}}
}

@misc{Tibshirani1996,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
eprint = {11/73273},
file = {:home/markg/Documents/lasso.pdf:pdf},
isbn = {0849320240},
issn = {00359246},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression Selection and Shrinkage via the Lasso}},
volume = {58},
year = {1996}
}
@article{Opper2009,
abstract = {The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an Omicron(N)(2) number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually Omicron(N). The approach is applied to gaussian process regression with nongaussian likelihoods.},
author = {Opper, Manfred and Archambeau, C{\'{e}}dric},
doi = {10.1162/neco.2008.08-07-592},
file = {:home/markg/Downloads/neco{\_}mo09{\_}web.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
number = {3},
pages = {786--792},
pmid = {18785854},
title = {{The variational gaussian approximation revisited.}},
volume = {21},
year = {2009}
}
@article{Lee2015,
author = {Lee, Cathy Yuen Yi and Wand, Matt P.},
doi = {10.1002/sim.6737},
file = {:home/markg/Downloads/sim6737.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {10.1002/sim.6737 and Bayesian inference,Markov chain Monte Carlo,approximation,bayesian inference,carlo,group-specific curves,longitudinal and multilevel data,markov chain monte,mean field variational Bayes approximation,mean field variational bayes,semiparametric regression},
number = {August},
pages = {n/a--n/a},
title = {{Variational methods for fitting complex Bayesian mixed effects models to health data}},
year = {2015}
}
@article{Knight2000,
author = {Knight, Keith and Fu, Wenjiang},
file = {:home/markg/Documents/euclid.aos.1015957397.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1356--1378},
title = {{ASYMPTOTICS FOR LASSO-TYPE ESTIMATORS By Keith Knight 1 and Wenjiang Fu 2}},
volume = {28},
year = {2000}
}
@article{Petersen2012,
abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
doi = {10.1111/j.1365-294X.2006.03161.x},
file = {:home/markg/Downloads/matrixcookbook.pdf:pdf},
isbn = {0962-1083 (Print)$\backslash$r0962-1083 (Linking)},
issn = {09621083},
journal = {Citeseer},
keywords = {acknowledgements,and suggestions,bill baxter,christian rish{\o}j,contributions,derivative of,derivative of inverse matrix,determinant,di erentiate a matrix,douglas l,esben,matrix algebra,matrix identities,matrix relations,thank the following for,theobald,we would like to},
pages = {1--66},
pmid = {17284204},
title = {{The Matrix Cookbook}},
year = {2012}
}
@article{George2000,
abstract = {For the problem of variable selection for the normal linear model, selection criteria such as AIC, Cp, BIC and RIC have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed-penalty criterion across a variety of orthogonal and nonorthogonal set-ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed.},
author = {George, Edward I and Foster, Dean P},
doi = {10.1093/biomet/87.4.731},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Foster - 2000 - Calibration and Empirical Bayes Variable Selection.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {731--747},
title = {{Calibration and Empirical Bayes Variable Selection}},
volume = {87},
year = {2000}
}
@article{Breiman1985,
abstract = {In regression analysis the response variable Y and the predictor variables X1, ⋯, Xp are often replaced by functions $\theta$(Y) and $\phi$1(X1), ⋯, $\phi$p(Xp). We discuss a procedure for estimating those functions $\theta$* and $\phi$*1, ⋯, $\phi$*p that minimize e{\^{}}2 = E{\{}$\backslash$lbrack $\theta$(Y) - $\backslash$sum{\^{}}p{\_}{\{}j = 1{\}} $\phi${\_}j(X{\_}j) $\backslash$rbrack{\^{}}2{\}}/ $\backslash$operatorname{\{}var{\}} $\backslash$lbrack$\theta$(Y) $\backslash$rbrack, given only a sample {\{}(yk, xk1, ⋯, xkp), 1 ≤ k ≤ N{\}} and making minimal assumptions concerning the data distribution or the form of the solution functions. For the bivariate case, p = 1, $\theta$* and $\phi$* satisfy $\rho$* = $\rho$($\theta$*, $\phi$*) = max$\theta$,$\phi$$\rho$[$\theta$(Y), $\phi$(X)], where $\rho$ is the product moment correlation coefficient and $\rho$* is the maximal correlation between X and Y. Our procedure thus also provides a method for estimating the maximal correlation between two variables.},
author = {Breiman, Leo and Friedman, Jerome H},
doi = {10.1080/01621459.1985.10478157},
file = {:home/markg/Documents/2288473.pdf:pdf},
isbn = {01621459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {smoothing},
number = {391},
pages = {580--598},
title = {{Estimating Optimal Transformations for Multiple Regression and Correlation}},
volume = {80},
year = {1985}
}
@article{Nan2014,
abstract = {Many exciting results have been obtained on model selection for high-dimensional data in both efficient algorithms and theoretical developments. The powerful penalized regression methods can give sparse representations of the data even when the number of predictors is much larger than the sample size. One important question then is: How do we know when a sparse pattern identified by such a method is reliable? In this work, besides investigating instability of model selection methods in terms of variable selection, we propose variable selection deviation measures that give one a proper sense on how many predictors in the selected set are likely trustworthy in certain aspects. Simulation and a real data example demonstrate the utility of these measures for application.},
author = {Nan, Ying and Yang, Yuhong},
doi = {10.1080/10618600.2013.829780},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nan, Yang - 2014 - Variable Selection Diagnostics Measures for High-Dimensional Regression.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Model selection diagnostics,Model selection instability,Variable selection deviation},
number = {3},
pages = {636--656},
title = {{Variable Selection Diagnostics Measures for High-Dimensional Regression}},
volume = {23},
year = {2014}
}
@article{Hansen2001,
author = {Hansen, Mark H and Yu, B},
doi = {10.1198/016214501753168398},
file = {:home/markg/Documents/2670311.pdf:pdf},
isbn = {0162145017531},
issn = {0162-1459},
journal = {Journal of the American Statistical {\ldots}},
keywords = {aic,bayesian methods},
number = {454},
pages = {746--774},
title = {{Model Selection and the Principle of Minimum Description Length}},
volume = {96},
year = {2001}
}
@book{Boyd2010,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:home/markg/Downloads/bv{\_}cvxbook.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
volume = {25},
year = {2010}
}
@article{Wood2006,
abstract = {A general method for constructing low-rank tensor product smooths for use as components of generalized additive models or generalized additive mixed models is presented. A penalized regression approach is adopted in which tensor product smooths of several variables are constructed from smooths of each variable separately, these "marginal" smooths being represented using a low-rank basis with an associated quadratic wiggliness penalty. The smooths offer several advantages: (i) they have one wiggliness penalty per covariate and are hence invariant to linear rescaling of covariates, making them useful when there is no "natural" way to scale covariates relative to each other; (ii) they have a useful tuneable range of smoothness, unlike single-penalty tensor product smooths that are scale invariant; (iii) the relatively low rank of the smooths means that they are computationally efficient; (iv) the penalties on the smooths are easily interpretable in terms of function shape; (v) the smooths can be generated completely automatically from any marginal smoothing bases and associated quadratic penalties, giving the modeler considerable flexibility to choose the basis penalty combination most appropriate to each modeling task; and (vi) the smooths can easily be written as components of a standard linear or generalized linear mixed model, allowing them to be used as components of the rich family of such models implemented in standard software, and to take advantage of the efficient and stable computational methods that have been developed for such models. A small simulation study shows that the methods can compare favorably with recently developed smoothing spline ANOVA methods.},
author = {Wood, Simon N.},
doi = {10.1111/j.1541-0420.2006.00574.x},
file = {:home/markg/Downloads/j.1541-0420.2006.00574.x.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
keywords = {Computationally efficient,Generalized additive mixed model (GAMM),Mixed effect variable coefficient model,Multiple penalties,Penalized regression,SS-ANOVA,Scale invariant,Smooth interaction,Smoothing penalty,Spline,Tensor product smooth},
number = {4},
pages = {392},
pmid = {17156276},
title = {{Low-rank scale-invariant tensor product smooths for generalized additive mixed models}},
volume = {62},
year = {2006}
}
@article{Miller1984,
abstract = {Computational algorithms for selecting subsets of regression variables are discussed. Only linear models and the least-squares criterion are considered. The use of planar- rotation algorithms, instead of Gauss-Jordan methods, is advocated. The advantages and disadvantages of a number of "cheap" search methods are described for use when it is not feasible to carry out an exhaustive search for the best-fitting subsets. Hypothesis testing for three purposes is considered, namely (i) testing for zero regression coefficients for remaining variables, (ii) comparing subsets and (iii) testing for any predictive value in a selected subset. Three small data sets are used to illustrate these tests. Spjotvoll's (1972a) test is discussed in detail, though an extension to this test appears desirable. Estimation problems have largely been overlooked in the past. Three types of bias are identified, namely that due to the omission of variables, that due to competition for selection and that due to the stopping rule. The emphasis here is on competition bias, which can be of the order of two or more standard errors when coefficients are estimated from the same data as were used to select the subset. Five possible ways of handling this bias are listed. This is the area most urgently requiring further research. Mean squared errors of prediction and stopping rules are briefly discussed. Com- petition bias invalidates the use of existing stopping rules as they are commonly applied to try to produce optimal prediction equations},
author = {Miller, By Alan J},
doi = {10.2307/2981576},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1984 - Selection of Subsets of Regression Variables(2).pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
keywords = {CONDITIONAL LIKELIHOOD,CRITERIA,LEAST SQUARES,MALLOWS' C AKAIKE'S INFORMATION,MEAN SQUARED ERRORS OF PREDICTION,MULTIPLE REGRESSION,PREDICTION,STEPWISE REGRESSION,SUBSET SELECTION,VARIABLE SELECTION},
number = {3},
pages = {389--425},
title = {{Selection of Subsets of Regression Variables}},
volume = {147},
year = {1984}
}
@article{Garay2015,
abstract = {In recent years, there has been considerable interest in regression models based on zero-inflated distribu-tions. These models are commonly encountered in many disciplines, such as medicine, public health, and environmental sciences, among others. The zero-inflated Poisson (ZIP) model has been typically consid-ered for these types of problems. However, the ZIP model can fail if the non-zero counts are overdispersed in relation to the Poisson distribution, hence the zero-inflated negative binomial (ZINB) model may be more appropriate. In this paper, we present a Bayesian approach for fitting the ZINB regression model. This model considers that an observed zero may come from a point mass distribution at zero or from the negative binomial model. The likelihood function is utilized to compute not only some Bayesian model selection measures, but also to develop Bayesian case-deletion influence diagnostics based on q-divergence measures. The approach can be easily implemented using standard Bayesian software, such as WinBUGS. The performance of the proposed method is evaluated with a simulation study. Further, a real data set is analyzed, where we show that ZINB regression models seems to fit the data better than the Poisson counterpart.},
author = {Garay, Aldo M and Lachos, Victor H and Bolfarine, Heleno and Paulo, S{\~{a}}o},
doi = {10.1080/02664763.2014.995610},
file = {:home/markg/Downloads/ZINB{\_}BayesianoRES.pdf:pdf},
issn = {0266-4763},
journal = {Journal of Applied Statistics},
keywords = {Bayesian inference,MCMC,binomial negative distribution,q-divergence measures,zero-inflated models},
number = {6},
pages = {1148--1165},
title = {{Bayesian estimation and case influence diagnostics for the zero-inflated negative binomial regression model}},
volume = {42},
year = {2015}
}
@article{Wood2008a,
abstract = {Conventional smoothing methods sometimes perform badly when used to smooth data over complex domains, by smoothing inappropriately across boundary features, such as peninsulas. Solutions to this smoothing problem tend to be computationally complex, and not to provide model smooth functions which are appropriate for incorporating as components of other models, such as generalized additive models or mixed additive models. We propose a class of smoothers that are appropriate for smoothing over difficult regions of R2 which can be represented in terms of a low rank basis and one or two quadratic penalties. The key features of these smoothers are that they do not `smooth across' boundary features, that their representation in terms of a basis and penalties allows straightforward incorporation as components of generalized additive models, mixed models and other non-standard models, that smoothness selection for these model components is straightforward to accomplish in a computationally efficient manner via generalized cross-validation, Akaike's information criterion or restricted maximum likelihood, for example, and that their low rank means that their use is computationally efficient.},
author = {Wood, Simon N. and Bravington, Mark V. and Hedley, Sharon L.},
doi = {10.1111/j.1467-9868.2008.00665.x},
file = {:home/markg/Downloads/j.1467-9868.2008.00665.x.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Basis penalty smooth,Differential equation smoothing,FELSPLINE,Finite window smoothing,Known boundary smoothing,Spline},
number = {5},
pages = {931--955},
title = {{Soap film smoothing}},
volume = {70},
year = {2008}
}
@article{Wand2011,
abstract = {We develop strategies for mean field variational Bayes approximate inference for Bayesian hierarchical models containing elaborate distributions. We loosely define elaborate distributions to be those having more complicated forms compared with common distributions such as those in the Normal and Gamma families. Examples are Asymmetric Laplace, Skew Normal and Generalized Ex- treme Value distributions. Such models suffer from the difficulty that the parameter updates do not admit closed form solutions. We circumvent this problem through a combination of (a) specially tailored auxiliary variables, (b) univariate quadrature schemes and (c) finite mixture approximations of troublesome density functions. An accuracy assessment is conducted and the new methodology is illustrated in an application.},
author = {Wand, Matthew P. and Ormerod, John T. and Padoan, Simone A. and Fr??hrwirth, Rudolf},
doi = {10.1214/11-BA631},
file = {:home/markg/Dropbox/Downloads/euclid.ba.1339616546.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Auxiliary mixture sampling,Bayesian inference,Quadrature,Variational methods},
number = {4},
pages = {847--900},
title = {{Mean field variational bayes for elaborate distributions}},
volume = {6},
year = {2011}
}
@misc{Mainland2013,
abstract = {Stream fusion [6] is a powerful technique for automatically trans- forming high-level sequence-processing functions into efficient im- plementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all. In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together mul- tiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are im- plemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C. Categories},
author = {Mainland, Geoffrey and Leshchinskiy, Roman and {Peyton Jones}, Simon},
booktitle = {the 18th ACM SIGPLAN international conference},
doi = {10.1145/2500365.2500601},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mainland, Leshchinskiy, Peyton Jones - 2013 - Exploiting Vector Instructions with Generalized Stream Fusion.pdf:pdf},
isbn = {9781450323260},
issn = {15232867},
title = {{Exploiting Vector Instructions with Generalized Stream Fusion}},
year = {2013}
}
@article{Challis2013,
abstract = {We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the fol-lowing novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate in-ference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.},
author = {Challis, Edward and Barber, David},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Challis, Barber DBARBER - 2013 - Gaussian Kullback-Leibler Approximate Inference.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Gaussian processes,active learning,experimental design,generalised linear models,large scale inference,latent linear models,sparse learning,variational approximate inference},
pages = {2239--2286},
title = {{Gaussian Kullback-Leibler Approximate Inference}},
volume = {14},
year = {2013}
}
@article{S.2000,
author = {S. and Wood, N},
file = {:home/markg/Downloads/mspfinal.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society Series B},
keywords = {generalized additive models,generalized cross-validation,generalized ridge re-,gression,likelihood,model selection,multiple smoothing parameters,non-linear modelling,penalized,penalized regression splines},
number = {2},
pages = {413--428},
title = {{Modelling and smoothing parameter estimation with multiple quadratic penalties}},
volume = {62},
year = {2000}
}
@article{George2000a,
abstract = {The problem of variable selection is one of the most pervasive model selection problems in statistical applications. Often referred to as the problem of subset selection, it arises when one wants to model the relationship between a variable of interest and a subset of potential explanatory variables or predictors, but there is uncertainty about which subset to use. This vignette reviews some of the key developments that have led to the wide variety of approaches for this problem.},
author = {George, Edward I and George, Edward},
doi = {10.2307/2669776},
file = {:home/markg/Documents/The Variable Selection Problem.pdf:pdf},
isbn = {0162-1459},
issn = {01621459 (ISSN)},
journal = {Journal of the American Statistical Association},
number = {452},
pages = {1304--1308},
title = {{The Variable Selection Problem}},
volume = {95},
year = {2000}
}
 
@misc{Geweke1996,
author = {Geweke, J},
booktitle = {Bayesian Statistics 5 (Edited by Bernardo, J.M., Berger, J. O., Dawid, A. P. and Smith, A. F. M.),},
file = {:home/markg/Documents/wp539.pdf:pdf},
pages = {609--620},
title = {{Variable selection and model comparison in regression}},
year = {1996}
}
@article{Bleier2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0412v1},
author = {Bleier, Arnim},
eprint = {arXiv:1312.0412v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bleier - 2013 - Practical Collapsed Stochastic Variational Inference for the HDP.pdf:pdf},
journal = {Proceedings of the NIPS workshop on topic models},
pages = {arXiv:1312.0412 [cs.LG]},
title = {{Practical Collapsed Stochastic Variational Inference for the HDP}},
year = {2013}
}
@article{Ormerod2012,
abstract = {Variational approximation methods have become a mainstay of contemporary machine learning methodology, but currently have little presence in statistics. We devise an effective variational approximation strategy for fitting generalized linear mixed models (GLMMs) appropriate for grouped data. It involves Gaussian approximation to the distributions of random effects vectors, conditional on the responses. We show that Gaussian variational approximation is a relatively simple and natural alternative to Laplace approximation for fast, non-Monte Carlo, GLMM analysis. Numerical studies show Gaussian variational approximation to be very accurate in grouped data GLMM contexts. Finally, we point to some recent theory on consistency of Gaussian variational approximation in this context. Supplemental materials are available online.},
author = {Ormerod, J. T. and Wand, M. P.},
doi = {10.1198/jcgs.2011.09118},
file = {:home/markg/Downloads/Ormerod11.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {best prediction,likelihood-based inference,longitudinal data analysis,machine learning,variance components},
number = {1},
pages = {2--17},
title = {{Gaussian Variational Approximate Inference for Generalized Linear Mixed Models}},
volume = {21},
year = {2012}
}
@article{Saul1995,
author = {Saul, Lawrence K and Jordan, Michael I},
file = {:home/markg/Downloads/1155-exploiting-tractable-substructures-in-intractable-networks.pdf:pdf},
title = {{Exploiting Tractable Substructures in Intractable Networks}},
year = {1995}
}
@article{Zellner2004,
author = {Zellner, Dietmar and Zellner, Dietmar and Keller, Frieder and Keller, Frieder and Zellner, G{\"{u}}nter E. and Zellner, G{\"{u}}nter E.},
doi = {10.1081/SAC-200033363},
file = {:home/markg/Downloads/sac-200033363.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {3},
pages = {787--805},
title = {{Variable Selection in Logistic Regression Models}},
volume = {33},
year = {2004}
}
@book{Nocedal2006,
author = {Nocedal, Jorge and Wright, Stephen},
publisher = {Springer Science {\&} Business Media},
title = {{Numerical optimization}},
year = {2006}
}
@misc{,
file = {:home/markg/Documents/rmh1997.pdf:pdf},
title = {rmh1997.pdf}
}
@article{Raftery1997,
author = {Raftery, A E and Madigan, D and Hoeting, J A},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery, Madigan, Hoeting - 1997 - Bayesian Model Averaging for Linear Regression.pdf:pdf},
journal = {Journal of the American Statistical Association},
pages = {179--191},
title = {{Bayesian Model Averaging for Linear Regression}},
volume = {92},
year = {1997}
}

@article{Huang2007,
	title={Bayesian inference of Micro{RNA} targets from sequence and expression data},
	author={Huang, J. C. and Morris, Q. D. and Frey, B. J.},
	journal={Journal of Computational Biology},
	volume={14},
	pages={550--563},
	year={2007},
	number={5} 
}
 
@article{Goel1986,
author = {Goel, P. and Zellner, A.},
journal = {Studies in Bayesian Econometrics},
pages = {233--243},
title = {{On Assessing Prior Distributions and Bayesian Regression Analysis with g Prior Distributions}},
volume = {6},
year = {1986}
}



@article{Logsdon2010,
	author = {Logsdon, B. A. and Hoffman, G. E. and Mezey, J. G.},
	title = {{A variational Bayes algorithm for fast
	and accurate multiple locus genome-wide association analysis}},
	journal = {BMC Bioinformatics},
	volume = {11},
	pages = {1--13},
	year = {2010},
	number = {58}    
}


@article{Carbonetto2011,
	author = {Carbonetto, P. and Stephens, M.},
	title = {Scalable variational inference for Bayesian variable
	selection in regression, and its accuracy in genetic association studies},
	journal = {Bayesian Analysis},
	volume = {6},
	pages = {1--42},
	year = {2011},    
	number = {4}
}

@article{ormerod2017,
	author = {Ormerod, John T. and You, Chong and M\"uller, Samuel},
	doi = "10.1214/17-EJS1332",
	fjournal = "Electronic Journal of Statistics",
	journal = "Electron. J. Statist.",
	number = "2",
	pages = "3549--3594",
	publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
	title = "A variational Bayes approach to variable selection",
	volume = "11",
	year = "2017"
}


@article{wandormerod2011,
	author = {Wand, M. P. and Ormerod, J. T.},
	journal = {Electronic Journal of Statistics},
	pages = {1654--1717},
	title = {Penalized wavelets: Embedding wavelets into semiparametric regression},
	volume = {5},
	year = {2011}
}

 
@article{Zellner1986,
	author = {Zellner, A.},
	title = {{On assessing prior distributions and Bayesian regression analysis with $g$-prior distributions}},
	journal = {Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti},
	pages = {233--243},
	year = {1986}
}


@Article{Tan2018,
	author="Tan, Linda S. L.
	and Nott, David J.",
	title="Gaussian variational approximation with sparse precision matrices",
	journal="Statistics and Computing",
	year="2018",
	month="Mar",
	day="01",
	volume="28",
	number="2",
	pages="259--275",
	abstract="We consider the problem of learning a Gaussian variational approximation to the posterior distribution for a high-dimensional parameter, where we impose sparsity in the precision matrix to reflect appropriate conditional independence structure in the model. Incorporating sparsity in the precision matrix allows the Gaussian variational distribution to be both flexible and parsimonious, and the sparsity is achieved through parameterization in terms of the Cholesky factor. Efficient stochastic gradient methods that make appropriate use of gradient information for the target distribution are developed for the optimization. We consider alternative estimators of the stochastic gradients, which have lower variation and are more stable. Our approach is illustrated using generalized linear mixed models and state-space models for time series.",
	issn="1573-1375",
	doi="10.1007/s11222-017-9729-7"
}




@article{Zellner1980,
author = {Zellner, A and Siow, A},
journal = {Bayesian Statistics},
keywords = {Bayesian odds ratios, hypothesis testing, regression hypotheses},
number = {1978},
pages = {585--648},
title = {{Posterior odds ratio for selected regression hypothesis}},
year = {1980}
}
 
@article{Shankar1997829,
author = {Shankar, V and Milton, J and Mannering, F},
doi = {http://dx.doi.org/10.1016/S0001-4575(97)00052-3},
issn = {0001-4575},
journal = {Accident Analysis and Prevention},
keywords = {Accident frequency,Poisson regression,Zero-inflated count models},
number = {6},
pages = {829--837},
title = {{Modeling accident frequencies as zero-altered probability processes: An empirical inquiry}},
volume = {29},
year = {1997}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - WandOrmerod08.rs:rs},
title = {{WandOrmerod08}}
}
@article{Tierney1989a,
author = {Tierney, Luke and Kass, Robert E. and Kadane, Joseph B.},
doi = {10.1093/biomet/76.3.425},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Kass, Kadane - 1989 - Approximate marginal densities of nonlinear functions.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Asymptotic normality,Laplace's method,Saddlepoint method},
number = {3},
pages = {425--433},
title = {{Approximate marginal densities of nonlinear functions}},
volume = {76},
year = {1989}
}
 
@article{Clarke2006,
abstract = {The distribution and biomass of phytoplankton in the upper layers of the ocean are important indicators of productivity and carbon cycling. Large scale perturbations in phytoplankton are linked to global climate change, so accurate monitoring is increasingly important. The chlorophyll-a pigment concentration in the water is routinely measured as an index of algal biomass. Direct water sampling from ships and moorings provides accurate data, but woefully poor spatial and temporal coverage of the oceans. In contrast, multispectral sea surface reflectance data from orbiting satellite-borne sensors, which in principle can be used to derive pigment concentration, give the prospect of globally detailed spatial and temporal coverage. Unfortunately, there are some locally variable confounding factors, which the algorithms for converting reflectance data to ocean chlorophyll-a concentration do not take into account. Hence, statistical methods are needed to obtain accurate predictions of chlorophyll-a. concentration by using data from both these sources. We use penalized regression splines to model water sample data as a three-dimensional function of satellite measurements, seabed depth and time of year. The models are effectively complex calibrations of the satellite data against the bottle data. We compare the results by using thin plate regression splines and tensor product splines using generalized cross-validation to choose the relative amounts of smoothing for each of the covariates. Since the thin plate spline penalty functional is isotropic, this requires the introduction of two scaling parameters, which are also chosen by generalized cross-validation, to scale the covariates relatively to one another. The tensor product spline smooths each covariate appropriately by use of separate smoothing parameters for each covariate. The models are tested by application to data from the north-east Atlantic, first randomly subsampling the data to achieve even coverage over the entire region. Both approaches perform equally well, achieving R-2 approximate to 65{\%}, both for the data that are used to fit the model and for a validation data set. Of particular concern in this application is that monthly predictions from the models should be biologically plausible over the whole region, describing the broad regional features that are apparent in the satellite data and extrapolating sensibly where satellite data are not available. To achieve this, the satellite data must be one of the covariates in the model; spatiotemporal covariates alone are not sufficient to extrapolate sensibly into areas where no data are available.},
author = {Clarke, E. D. and Speirs, D. C. and Heath, M. R. and Wood, S. N. and Gurney, W. S. C. and Holmes, S. J.},
doi = {10.1111/j.1467-9876.2006.00540.x},
file = {:home/markg/Downloads/j.1467-9876.2006.00540.x.pdf:pdf},
issn = {0035-9254},
journal = {Journal of the Royal Statistical Society Series C-Applied Statistics},
pages = {331--353},
title = {{Calibrating remotely sensed chlorophyll-a data by using penalized regression splines}},
volume = {55},
year = {2006}
}
@article{Saul1996,
abstract = {We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition--the classification of handwritten digits.},
archivePrefix = {arXiv},
arxivId = {http://arxiv.org/pdf/cs/9603102.pdf},
author = {Saul, L K and Jaakkola, T and Jordan, M I},
doi = {10.1.1.54.4128},
eprint = {/arxiv.org/pdf/cs/9603102.pdf},
file = {:home/markg/Downloads/live-251-1520-jair.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
number = {1570},
pages = {61--76},
primaryClass = {http:},
title = {{Mean Field Theory for Sigmoid Belief Networks}},
volume = {4},
year = {1996}
}
@article{Million2007,
author = {Million, Elizabeth and Million, Elizabeth},
file = {:home/markg/Downloads/million-paper.pdf:pdf},
pages = {1--7},
title = {{The Hadamard Product}},
year = {2007}
}

@book{Gradshteyn2007,
author = {Gradshteyn, I.S. and Ryzhik, I.M.},
title = {Tables of Integrals, Series, and Products},
publisher = {Academic Press},
editors = {Jeffrey, A. and Zwillinger, D.},
year = {2007}
}


@article{Rockova2014,
	author = {Ro\v{c}kov\'a, V. and George, E. I.},
	title = {{EMVS: The EM approach to Bayesian variable selection}},
	journal = {Journal of the American Statistical Association},
	year = {2014},
	volume = {109},
	pages = {828-846},  
	number = {506}     
}

@Article{HernandezLobato2015, 
	author="Hern{\'a}ndez-Lobato, Jos{\'e} Miguel
	and Hern{\'a}ndez-Lobato, Daniel
	and Su{\'a}rez, Alberto",
	title="Expectation propagation in linear regression models with spike-and-slab priors",
	journal="Machine Learning",
	year="2015",
	volume="99",
	number="3",
	pages="437--487"
}

@article{CarlinChib1995,
	author = {Bradley P. Carlin and Siddhartha Chib},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {3},
	pages = {473-484},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Bayesian Model Choice via Markov Chain Monte Carlo Methods},
	volume = {57},
	year = {1995}
}

@article{OHara2009,
	title={{A review of Bayesian variable selection methods: what, how and which}},
	author={O'Hara, R. B. and Sillanp{\"a}{\"a}, M. J.},
	journal={Bayesian Analysis},
	volume={4},
	pages={85--117},
	year={2009},
	number={1}
}

@article{ClydeEtal2011,
	author = {Merlise A. Clyde and Joyee Ghosh and Michael L. Littman},
	title = {Bayesian Adaptive Sampling for Variable Selection and Model Averaging},
	journal = {Journal of Computational and Graphical Statistics},
	volume = {20},
	number = {1},
	pages = {80-101},
	year  = {2011}	
}




@article{Nott2004,
	author = {David J. Nott and Daniela Leonte},
	journal = {Journal of Computational and Graphical Statistics},
	number = {2},
	pages = {362-382},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	title = {Sampling Schemes for Bayesian Variable Selection in Generalized Linear Models},
	volume = {13},
	year = {2004}
}



@article{GhoshClyde2011,
	author = {Joyee Ghosh and Merlise A. Clyde},
	title = {Rao–Blackwellization for Bayesian Variable Selection and Model Averaging in Linear and Binary Regression: A Novel Data Augmentation Approach},
	journal = {Journal of the American Statistical Association},
	volume = {106},
	number = {495},
	pages = {1041-1052},
	year  = {2011}	
}


@article{castillo2015,
	author = "Castillo, Ismaël and Schmidt-Hieber, Johannes and van der Vaart, Aad",
	doi = "10.1214/15-AOS1334",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "10",
	number = "5",
	pages = "1986--2018",
	publisher = "The Institute of Mathematical Statistics",
	title = "Bayesian linear regression with sparse priors",
	volume = "43",
	year = "2015"
}

@article{Rockova2017,
	author = {Veronika Ro\v{c}kov\'{a}},
	title = {Particle EM for Variable Selection},
	journal = {Journal of the American Statistical Association},
	volume = {0},
	pages = {0-0},
	year  = {2017},
	doi = {10.1080/01621459.2017.1360778}
}

@article{scott2010,
	author = "Scott, James G. and Berger, James O.",
	doi = "10.1214/10-AOS792",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "10",
	number = "5",
	pages = "2587--2619",
	publisher = "The Institute of Mathematical Statistics",
	title = "Bayes and empirical-Bayes multiplicity adjustment in the variable-selection problem",
	volume = "38",
	year = "2010"
}

@incollection{Teh2017,
	title = {{A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation}},
	author = {Yee W. Teh and David Newman and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems 19},
	editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
	pages = {1353--1360},
	year = {2007},
	publisher = {MIT Press}
}

@BOOK{Bishop2006,
	AUTHOR       = {Bishop, C. M.},
	TITLE        = {{Pattern Recognition and Machine Learning}},
	PUBLISHER    = {{Springer}},
	YEAR         = {2006},
	ADDRESS      = {{New York}}
}

@article{Ormerod2010,
	author	= {Ormerod, J. T. and Wand, M. P.},
	title    = {{Explaining variational approximations}},
	journal	= {{The American Statistician}},
	year		= {2010},
	volume	= {64},
	pages    = {140--153}
}

@ARTICLE{Grimmer2011,
	title = {An Introduction to Bayesian Inference via Variational Approximations},
	author = {Grimmer, Justin},
	year = {2011},
	journal = {Political Analysis},
	volume = {19},
	number = {01},
	pages = {32-47}
}



@article{BleiEtal2017,
	author = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
	title = {Variational Inference: A Review for Statisticians},
	journal = {Journal of the American Statistical Association},
	volume = {112},
	pages = {859-877},
	year  = {2017}
}

@article{Wand2017,
	author = {M. P. Wand},
	title = {Fast Approximate Inference for Arbitrarily Large Semiparametric Regression Models via Message Passing},
	journal = {Journal of the American Statistical Association},
	volume = {112},
	pages = {137-168},
	year  = {2017}
	
}

@book{LuenbergerYe2008,
	author = {Luenberger, D. G. and Ye, Y.},
	title = {{Linear and Nonlinear Programming}},
	year = {2008},
	edition = {3rd edition},
	publisher = {Springer},
	ADDRESS = {New York}
}



@article{RafteryEtal1997,
	author = {Adrian E.   Raftery  and  David   Madigan  and  Jennifer A.   Hoeting },
	title = {Bayesian Model Averaging for Linear Regression Models},
	journal = {Journal of the American Statistical Association},
	volume = {92},
	pages = {179-191},
	year  = {1997}
}





@inproceedings{wang2013,
	title = "Collapsed {V}ariational {B}ayesian {I}nference for {H}idden {M}arkov {M}odels",
	author = "Wang, Pengyu and Blunsom, Phil",
	year = "2013",
	address = "Scottsdale, AZ, USA",
	booktitle = "Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AISTATS)",
	publisher = "Volume 31 of JMLR: W{\&amp;}CP 31",
}

@incollection{Teh2008,
	title = {Collapsed Variational Inference for HDP},
	author = {Yee W. Teh and Kenichi Kurihara and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems 20},
	editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
	pages = {1481--1488},
	year = {2008},
	publisher = {Curran Associates, Inc.},
}


@Article{Jaakkola2000,
	author="Jaakkola, Tommi S.
	and Jordan, Michael I.",
	title="Bayesian parameter estimation via variational methods            ",
	journal="Statistics and Computing",
	year="2000",
	month="Jan",
	day="01",
	volume="10",
	number="1",
	pages="25--37",
	abstract="We consider a logistic regression model with a Gaussian prior distribution over the parameters. We show that an accurate variational transformation can be used to obtain a closed form approximation to the posterior distribution of the parameters thereby yielding an approximate posterior predictive model. This approach is readily extended to binary graphical model with complete observations. For graphical models with incomplete observations we utilize an additional variational transformation and again obtain a closed form approximation to the posterior. Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates.",
	issn="1573-1375",
	doi="10.1023/A:1008932416310",
}



 



@article{Bhadra2016,
abstract = {Predictive performance in shrinkage regression suffers from two major difficulties: (i) the amount of relative shrinkage is monotone in the singular values of the design matrix and (ii) the amount of shrinkage does not depend on the response variables. Both of these factors can translate to a poor prediction performance, the risk of which can be explicitly quantified using Stein's unbiased risk estimate. We show that using a component-specific local shrinkage term that can be learned from the data under a suitable heavy-tailed prior, in combination with a global term providing shrinkage towards zero, can alleviate both these difficulties and consequently, can result in an improved risk for prediction. Demonstration of improved prediction performance over competing approaches in a simulation study and in a pharmacogenomics data set confirms the theoretical findings.},
archivePrefix = {arXiv},
arxivId = {1605.04796},
author = {Bhadra, Anindya and Datta, Jyotishka and Li, Yunfan and Polson, Nicholas G. and Willard, Brandon},
eprint = {1605.04796},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhadra et al. - 2016 - Prediction risk for global-local shrinkage regression.pdf:pdf},
number = {May},
title = {{Prediction risk for global-local shrinkage regression}},
year = {2016}
}
@article{Lambert1992,
abstract = {Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 - p, a Poisson(lambda) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(lambda) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects-lambda in the imperfect state-may depend on covariates. Sometimes p and lambda are unrelated; other times p is a simple function of lambda such as p = 1/(1 + lambda(tau)) for an unknown constant-tau. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.},
author = {Lambert, Diane},
file = {:home/markg/Downloads/1269547.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {EM ALGORITHM,NEGATIVE BINOMIAL,OVERDISPERSION,P},
number = {1},
pages = {1--14},
title = {{Zero-Inflated Poisson Regression, With an Application To Defects in Manufacturing}},
volume = {34},
year = {1992}
}
@article{Pregibon1981,
abstract = {A maximum likelihood fit of a logistic regression model (and other similar models) is extremely sensitive to outlying responses and extreme points in the design space. We develop diagnostic measures to aid the analyst in detecting such observations and in quantifying their effect on various aspects of the maximum likelihood fit. The elements of the fitting process which constitute the usual output (parameter estimates, standard errors, residuals, etc.) will be used for this purpose. With a properly designed computing package for fitting the usual maximum-likelihood model, the diagnostics are essentially "free for the asking." In particular, good data analysis for logistic regression models need not be expensive or time-consuming.},
author = {Pregibon, Daryl},
doi = {10.1214/aos/1176345513},
file = {:home/markg/Downloads/2240841.pdf:pdf},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {4},
pages = {705--724},
title = {{Logistic Regression Diagnostics}},
volume = {9},
year = {1981}
}
@article{Papaspiliopoulos2016,
abstract = {We show how to carry out fully Bayesian variable selection and model averaging in linear models when both the number of observations and covariates are large. We work under the assumption that the Gram matrix is block-diagonal. Apart from orthogonal regression and various contexts where this is satisfied by design, this framework may serve in future work as a basis for computational approximations to more general design matrices with clusters of correlated predictors. Our approach returns the most probable model of any given size without resorting to numerical integration, posterior probabilities for any number of models by evaluating a single one-dimensional integral that can be computed upfront, and other quantities of interest such as variable inclusion probabilities and model averaged regression estimates by carrying out an adaptive, deterministic one-dimensional numerical integration. This integration and model search are done using novel schemes we introduce in this article. We do not require Markov Chain Monte Carlo. The overall computational cost scales linearly with the number of blocks, which can be processed in parallel, and exponentially with the block size, rendering it most adequate in situations where predictors are organized in many moderately-sized blocks.},
archivePrefix = {arXiv},
arxivId = {1606.03749},
author = {Papaspiliopoulos, Omiros and Rossell, David},
eprint = {1606.03749},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papaspiliopoulos, Rossell - 2016 - Scalable Bayesian variable selection and model averaging under block orthogonal design.pdf:pdf},
title = {{Scalable Bayesian variable selection and model averaging under block orthogonal design}},
year = {2016}
}
@article{Clyde2000,
abstract = {Wavelet shrinkage estimation is an increasingly popular method for signal denoising and compression. Although Bayes estimators can provide excellent mean-squared error ({\{}MSE){\}} properties, the selection of an effective prior is a difficult task. To address this problem, we propose empirical Bayes ({\{}EB){\}} prior selection methods for various error distributions including the normal and the heavier-tailed Student t-distributions. Under such {\{}EB{\}} prior distributions, we obtain threshold shrinkage estimators based on model selection, and multiple-shrinkage estimators based on model averaging. These {\{}EB{\}} estimators are seen to be computationally competitive with standard classical thresholding methods, and to be robust to outliers in both the data and wavelet domains. Simulated and real examples are used to illustrate the flexibility and improved {\{}MSE{\}} performance of these methods in a wide variety of settings.},
author = {Clyde, Merlise and George, Edward I},
doi = {10.1111/1467-9868.00257},
file = {:home/markg/Documents/Clyde George 00 JRSSB.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society Series B},
number = {4},
pages = {681--698},
title = {{Flexible Empirical Bayes Estimation for Wavelets}},
volume = {62},
year = {2000}
}
@article{Mueller2007,
abstract = {In this paper, we extend to generalized linear models (including logistic and other binary regression models, Poisson regression and gamma regression models) the robust model selection methodology developed by Mueller and Welsh (2005; JASA) for linear regression models. As in Mueller and Welsh (2005), we combine a robust penalized measure of fit to the sample with a robust measure of out of sample predictive ability which is estimated using a post-stratified m-out-of-n bootstrap. A key idea is that the method can be used to compare different estimators (robust and nonrobust) as well as different models. Even when specialized back to linear regression models, the methodology presented in this paper improves on that of Mueller and Welsh (2005). In particular, we use a new bias-adjusted bootstrap estimator which avoids the need to centre the explanatory variables and to include an intercept in every model. We also use more sophisticated arguments than Mueller and Welsh (2005) to establish an essential monotonicity condition.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.2349v1},
author = {Mueller, Samuel and Welsh, a H},
eprint = {arXiv:0711.2349v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mueller, Welsh - 2007 - Robust model selection in generalized linear models.pdf:pdf},
journal = {arXiv07112349v1 statME},
keywords = {bootstrap model selection,estimation,generalized linear models,paired bootstrap,robust,robust model selection,stratified bootstrap},
number = {2005},
pages = {24},
title = {{Robust model selection in generalized linear models}},
year = {2007}
}
@article{Bayarri2008,
author = {Bayarri, MJ and Garc{\'{i}}a-Donato, G},
file = {:home/markg/Dropbox/Downloads/06-23.pdf:pdf},
journal = {Journal of the Royal Statistical Society, series B},
number = {70},
pages = {981--1003},
title = {{Divergence Based Priors for Bayesian Hypothesis testing}},
volume = {1},
year = {2008}
}
@article{Zhu2004,
abstract = {Classification of patient samples is an important aspect of cancer diagnosis and treatment. The support vector machine (SVM) has been successfully applied to microarray cancer diagnosis problems. However, one weakness of the SVM is that given a tumor sample, it only predicts a cancer class label but does not provide any estimate of the underlying probability. We propose penalized logistic regression (PLR) as an alternative to the SVM for the microarray cancer diagnosis problem. We show that when using the same set of genes, PLR and the SVM perform similarly in cancer classification, but PLR has the advantage of additionally providing an estimate of the underlying probability. Often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classification, rather than class prediction. We consider two gene selection methods in this paper, univariate ranking (UR) and recursive feature elimination (RFE). Empirical results indicate that PLR combined with RFE tends to select fewer genes than other methods and also performs well in both cross-validation and test samples. A fast algorithm for solving PLR is also described.},
author = {Zhu, Ji and Hastie, Trevor},
doi = {10.1093/biostatistics/kxg046},
file = {:home/markg/Downloads/Zhu-Biostat04.pdf:pdf},
isbn = {1465-4644 (Print)$\backslash$r1465-4644 (Linking)},
issn = {14654644},
journal = {Biostatistics},
keywords = {Cancer diagnosis,Feature selection,Logistic regression,Microarray,Support vector machines},
number = {3},
pages = {427--443},
pmid = {15208204},
title = {{Classification of gene microarrays by penalized logistic regression}},
volume = {5},
year = {2004}
}
@article{Masada2009,
abstract = {In this paper, we propose an acceleration of collapsed variational Bayesian (CVB) inference for latent Dirichlet allocation (LDA) by using Nvidia CUDA compatible devices. While LDA is an efficient Bayesian multi-topic document model, it requires complicated computations for parameter estimation in comparison with other simpler document models, e.g. probabilistic latent semantic indexing, etc. Therefore, we accelerate CVB inference, an efficient deterministic inference method for LDA, with Nvidia CUDA. In the evaluation experiments, we used a set of 50,000 documents and a set of 10,000 images. We could obtain inference results comparable to sequential CVB inference.},
author = {Masada, Tomonari and Hamada, Tsuyoshi and Shibata, Yuichiro and Oguri, Kiyoshi},
doi = {10.1007/978-3-642-02568-6_50},
file = {:home/markg/Downloads/CVM{\_}CUDA.pdf:pdf},
isbn = {3642025676},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {491--500},
title = {{Accelerating collapsed variational bayesian inference for latent dirichlet allocation with nvidia CUDA compatible devices}},
volume = {5579 LNAI},
year = {2009}
}
@article{Cui2008,
abstract = {For the problem of variable selection for the normal linear model, fixed penalty selection criteria such as AIC, Cp, BIC and RIC correspond to the posterior modes of a hierarchical Bayes model for various fixed hyperparameter settings. Adaptive selection criteria obtained by empirical Bayes estimation of the hyperparameters have been shown by George and Foster [2000. Calibration and Empirical Bayes variable selection. Biometrika 87(4), 731-747] to improve on these fixed selection criteria. In this paper, we study the potential of alternative fully Bayes methods, which instead margin out the hyperparameters with respect to prior distributions. Several structured prior formulations are considered for which fully Bayes selection and estimation methods are obtained. Analytical and simulation comparisons with empirical Bayes counterparts are studied. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Cui, Wen and George, Edward I.},
doi = {10.1016/j.jspi.2007.02.011},
file = {:home/markg/Dropbox/Downloads/CG JSPI 2008.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Hyperparameter uncertainty,Model selection,Objective Bayes},
number = {4},
pages = {888--900},
title = {{Empirical Bayes vs. fully Bayes variable selection}},
volume = {138},
year = {2008}
}
@article{broydon1970,
author = {Broydon, C G},
journal = {Journal of the Institute of Mathematics and its Applications},
title = {{The convergence of a class of double rank minimisation algorithms: 1. General considerations}},
volume = {6},
year = {1970}
}
 
@article{??zaltin2011,
abstract = {The most widely used progress measure for branch-and-bound (B{\&}B) algorithms when solving mixed-integer programs (MIPs) is the MIP gap. We introduce a new progress measure that is often much smoother than the MIP gap. We propose a double exponential smoothing technique to predict the solution time of B{\&}B algorithms and evaluate the prediction method using three MIP solvers. Our computational experiments show that accurate predictions of the solution time are possible, even in the early stages of B{\&}B algorithms.},
author = {??zaltin, Osman Y. and Hunsaker, Brady and Schaefer, Andrew J.},
doi = {10.1287/ijoc.1100.0405},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/zaltin, Hunsaker, Schaefer - 2011 - Predicting the solution time of branch-and-bound algorithms for mixed-integer programs.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {INFORMS Journal on Computing},
keywords = {Branch-and-bound algorithm,Mixed-integer programming,Solution time prediction},
number = {3},
pages = {392--403},
title = {{Predicting the solution time of branch-and-bound algorithms for mixed-integer programs}},
volume = {23},
year = {2011}
}
@article{Lawrence2013,
abstract = {Major international projects are underway that are aimed at creating a comprehensive catalogue of all the genes responsible for the ini-tiation and progression of cancer 1–9 . These studies involve the sequencing of matched tumour–normal samples followed by math-ematical analysis to identify those genes in which mutations occur more frequently than expected by random chance. Here we describe a fundamental problem with cancer genome studies: as the sample size increases, the list of putatively significant genes produced by current analytical methods burgeons into the hundreds. The list includes many implausible genes (such as those encoding olfactory receptors and the muscle protein titin), suggesting extensive false-positive findings that overshadow true driver events. We show that this problem stems largely from mutational heterogeneity and provide a novel analytical methodology, MutSigCV, for resolving the problem. We apply MutSigCV to exome sequences from 3,083 tumour–normal pairs and discover extraordinary variation in mutation frequency and spectrum within cancer types, which sheds light on mutational processes and disease aetiology, and in mutation frequency across the genome, which is strongly correlated with DNA replication timing and also with transcriptional activity. By incorporating mutational heterogeneity into the analyses, MutSigCV is able to eliminate most of the apparent artefactual findings and enable the identification of genes truly associated with cancer.},
author = {Lawrence, Michael S and Stojanov, Petar and Polak, Paz and Kryukov, Gregory V and Cibulskis, Kristian and Sivachenko, Andrey and Carter, Scott L and Stewart, Chip and Mermel, Craig H and Roberts, Steven A and Kiezun, Adam and Hammerman, Peter S and Mckenna, Aaron and Drier, Yotam and Zou, Lihua and Ramos, Alex H and Pugh, Trevor J and Stransky, Nicolas and Helman, Elena and Kim, Jaegil and Sougnez, Carrie and Ambrogio, Lauren and Nickerson, Elizabeth and Shefler, Erica and Cort{\'{e}}s, Maria L and Auclair, Daniel and Saksena, Gordon and Voet, Douglas and Noble, Michael and Dicara, Daniel and Lin, Pei and Lichtenstein, Lee and Heiman, David I and Fennell, Timothy and Imielinski, Marcin and Hernandez, Bryan and Hodis, Eran and Baca, Sylvan and Dulak, Austin M and Lohr, Jens and Landau, Dan-Avi and Wu, Catherine J and Melendez-Zajgla, Jorge and Hidalgo-Miranda, Alfredo and Koren, Amnon and Mccarroll, Steven A and Mora, Jaume and Lee, Ryan S and Crompton, Brian and Onofrio, Robert and Parkin, Melissa and Winckler, Wendy and Ardlie, Kristin and Gabriel, Stacey B and Roberts, Charles W M and Biegel, Jaclyn A and Stegmaier, Kimberly and Bass, Adam J and Garraway, Levi A and Meyerson, Matthew and Golub, Todd R and Gordenin, Dmitry A and Sunyaev, Shamil and Lander, Eric S and Getz, Gad},
doi = {10.1038/nature12213},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lawrence et al. - 2013 - Mutational heterogeneity in cancer and the search for new cancer-associated genes.pdf:pdf},
journal = {Nature},
title = {{Mutational heterogeneity in cancer and the search for new cancer-associated genes}},
volume = {499},
year = {2013}
}
@article{Wilson2010,
author = {Wilson, Melanie A},
file = {:home/markg/Downloads/melaniew.pdf:pdf},
journal = {Analysis},
keywords = {GWAS},
pages = {1--99},
title = {{Bayesian model uncertainty and prior choice with applications to genetic association studies}},
year = {2010}
}
@article{Berger2016,
author = {Berger, James O and Pericchi, Luis R and Journal, Source and Statistical, American and Mar, No and Berger, James and Pericchi, Luis R},
file = {:home/markg/Dropbox/Downloads/2291387.pdf:pdf},
keywords = {asymptotic bayes factors,hypothesis testing,noninformative prior,posterior probability,training sample},
number = {433},
pages = {109--122},
title = {{The Intrinsic Bayes Factor for Model Selection and Prediction Stable Linked references are available on JSTOR for this article : The Intrinsic Bayes Factor for Model Selection and Prediction}},
volume = {91},
year = {2016}
}
@article{Seeger1999,
author = {Seeger, M},
file = {:home/markg/Downloads/1722-bayesian-model-selection-for-support-vector-machines-gaussian-processes-and-other-kernel-classifiers.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 12},
title = {{Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers}},
year = {1999}
}
@article{Barber1998,
abstract = {Bayesian treatments of learning in neural networks$\backslash$nare typically based either on local Gaussian$\backslash$napproximations to a mode of the posterior weight$\backslash$ndistribution, or on Markov chain Monte Carlo$\backslash$nsimulations. A third approach, called ensemble$\backslash$nlearning, was introduced by Hinton and van Camp$\backslash$n(1993). It aims to approximate the posterior$\backslash$ndistribution by minimizing the Kullback-Leibler$\backslash$ndivergence between the true posterior and a$\backslash$nparametric approximating distribution. However, the$\backslash$nderivation of a deterministic algorithm relied on$\backslash$nthe use of a Gaussian approximating distribution$\backslash$nwith a diagonal covariance matrix and so was unable$\backslash$nto capture the posterior correlations between$\backslash$nparameters. In this paper, we show how the ensemble$\backslash$nlearning approach can be extended to$\backslash$nfull-covariance Gaussian distributions while$\backslash$nremaining computationally tractable. We also extend$\backslash$nthe framework to deal with hyperparameters, leading$\backslash$nto a simple re-estimation procedure. Initial$\backslash$nresults from a standard benchmark problem are$\backslash$nencouraging.},
author = {Barber, D and Bishop, C M},
file = {:home/markg/Downloads/1480-ensemble-learning-for-multi-layer-networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {395--401},
title = {{Ensemble learning for multi-layer networks}},
year = {1998}
}
@article{Clyde2012,
abstract = {Monte Carlo algorithms are commonly used to identify a set of models for Bayesian model selection or model averaging. Because empirical frequencies of models are often zero or one in high-dimensional problems, posterior probabilities calculated from the observed marginal likelihoods, renormalized over the sampled models, are often employed. Such estimates are the only recourse in several newer stochastic search algorithms. In this paper, we prove that renormalization of posterior probabilities over the set of sampled models generally leads to bias that may dominate mean squared error. Viewing the model space as a finite population, we propose a new estimator based on a ratio of Horvitz–Thompson estimators that incorporates observed marginal likelihoods, but is approximately unbiased. This is shown to lead to a reduction in mean squared error compared to the empirical or renormalized estimators, with little increase in computational cost.},
author = {Clyde, Merlise A. and Ghosh, Joyee},
doi = {10.1093/biomet/ass040},
file = {:home/markg/Downloads/10-11.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Bayesian model averaging,Horvitz-Thompson estimator,Inclusion probability,Markov chain Monte Carlo,Median probability model,Model uncertainty,Variable selection},
number = {4},
pages = {981--988},
title = {{Finite population estimators in stochastic search variable selection}},
volume = {99},
year = {2012}
}
@article{Demyanov2006,
abstract = {Population dynamic modelling often entails parameterizing quite sophisticated biological and ecological mechanisms. For models of moderate mechanistic complexity, this has traditionally been done in an ad hoc manner, with different parameters being estimated independently. The point estimates so obtained are then used for model simulation, perhaps with some further ad hoc adjustment based on comparison with any available data on population dynamics.Quantitative assessments of model adequacy and prediction uncertainty are not easily made using this approach. As an alternative, the paper investigates the practical feasibility of fitting a moderately complex population dynamic model directly and simultaneously to all the data available for parameterization of the model, and to all available data on the population dynamics of the target animal. This alternative approach allows us to combine all available quantitative information on the target species, to assess the viability of the model, the mutual consistency of model and different sources of data and to estimate the uncertainties that are associated with model-based predictions. The target organism in this study is the freshwater amphipod Gammarus pulex (L.), which we model using a stage-structured population dynamic model, implemented via a set of delay differential equations describing the basic demography of the population. Target data include population dynamic data from two sites, information on basic physiological relationships and environmental temperature data. Fitting is performed by using a non-linear least squares approach supplemented with a bootstrapping method for avoiding small scale local minima in the least squares objective function. Variance estimation is performed by further bootstrapping. Interest in Gammarus pulex population dynamics in this case is primarily related to likely population level responses to chemical stressors, and for this we examine predicted ‘recovery times' following exposure to a known toxicant.},
author = {Demyanov, V. and Wood, S. N. and Kedwards, T. J. and Dernyanov, V},
doi = {10.1111/j.1467-9876.2005.00527.x},
file = {:home/markg/Downloads/j.1467-9876.2005.00527.x.pdf:pdf},
issn = {0035-9254},
journal = {Applied Statistics},
keywords = {differential equation model,ecological prediction,ecological risk assessment,population dynamic model},
number = {1},
pages = {41--62},
title = {{Improving ecological impact assessment by statistical data synthesis using process-based models}},
volume = {55},
year = {2006}
}
@article{Casella1980,
author = {Casella, G},
file = {:home/markg/Documents/euclid.aos.1176345141.pdf:pdf},
journal = {The Annals of Statistics},
pages = {1036--1056},
title = {{Minimax Ridge Regression Estimation}},
volume = {8},
year = {1980}
}
@article{Wood2010,
abstract = {Generalized additive models (GAMs) have been popularized by the work of Hastie and Tibshirani (Generalized Additive Models (1990)) and the availability of user friendly gam software in Splus. However, whilst it is flexible and efficient, the gam framework based on backfitting with linear smoothers presents some difficulties when it comes to model selection and inference. On the other hand, the mathematically elegant work of Wahba (Spline Models for Observational Data (1990)) and co-workers on Generalized Spline Smoothing (GSS) provides a rigorous framework for model selection (SIAM J. Sci. Statist. Comput. 12 (1991) 383) and inference with GAMs constructed from smoothing splines: but unfortunately these models are computationally very expensive with operations counts that are of cubic order in the number of data. A `middle way' between these approaches is to construct GAMs using penalized regression splines (e.g. Marx and Eilers, Comput. Statist. Data Anal. (1998)). In this paper, we develop this idea further and show how GAMs constructed using penalized regression splines can be used to get most of the practical benefits of GSS models, including well founded model selection and multi-dimensional smooth terms, with the ease of use and low computational cost of backfit GAMs. Inference with the resulting methods also requires slightly fewer approximations than are employed in the GAM modelling software provided in Splus. This paper presents the basic mathematical and numerical approach to GAMs implemented in the package mgcv, and includes two environmental examples using the methods as implemented in the package.},
author = {Wood, S N and Augustin, N H},
doi = {10.1016/S0304-3800(02)00193-X},
file = {:home/markg/Downloads/wagam.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {gam,gcv,penalized regression spline},
number = {2-3},
pages = {157--177},
title = {{GAMs with integrated model selection using penalized regression splines and applications to environmental modelling}},
volume = {157},
year = {2010}
}
@article{Tierney1989,
abstract = {Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions LUKE TIERNEY, ROBERT E. KASS, and JOSEPH B. KADANE* Tierney and Kadane (1986) presented a simple second-order approximation for posterior expectations of positive functions. They used Laplace's method for asymptotic evaluation of integrals, in which the integrand is written as J(8)exp( -nh(8)) and the function h is approximated by a quadratic. The form in which they applied Laplace's method, however, was fully exponential: The integrand was written instead as exp[ -nh(8) + log J(8)]; this allowed first-order approximations to be used in the numerator and denominator of a ratio of integrals to produce a second-order expansion for the ratio. Other second-order expansions (Hartigan 1965; Johnson 1970; Lindley 1961, 1980; Mosteller and Wallace 1964) require computation of more derivatives of the log-likelihood function. In this article we extend the fully exponential method to apply to expectations and variances of nonpositive functions. To obtain a second-order approximation to an expectation E(g(8)), we use the fully exponential method to approximate the moment-generating function E(exp(sg(8))), whose integrand is positive, and then differentiate the result. This method is formally equivalent to that of Lindley and that of Mosteller and Wallace, yet does not require third derivatives of the likelihood function. It is also equivalent to another alternative approach to the approximation of E(g(O)): We may add a large constant c to g(8), apply the fully exponential method to E(c + g(8)), and subtract c; on passing to the limit as c tends to infinity we regain the approximation based on the moment-generating function. Furthermore, the second derivative of the logarithm of the approximation E(exp(sg(8))), which is an approximate cumulant-generating function, yields a simple second-order approximation to the variance. In deriving these results we omit rigorous justification of formal manipulations, which may be found in Kass, Tierney, and Kadane (in press). Although our point of view is Bayesian, our results have applications to non-Bayesian inference as well (DiCiccio 1986).},
author = {Tierney, Luke and Kass, Robert E. and Kadane, Joseph B.},
doi = {10.1080/01621459.1989.10478824},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Kass, Kadane - 1989 - Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {407},
pages = {710--716},
title = {{Fully Exponential Laplace Approximations to Expectations and Variances of Nonpositive Functions}},
volume = {84},
year = {1989}
}
@book{Gelman2007,
abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/{\~{}}gelman/arm/},
author = {Gelman, Andrew and Hill, Jennifer},
booktitle = {Policy Analysis},
doi = {10.2277/0521867061},
isbn = {052168689X},
issn = {0022-0655},
pages = {625},
pmid = {14341096},
title = {{Data analysis using regression and multilevel/hierarchical models}},
volume = {625},
year = {2007}
}

@article{Liang2008,
author = {Liang, F and Paulo, R and Molina, G and Clyde, M. A. and Berger, J. O.},
journal = {Journal of the American Statistical Association},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}

@article{Piironen2016,
abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. Better and much less varying results are obtained by incorporating all the uncertainties into a full encompassing model and projecting this information onto the submodels. The reference model projection appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08650v1},
author = {Piironen, Juho and Vehtari, Aki},
doi = {10.1007/s11222-016-9649-y},
eprint = {arXiv:1503.08650v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piironen, Vehtari - 2016 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian model selection,Cross-validation,Projection,Reference model,Selection bias},
pages = {1--25},
title = {{Comparison of Bayesian predictive methods for model selection}},
year = {2016}
}
@article{Rue2016,
abstract = {The key operation in Bayesian inference, is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre- Simon Laplace (1774). This simple idea approximates the integrand with a second order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of Integrated Nested Laplace Approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model-abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we will discuss the reasons for the success of the INLA-approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute and why LGMs make such a useful concept for Bayesian computing.},
archivePrefix = {arXiv},
arxivId = {1604.00860},
author = {Rue, H{\aa}vard and Riebler, Andrea and S{\o}rbye, Sigrunn H and Illian, Janine B and Simpson, Daniel P and Lindgren, Finn K},
eprint = {1604.00860},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rue et al. - 2016 - Bayesian Computing with INLA A Review.pdf:pdf},
keywords = {approximate bayesian,gaussian markov random fields,inference,laplace approximations,latent gaussian models,numerical integration,sparse matrices},
pages = {1--26},
title = {{Bayesian Computing with INLA: A Review}},
year = {2016}
}
@article{Zhao2006,
abstract = {Linear mixed models are able to handle an extraordinary range of complications in regression-type analyses. Their most common use is to account for within-subject correlation in longitudinal data analysis. They are also the standard vehicle for smoothing spatial count data. However, when treated in full generality, mixed models can also handle spline-type smoothing and closely approximate kriging. This allows for nonparametric regression models (e.g., additive models and varying coefficient models) to be handled within the mixed model framework. The key is to allow the ran-dom effects design matrix to have general structure; hence our label general design. For continuous response data, particularly when Gaussianity of the response is reasonably assumed, computation is now quite mature and sup-ported by the R, SAS and S-PLUS packages. Such is not the case for bi-nary and count responses, where generalized linear mixed models (GLMMs) are required, but are hindered by the presence of intractable multivariate in-tegrals. Software known to us supports special cases of the GLMM (e.g., PROC NLMIXED in SAS or glmmML in R) or relies on the sometimes crude Laplace-type approximation of integrals (e.g., the SAS macro glimmix or glmmPQL in R). This paper describes the fitting of general design general-ized linear mixed models. A Bayesian approach is taken and Markov chain Monte Carlo (MCMC) is used for estimation and inference. In this gener-alized setting, MCMC requires sampling from nonstandard distributions. In this article, we demonstrate that the MCMC package WinBUGS facilitates sound fitting of general design Bayesian generalized linear mixed models in practice.},
author = {Zhao, Y and Staudenmayer, J and Coull, B A and Wand, M P},
doi = {10.1214/088342306000000015},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2006 - General Design Bayesian Generalized Linear Mixed Models.pdf:pdf},
journal = {Statistical Science},
keywords = {Generalized additive models,Markov chain Monte Carlo,WinBUGS,and phrases,hierarchical center-ing,kriging,nonparametric regression,penalized splines,spatial count data},
number = {1},
pages = {35--51},
title = {{General Design Bayesian Generalized Linear Mixed Models}},
volume = {21},
year = {2006}
}
@article{Wood2004,
author = {Wood, Simon N},
doi = {10.1198/016214504000000980},
file = {:home/markg/Downloads/016214504000000980.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {generalized additive mixed model,generalized cross-validation,penalized quasi-likelihood,regularization,reml,ridge regression,smoothing spline analysis of,spline,stable computation,variance},
number = {467},
pages = {673--686},
title = {{Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models}},
volume = {99},
year = {2004}
}
@article{Li2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06913v1},
author = {Li, Yingbo and Clyde, Merlise A},
eprint = {arXiv:1503.06913v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Clyde - 2015 - Mixtures of g -priors in Generalized Linear Models.pdf:pdf},
journal = {arXiv},
title = {{Mixtures of g -priors in Generalized Linear Models}},
volume = {1503.06913},
year = {2015}
}
@article{Carlo1990,
author = {Carlo, Monte},
file = {:home/markg/Downloads/p5-hinton.pdf:pdf},
keywords = {gaussian quadrature,importance sampling,laplacian approximation,maximum likelihood estimation,nonlinear mixed effects models},
title = {{of the Weights P ( d ;}},
year = {1990}
}
@article{Ibrahim2001,
abstract = {How to assess a Bayesian Model ?},
author = {Ibrahim, Joseph G and Chen, M--H. and Sinha, Debajyoti},
file = {:home/markg/Dropbox/Downloads/A11n23.pdf:pdf},
issn = {1017-0405},
journal = {Statistica Sinica},
keywords = {and phrases,calibration,distribution,model selection,predictive,predictive criterion,variable selection},
pages = {419--443},
title = {{Criterion--based models for {\{}B{\}}ayesian model assessment}},
volume = {11},
year = {2001}
}
 
@book{Agresti2002,
abstract = {Cap{\'{i}}tulo 4: Introducci{\'{o}}n a los GLM. La secci{\'{o}}n 4.3 est{\'{a}} dedicada a datos de conteo. Es una buena introducci{\'{o}}n, con un poco m{\'{a}}s de profundidad en cada tema. Todo el libro es muy bueno.},
author = {Agresti, Alan},
booktitle = {Statistical methodology in the pharmaceutical sciences},
doi = {10.1002/0471249688},
isbn = {0471360937},
issn = {0949-1775},
pages = {1--17},
pmid = {15003161},
title = {{Categorical Data Analysis}},
volume = {13},
year = {2002}
}
 
@article{Challis2011,
abstract = {Two popular approaches to forming principled bounds in approximate Bayesian inference are local variational methods and minimal Kullback-Leibler divergence methods. For a large class of models, we explicitly relate the two approaches, showing that the local variational method is equivalent to a weakened form of Kullback-Leibler Gaussian approximation. This gives a strong motivation to develop ecient methods for KL minimisation. An important and previously unproven property of the KL variational Gaussian bound is that it is a concave function in the parameters of the Gaussian for log concave sites. This observation, along with compact concave parameterisations of the covariance, enables us to develop fast scalable optimisation procedures to obtain lower bounds on the marginal likelihood in large scale Bayesian linear models.},
author = {Challis, Edward and Barber, D},
file = {:home/markg/Downloads/challis11a.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Fourteenth International {\ldots}},
number = {2009},
pages = {199--207},
title = {{Concave Gaussian variational approximations for inference in large-scale Bayesian linear models}},
volume = {15},
year = {2011}
}
@article{Forte,
archivePrefix = {arXiv},
arxivId = {1612.02357},
author = {Forte, Anabel and Garc, Gonzalo},
eprint = {1612.02357},
file = {:home/markg/Documents/1612.02357.pdf:pdf},
pages = {1--27},
title = {Methods and Tools for Bayesian Variable Selection and Model Averaging in Univariate Linear Regression}
}
@article{Pauler1999,
abstract = {In this article we consider tests of variance components using Bayes factors. Such tests arise in many fields of application, including medicine, agriculture, and engineering. When using Bayes factors, the choice of prior distribution on the parameter of interest is of great importance; we propose a "unit-information" reference method for variance component models. The calculation of Bayes factors in this context is not straightforward; there are well-documented difficulties with Markov chain Monte Carlo approaches such as Gibbs sampling, and the usual Laplace approximation is not appropriate, due to the boundary null hypothesis. We describe both an importance sampling approach and an analytical approximation for calculating the numerator and denominator of the Bayes factor. The importance sampling approach is straightforward to implement and also forms the basis for a rejection algorithm that allows generation of samples from the posterior distributions under the null and alternative hypotheses. We suggest that the proposal for the rejection algorithm be based on the likelihood of a subset of the data. For large samples, we develop a boundary Laplace approximation that is accurate to order op(l). We investigate the accuracy of the approximation via simulation, and examine its relationship to the Schwarz criterion. We illustrate the importance sampling/rejection method and boundary Laplace approximation on a number of examples, including a challenging two-way, highly unbalanced dataset and compare our methods with frequentist alternatives.},
author = {Pauler, Dk and Wakefield, Jc and Kass, Re},
doi = {10.1080/01621459.1999.10473877},
file = {:home/markg/Documents/2669938.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {boundary problem,importance sampling,laplace,reference prior,rejection method,s method,schwarz crite-},
number = {448},
pages = {1242--1253},
title = {{Bayes Factors and Approximations for Variance Component Models.}},
volume = {94},
year = {1999}
}
@article{BIOM:BIOM1030,
author = {Hall, Daniel B},
doi = {10.1111/j.0006-341X.2000.01030.x},
issn = {1541-0420},
journal = {Biometrics},
keywords = {EM algorithm,Excess zeros,Generalized linear mixed model,Heterogeneity,Mixed effects,Overdispersion,Repeated measures},
number = {4},
pages = {1030--1039},
publisher = {Blackwell Publishing Ltd},
title = {{Zero-Inflated Poisson and Binomial Regression with Random Effects: A Case Study}},
volume = {56},
year = {2000}
}
@article{Maruyama2005,
abstract = {Let y=A$\backslash$beta+$\backslash$epsilon, where y is an N$\backslash$times1 vector of observations, $\backslash$beta is a p$\backslash$times1 vector of unknown regression coefficients, A is an N$\backslash$times p design matrix and $\backslash$epsilon is a spherically symmetric error term with unknown scale parameter $\backslash$sigma. We consider estimation of $\backslash$beta under general quadratic loss functions, and, in particular, extend the work of Strawderman [J. Amer. Statist. Assoc. 73 (1978) 623-627] and Casella [Ann. Statist. 8 (1980) 1036-1056, J. Amer. Statist. Assoc. 80 (1985) 753-758] by finding adaptive minimax estimators (which are, under the normality assumption, also generalized Bayes) of $\backslash$beta, which have greater numerical stability (i.e., smaller condition number) than the usual least squares estimator. In particular, we give a subclass of such estimators which, surprisingly, has a very simple form. We also show that under certain conditions the generalized Bayes minimax estimators in the normal case are also generalized Bayes and minimax in the general case of spherically symmetric errors.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508282v1},
author = {Maruyama, Yuzo and Strawderman, William E.},
doi = {10.1214/009053605000000327},
eprint = {0508282v1},
file = {:home/markg/Dropbox/Downloads/euclid.aos.1123250228.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Condition number,Generalized bayes,Minimaxity,Ridge regression},
number = {4},
pages = {1753--1770},
primaryClass = {arXiv:math},
title = {{A new class of generalized bayes minimax ridge regression estimators}},
volume = {33},
year = {2005}
}


@article{Ghosh2006,
abstract = {In modeling defect counts collected from an established manufacturing processes, there are usually a relatively large number of zeros (non-defects). The commonly used models such as Poisson or Geometric distributions can underestimate the zero-defect probability and hence make it difficult to identify significant covariate effects to improve production quality. This article introduces a flexible class of zero inflated models which includes other familiar models such as the Zero Inflated Poisson (ZIP) models, as special cases. A Bayesian estimation method is developed as an alternative to tra-ditionally used maximum likelihood based methods to analyze such data. Simulation studies show that the proposed method has better finite sample performance than the classical method with tighter interval estimates and better coverage probabilities. A real-life data set is analyzed to illustrate the practicability of the proposed method easily implemented using WinBUGS.},
author = {Ghosh, Sujit K and Mukhopadhyay, Pabak and Lu, Jye-Chyi},
doi = {10.1016/j.jspi.2004.10.008},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosh, Mukhopadhyay, Lu - 2006 - Bayesian analysis of zero-inflated regression models.pdf:pdf},
journal = {Journal of Statistical Planning and Inference},
keywords = {62E10,62F15,62P30 Keywords,Bayesian inference,Data augmentation,Gibbs sampling,MSC,Markov chain Monte Carlo,WinBUGS,Zero-inflated power series models},
pages = {1360--1375},
title = {{Bayesian analysis of zero-inflated regression models}},
volume = {136},
year = {2006}
}

@article{Maruyama2011,
author = {Maruyama, Yuzo and George, Edward I.},
journal = {Annals of Statistics},
number = {5},
pages = {2740--2765},
title = {{Fully Bayes factors with a generalized g-prior}},
volume = {39},
year = {2011}
}

@article{Breiman1996,
abstract = {In model selection, usually a "best" predictor is chosen from a collection {\{}$\mu$̂({\textperiodcentered}, s){\}} of predictors where $\mu$̂({\textperiodcentered}, s) is the minimum least-squares predictor in a collection Us of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in Us. If L is the data used to derive the sequence {\{}$\mu$̂({\textperiodcentered}, s){\}}, the procedure is called unstable if a small change in L can cause large changes in {\{}$\mu$̂({\textperiodcentered}, s){\}}. With a crystal ball, one could pick the predictor in {\{}$\mu$̂({\textperiodcentered}, s){\}} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\{}$\mu$̂'({\textperiodcentered}, s){\}} and then averaging over many such predictor sequences.},
author = {Breiman, Leo},
doi = {10.1214/aos/1032181158},
file = {:home/markg/Dropbox/Downloads/euclid.aos.1032181158.pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Regression,cross-validation,prediction error,predictive loss,subset selection},
number = {6},
pages = {2350--2383},
title = {{Heuristics of instability in model selection}},
volume = {24},
year = {1996}
}
@article{Casella1985,
author = {Casella, George},
doi = {10.2307/2288496},
file = {:home/markg/Documents/2288496.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {matrix conditioning,mean,numerical stability},
number = {391},
pages = {753},
title = {{Condition Numbers and Minimax Ridge Regression Estimators}},
volume = {80},
year = {1985}
}
@misc{rstan-software:2015,
title = {{No Title}}
}
@article{Meinshausen2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0809.2932v2},
author = {Meinshausen, Nicolai},
eprint = {arXiv:0809.2932v2},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meinshausen - 2009 - Stability selection (Slides).pdf:pdf},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
pages = {1--30},
title = {{Stability selection (Slides)}},
year = {2009}
}
@article{Wand2008,
author = {Wand, M P and Ormerod, J T},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wand, Ormerod - 2008 - ON SEMIPARAMETRIC REGRESSION WITH O'SULLIVAN PENALIZED SPLINES.pdf:pdf},
journal = {Australian {\&} New Zealand Journal of Statistics},
number = {2},
pages = {179--198},
publisher = {Wiley Online Library},
title = {{On semiparametric regression with O'Sullivan penalized splines}},
volume = {50},
year = {2008}
}
@article{Ruli2013,
author = {Ruli, Erlis and Ventura, Laura},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruli, Ventura - 2013 - Modern Bayesian Inference in Zero-Inflated Poisson Models.pdf:pdf},
keywords = {asymptotic expansions,count data,likelihood,matching prior,modified profile,nuisance parameter,tail area probability,zip regression},
title = {{Modern Bayesian Inference in Zero-Inflated Poisson Models}},
year = {2013}
}
@article{Kass1993,
abstract = {... See Slate (1992) for a detailed discussion of sample sizes re - quired to obtain posterior Normality (which would guarantee accuracy of Laplace's  method ) ... is available in S and in LispStat ( Tierney , 1989, 1990 ). ... For the general formulation see Kass  ...},
author = {Kass, Robert E and Raftery, Adrian E},
doi = {10.2307/2291091},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kass, Raftery - 1993 - Bayes Factors and Model Uncertainty.pdf:pdf},
issn = {01621459},
journal = {Technical Report},
number = {254},
pages = {1--73},
title = {{Bayes Factors and Model Uncertainty}},
year = {1993}
}
@article{Raiko2007,
abstract = {We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas au- tomatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords:},
author = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen, Juha},
file = {:home/markg/Downloads/raiko07a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian modelling,building blocks,graphical models,latent variable models,local computation,variational Bayesian learning},
pages = {155--201},
title = {{Building Blocks for Variational Bayesian Learning of Latent Variable Models}},
volume = {8},
year = {2007}
}
@misc{stan-manual:2015,
title = {{No Title}}
}
@article{Eklund2007,
abstract = {Large scale Bayesian model averaging and variable selection exercises present, despite the great increase in desktop computing power, considerable computational challenges. Due to the large scale it is impossible to evaluate all possible models and estimates of posterior probabilities are instead obtained from stochastic (MCMC) schemes designed to converge on the posterior distribution over the model space. While this frees us from the requirement of evaluating all possible models the computational effort is still substantial and efficient implementation is vital. Efficient implementation is concerned with two issues: the efficiency of the MCMC algorithm itself and efficient computation of the quantities needed to obtain a draw from the MCMC algorithm. We evaluate several different MCMC algorithms and find that relatively simple algorithms with local moves perform competitively except possibly when the data is highly collinear. For the second aspect, efficient computation within the sampler, we focus on the important case of linear models where the computations essentially reduce to least squares calculations. Least squares solvers that update a previous model estimate are appealing when the MCMC algorithm makes local moves and we find that the Cholesky update is both fast and accurate.},
author = {Eklund, Jana and Karlsson, Sune},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eklund, Karlsson - 2007 - Computational Efficiency in Bayesian Model and Variable Selection.pdf:pdf},
keywords = {Bayesian Model Averaging,Cholesky decomposition,QR decomposition,Sweep operator,Swendsen-Wang algorithm},
number = {2007:4},
title = {{Computational Efficiency in Bayesian Model and Variable Selection}},
year = {2007}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Jaakola{\_}Jordan{\_}2000.ps:ps},
title = {{Jaakola{\_}Jordan{\_}2000}}
}
@article{Teh2006,
abstract = {Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational Bayesian inference for LDA.},
author = {Teh, Yee Whye and Newman, David and Welling, Max},
file = {:home/markg/Downloads/4535.pdf:pdf},
journal = {Neural Information Processing Systems},
pages = {1353----1360},
title = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
year = {2006}
}
@article{Wood2001,
abstract = {Objective functions that arise when fitting nonlinear models often contain local minima that are of little significance except for their propensity to trap minimization algorithms. The standard methods for attempting to deal with this problem treat the objective function as fixed and employ stochastic minimization approaches in the hope of randomly jumping out of local minima. This article suggests a simple trick for performing such minimizations that can be employed in conjunction with most conventional nonstochastic fitting methods. The trick is to stochastically perturb the objective function by bootstrapping the data to be fit. Each bootstrap objective shares the large-scale structure of the original objective but has different small-scale structure. Minimizations of bootstrap objective functions are alternated with minimizations of the original objective function starting from the parameter values with which minimization of the previous bootstrap objective terminated. An example is presented, fitting a nonlinear population dynamic model to population dynamic data and including a comparison of the suggested method with simulated annealing. Convergence diagnostics are discussed.},
author = {Wood, S N},
doi = {10.1111/j.0006-341X.2001.00240.x},
file = {:home/markg/Downloads/j.0006-341X.2001.00240.x.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
keywords = {ecological model,fitting,global optimization,nonlinear model fitting,population dynamic model,simulated annealing,stochastic optimization},
number = {1},
pages = {240--244},
pmid = {11252605},
title = {{Minimizing model fitting objectives that contain spurious local minima by bootstrap restarting.}},
volume = {57},
year = {2001}
}
@article{Liu1989,
author = {Liu, Dong C and Nocedal, Jorge},
journal = {Mathematical programming},
number = {1-3},
pages = {503--528},
publisher = {Springer},
title = {{On the limited memory BFGS method for large scale optimization}},
volume = {45},
year = {1989}
}
@article{Nott2012,
abstract = {Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture models and our article considers flexible mixtures of heteroscedastic regression (MHR) models where the response distribution is a normal mixture, with the component means, variances and mixture weights all varying as a function of covariates. Our article develops fast variational approximation methods for inference. Our motivation is that alternative computationally intensive MCMC methods for fitting mixture models are difficult to apply when it is desired to fit models repeatedly in exploratory analysis and model choice. Our article makes three contributions. First, a variational approximation for MHR models is described where the variational lower bound is in closed form. Second, the basic approximation can be improved by using stochastic approximation methods to perturb the initial solution to attain higher accuracy. Third, the advantages of our approach for model choice and evaluation compared to MCMC based approaches are illustrated. These advantages are particularly compelling for time series data where repeated refitting for one step ahead prediction in model choice and diagnostics and in rolling window computations is very common. Supplemental materials for the article are available online.},
author = {Nott, David J. and Tan, Siew Li and Villani, Mattias and Kohn, Robert},
doi = {10.1080/10618600.2012.679897},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nott et al. - 2012 - Regression Density Estimation With Variational Methods and Stochastic Approximation.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Bayesian model selection,heteroscedasticity,mixtures of experts,stochastic approximation,variational approximation},
number = {3},
pages = {797--820},
title = {{Regression Density Estimation With Variational Methods and Stochastic Approximation}},
volume = {21},
year = {2012}
}
@article{Tan2016,
abstract = {We consider the problem of learning a Gaussian variational approximation to the posterior distribution for a high-dimensional parameter, where we impose sparsity in the precision matrix to reflect appropriate conditional independence structure in the model. Incorporating sparsity in the precision matrix allows the Gaussian variational distribution to be both flexible and parsimonious, and the sparsity is achieved through parameterization in terms of the Cholesky factor. Efficient stochastic gradient methods which make appropriate use of gradient information for the target distribution are developed for the optimization. We consider alternative estimators of the stochastic gradients which have lower variation and are more stable. Our approach is illustrated using generalized linear mixed models and state space models for time series.},
archivePrefix = {arXiv},
arxivId = {1605.05622},
author = {Tan, Linda S. L. and Nott, David J.},
eprint = {1605.05622},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2016 - Gaussian variational approximation with sparse precision matrix.pdf:pdf},
keywords = {gaussian variational approximation,matrix,sparse precision,stochastic gradient algorithms,variational bayes},
number = {2014},
pages = {16},
title = {{Gaussian variational approximation with sparse precision matrix}},
year = {2016}
}
@article{Rohde2015,
author = {Rohde, D and Wand, M. P.},
file = {:home/markg/Downloads/RohdeWand.pdf:pdf},
keywords = {bayesian computing,conjugate variational message passing,fixed-form variational bayes,fixed-point iteration,non-,nonlinear conjugate gradient method},
number = {January},
pages = {1--41},
title = {{Semiparametric Mean Field Variational Bayes : General Principles and Numerical Issues Semiparametric Mean Field Variational Bayes :}},
year = {2015}
}
@article{Zou2006,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection. The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
archivePrefix = {arXiv},
arxivId = {NIHMS201118},
author = {Zou, Hui},
doi = {10.1198/016214506000000735},
eprint = {NIHMS201118},
file = {:home/markg/Documents/zou2006.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {asymptotic normality,lasso,minimax,oracle inequality,oracle procedure,variable selection},
number = {476},
pages = {1418--1429},
pmid = {20122298},
title = {{The Adaptive Lasso and Its Oracle Properties}},
volume = {101},
year = {2006}
}
@article{Luts2013,
abstract = {A mean field variational Bayes approach to support vector machines (SVMs) using the latent variable representation on Polson {\&} Scott (2012) is presented. This representation allows circumvention of many of the shortcomings associated with classical SVMs including automatic penalty parameter selection, the ability to handle dependent samples, missing data and variable selection. We demonstrate on simulated and real datasets that our approach is easily extendable to non-standard situations and outperforms the classical SVM approach whilst remaining computationally efficient.},
archivePrefix = {arXiv},
arxivId = {1305.2667},
author = {Luts, Jan and Ormerod, John T.},
doi = {10.1016/j.csda.2013.10.030},
eprint = {1305.2667},
file = {:home/markg/Downloads/1305.2667v1.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {approximate bayesian inference},
number = {2000},
pages = {18},
title = {{Mean field variational Bayesian inference for support vector machine classification}},
volume = {73},
year = {2013}
}
@article{Lesieur2017,
abstract = {This article is an extended version of previous work of the authors [40, 41] on low-rank matrix estimation in the presence of constraints on the factors into which the matrix is factorized. Low-rank matrix factorization is one of the basic methods used in data analysis for unsupervised learning of relevant features and other types of dimensionality reduction. We present a framework to study the constrained low-rank matrix estimation for a general prior on the factors, and a general output channel through which the matrix is observed. We draw a paralel with the study of vector-spin glass models - presenting a unifying way to study a number of problems considered previously in separate statistical physics works. We present a number of applications for the problem in data analysis. We derive in detail a general form of the low-rank approximate message passing (Low- RAMP) algorithm, that is known in statistical physics as the TAP equations. We thus unify the derivation of the TAP equations for models as different as the Sherrington-Kirkpatrick model, the restricted Boltzmann machine, the Hopfield model or vector (xy, Heisenberg and other) spin glasses. The state evolution of the Low-RAMP algorithm is also derived, and is equivalent to the replica symmetric solution for the large class of vector-spin glass models. In the section devoted to result we study in detail phase diagrams and phase transitions for the Bayes-optimal inference in low-rank matrix estimation. We present a typology of phase transitions and their relation to performance of algorithms such as the Low-RAMP or commonly used spectral methods.},
archivePrefix = {arXiv},
arxivId = {1701.00858},
author = {Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
eprint = {1701.00858},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lesieur, Krzakala, Zdeborov{\'{a}} - 2017 - Constrained Low-rank Matrix Estimation Phase Transitions, Approximate Message Passing and Applica.pdf:pdf},
pages = {1--64},
title = {{Constrained Low-rank Matrix Estimation: Phase Transitions, Approximate Message Passing and Applications}},
year = {2017}
}
@article{Wang2015,
abstract = {Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.},
archivePrefix = {arXiv},
arxivId = {1506.02222},
author = {Wang, Xiangyu and Dunson, David and Leng, Chenlei},
eprint = {1506.02222},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Dunson, Leng - 2015 - No penalty no tears Least squares in high-dimensional linear models.pdf:pdf},
pages = {1--26},
title = {{No penalty no tears: Least squares in high-dimensional linear models}},
year = {2015}
}
@article{Gatu2006,
abstract = {An efficient branch-and-bound algorithm for computing the best-subset regression models is proposed. The algorithm avoids the computation of the whole regression tree that generates all possible subset models. It is formally shown that if the branch-and-bound test holds, then the current subtree together with its right-hand side subtrees are cut. This reduces significantly the computational burden of the proposed algorithm when compared to an existing leaps-and-bounds method which generates two trees. Specifically, the proposed algorithm, which is based on orthogonal transformations, outperforms by O(n 3) the leaps-and-bounds strategy. The criteria used in identifying the best subsets are based on monotone functions of the residual sum of squares (RSS) such as R2, adjusted R2, mean square error of prediction, and Cp. Strategies and heuristics that improve the computational performance of the proposed algorithm are investigated. A computationally efficient heuristic version of the branch-and-bound strategy which decides to cut subtrees using a tolerance parameter is proposed. The heuristic algorithm derives models close to the best ones. However, it is shown analytically that the relative error of the RSS, and consequently the corresponding statistic, of the computed subsets is smaller than the value of the tolerance parameter which lies between zero and one. Computational results and experiments on random and real data are presented and analyzed. {\textcopyright} 2006 American Statistical Association.},
author = {Gatu, Cristian and Kontoghiorghes, Erricos John},
doi = {10.1198/106186006X100290},
file = {:home/markg/Downloads/BBASubset.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {least squares,qr decomposition,subset regression},
number = {1},
pages = {139--156},
title = {{Branch-and-Bound Algorithms for Computing the Best-Subset Regression Models}},
volume = {15},
year = {2006}
}
@article{Welham2007,
abstract = {Three types of polynomial mixed model splines have been proposed: smoothing splines, P-splines and penalized splines using a truncated power function basis. The close connections between these models are demonstrated, showing that the default cubic form of the splines differs only in the penalty used. A general definition of the mixed model spline is given that includes general constraints and can be used to produce natural or periodic splines. The impact of different penalties is demonstrated by evaluation across a set of functions with specific features, and shows that the best penalty in terms of mean squared error of prediction depends on both the form of the underlying function and the signal:noise ratio.},
author = {Welham, Sue J and Cullis, Brian R and Kenward, Michael G and Thompson, Robin},
doi = {10.1111/j.1467-842X.2006.00454.x},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Welham et al. - 2007 - A COMPARISON OF MIXED MODEL SPLINES FOR CURVE FITTING.pdf:pdf},
journal = {Aust. N. Z. J. Stat},
keywords = {P-splines,best linear unbiased prediction,mixed models,penalized splines,residual maximum likelihood,smoothing splines},
number = {1},
pages = {1--23},
title = {{A COMPARISON OF MIXED MODEL SPLINES FOR CURVE FITTING}},
volume = {49},
year = {2007}
}
@article{Chib1995,
author = {Chib, Siddhartha and Greenberg, Edward},
file = {:home/markg/Downloads/chib{\_}1995.pdf:pdf},
journal = {American Statistician},
number = {4},
pages = {327--335},
title = {{Understanding the metropolis-hastings algorithm}},
volume = {49},
year = {1995}
}
@article{Tan2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.4949v4},
author = {Tan, Linda S L and Nott, David J.},
doi = {10.1214/14-BA885},
eprint = {arXiv:1208.4949v4},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2014 - A stochastic variational framework for fitting and diagnosing generalized linear mixed models.pdf:pdf;:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2014 - A stochastic variational framework for fitting and diagnosing generalized linear mixed models(2).pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Conflict diagnostics,Hierarchical models,Identify divergent units,Nonconjugate variational message passing,Stochastic approximation,Variational bayes},
number = {4},
pages = {963--1004},
title = {{A stochastic variational framework for fitting and diagnosing generalized linear mixed models}},
volume = {9},
year = {2014}
}
@book{James:2014:ISL:2517747,
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
isbn = {1461471370, 9781461471370},
publisher = {Springer Publishing Company, Incorporated},
title = {{An Introduction to Statistical Learning: With Applications in R}},
year = {2014}
}
@misc{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - socmeth1995.pdf.pdf:pdf},
title = {socmeth1995.pdf}
}
@article{Hodis2012,
abstract = {SUMMARY Despite recent insights into melanoma genetics, systematic surveys for driver mutations are chal-lenged by an abundance of passenger mutations caused by carcinogenic UV light exposure. We devel-oped a permutation-based framework to address this challenge, employing mutation data from in-tronic sequences to control for passenger mutational load on a per gene basis. Analysis of large-scale melanoma exome data by this approach discovered six novel melanoma genes (PPP6C, RAC1, SNX31, TACC1, STK19, and ARID2), three of which—RAC1, PPP6C, and STK19—harbored recurrent and poten-tially targetable mutations. Integration with chromo-somal copy number data contextualized the land-scape of driver mutations, providing oncogenic insights in BRAF-and NRAS-driven melanoma as well as those without known NRAS/BRAF mutations. The landscape also clarified a mutational basis for RB and p53 pathway deregulation in this malignancy. Finally, the spectrum of driver mutations provided unequivocal genomic evidence for a direct muta-genic role of UV light in melanoma pathogenesis.},
author = {Hodis, Eran and Watson, Ian R and Kryukov, Gregory V and Arold, Stefan T and Imielinski, Marcin and Theurillat, Jean-Philippe and Nickerson, Elizabeth and Auclair, Daniel and Li, Liren and Place, Chelsea and Dicara, Daniel and Ramos, Alex H and Lawrence, Michael S and Cibulskis, Kristian and Sivachenko, Andrey and Voet, Douglas and Saksena, Gordon and Stransky, Nicolas and Onofrio, Robert C and Winckler, Wendy and Ardlie, Kristin and Wagle, Nikhil and Wargo, Jennifer and Chong, Kelly and Morton, Donald L and Stemke-Hale, Katherine and Chen, Guo and Noble, Michael and Meyerson, Matthew and Ladbury, John E and Davies, Michael A and Gershenwald, Jeffrey E and Wagner, Stephan N and Hoon, Dave S B and Schadendorf, Dirk and Lander, Eric S and Gabriel, Stacey B and Getz, Gad and Garraway, Levi A},
doi = {10.1016/j.cell.2012.06.024},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodis et al. - 2012 - A Landscape of Driver Mutations in Melanoma.pdf:pdf},
journal = {Cell},
pages = {251--263},
title = {{A Landscape of Driver Mutations in Melanoma}},
volume = {150},
year = {2012}
}
@article{,
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1989 - Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annal.pdf:pdf},
number = {4},
pages = {367--393},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}},
volume = {4},
year = {1989}
}
@book{Demmel1997,
author = {Demmel, James W},
publisher = {Siam},
title = {{Applied numerical linear algebra}},
year = {1997}
}
@article{LeeWangScottYauMcLachlan2006,
abstract = {Count data with excess zeros relative to a Poisson distribution are common in many biomedical applications. A popular approach to the analysis of such data is to use a zero-inflated Poisson (ZIP) regression model. Often, because of the hierarchical study design or the data collection procedure, zero-inflation and lack of independence may occur simultaneously, which render the standard ZIP model inadequate. To account for the preponderance of zero counts and the inherent correlation of observations, a class of multi-level ZIP regression model with random effects is presented. Model fitting is facilitated using an expectation-maximization algorithm, whereas variance components are estimated via residual maximum likelihood estimating equations. A score test for zero-inflation is also presented. The multi-level ZIP model is then generalized to cope with a more complex correlation structure. Application to the analysis of correlated count data from a longitudinal infant feeding study illustrates the usefulness of the approach.},
annote = {Copyright - {\textcopyright} 2006 Arnold; Last updated - 2014-04-29},
author = {Lee, Andy H and Wang, Kui and Scott, Jane A and Yau, Kelvin K W and McLachlan, Geoffrey J},
file = {:home/markg/Downloads/lwsym{\_}smmr06.pdf:pdf},
isbn = {09622802},
journal = {Statistical methods in medical research},
keywords = {Medical Sciences; Infant; Breast Feeding -- statis,Newborn; Models,Statistical; Longitudinal Studies; Male; Female;},
number = {1},
pages = {47--61},
title = {{Multi-level zero-inflated Poisson regression modelling of correlated count data with excess zeros}},
volume = {15},
year = {2006}
}
@article{Hoeting1996,
abstract = {We suggest a method for simultaneous variable selection and outlier identification based on the computation of posterior model probabilities. This avoids the problem that the model you select depends upon the order in which variable selection and outlier identification are carried out. Our method can find multiple outliers and appears to be successful in identifying masked outliers. We also address the problem of model uncertainty via Bayesian model averaging. For problems where the number of models is large, we suggest a Markov chain Monte Carlo approach to approximate the Bayesian model average over the space of all possible variables and outliers under consideration. Software for implementing this approach is described. In an example, we show that model averaging via simultaneous variable selection and outlier identification improves predictive performance and provides more accurate prediction intervals as compared to any single model that might reasonably be selected.},
author = {Hoeting, Jennifer and Raftery, Adrian E and Madigan, David},
doi = {10.1016/0167-9473(95)00053-4},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoeting, Raftery, Madigan - 1996 - A Method for Simultaneous Variable Selection and Outlier Identification in Linear Regression.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {Bayesian model averaging,Markov chain Monte Carlo model composition,Masking,Model uncertainty,Posterior model probability},
number = {3},
pages = {251--270},
title = {{A Method for Simultaneous Variable Selection and Outlier Identification in Linear Regression}},
volume = {22},
year = {1996}
}
@article{Wood2003,
abstract = {discuss the production of low rank smoothers for d greater than or equal to 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with 'knot placement' that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics},
author = {Wood, Simon N.},
doi = {10.1111/1467-9868.00374},
file = {:home/markg/Downloads/1467-9868.00374.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Generalized additive model,Regression spline,Thin plate spline},
pages = {95--114},
title = {{Thin plate regression splines}},
volume = {65},
year = {2003}
}
@article{Hosmer1989,
abstract = {Selection of a subset of meaningful covariates for a statistical model is an important and often time-consuming task in model building. Lawless and Singhal (1978, Biometrics 34, 318-327) proposed a method for best subsets selection for nonnormal models. We develop a method for logistic regression that may be performed with any best subsets linear regression program. CR - Copyright {\&}{\#}169; 1989 International Biometric Society},
author = {Hosmer, David W and Jovanovic, Borko and Lemeshow, Stanley},
doi = {10.2307/2531779},
file = {:home/markg/Downloads/2531779.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
number = {4},
pages = {1265--1270},
title = {{Best Subsets Logistic Regression}},
volume = {45},
year = {1989}
}
@article{Vandenberghe1996,
abstract = {In sernidefinite programming, one minimizes a linear function subject to the constraint that an affine combination ofsynunetric matrices is positive semidefinite. Such a constraint is nonlinear and nonsmooth, but convex, so semidefinite programs are convex optimization problems. Semidefinite programming unifies several standard problems (e.g., linear and quadratic programming) and finds many applications in engineering and combinatorial optimization. Although semidefinite programs are much more general than linear programs, they are not much harder to solve. Most interior-point methods for linear programming have been generalized to semidefinite programs. As in linear programming, these methods have polynomial worst-case complexity and perform very well in practice. This paper gives a survey of the theory and applications of semidefinite programs and an introduction to primal- dual interior-point methods for their solution.},
author = {Vandenberghe, Lieven and Boyd, Stephen},
doi = {10.1137/1038003},
file = {:home/markg/Downloads/semidef{\_}prog.pdf:pdf},
isbn = {10.1137/1038003},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {combinatorial optimization,convex optimization,eigenvalue optimization,interior-point methods,semidefinite programming,system and control theory},
number = {1},
pages = {49--95},
pmid = {18059682},
title = {{Semidefinite Programming}},
volume = {38},
year = {1996}
}
@article{Mcleod2010,
abstract = {The function bestglm selects the best subset of inputs for the glm family. The selection methods available include a variety of information criteria as well as cross-validation. Several examples are provided to show that this approach is sometimes more accurate than using the built-in R function step. In the Gaussian case the leaps-and-bounds algorithm in leaps is used provided that there are no factor variables with more than two levels. In the non-Gaussian glm case or when there are factor variables present with three or more levels, a simple exhaustive enumeration approach is used. This vignette also explains how the applications given in our article Xu and McLeod (2010) may easily be reproduced. A separate vignette is available to provide more details about the simulation results reported in Xu and McLeod (2010, Table 2) and to explain how the results may be reproduced.},
author = {Mcleod, a I},
file = {:home/markg/Downloads/bestglm.pdf:pdf},
journal = {Prostate The},
keywords = {aic,best subset glm,bic,cross validation,extended bic},
pages = {1--39},
title = {{bestglm : Best Subset GLM}},
year = {2010}
}
@article{Bursac2008,
author = {Bursac, Zoran and Gauss, C Heath and Williams, David Keith and Hosmer, David W},
doi = {10.1186/1751-0473-3-17},
file = {:home/markg/Downloads/1751-0473-3-17.pdf:pdf},
issn = {1751-0473},
journal = {Source Code for Biology and Medicine},
number = {1},
pages = {17},
title = {{Purposeful selection of variables in logistic regression}},
volume = {3},
year = {2008}
}
@article{Santis1999,
abstract = {In the Bayesian approach to model selection and hypothesis testing, the Bayes factor plays a central role. However, the Bayes factor is very sensitive to prior distributions of parameters. This is a problem especially in the presence of weak prior information on the parameters of the models. The most radical consequence of this fact is that the Bayes factor is undetermined when improper priors are used. Nonetheless, extending the non-informative approach of Bayesian analysis to model selection/testing procedures is important both from a theoretical and an applied viewpoint. The need to develop automatic and robust methods for model comparison has led to the introduction of several alternative Bayes factors. In this paper we review one of these methods: the fractional Bayes factor (O'Hagan, 1995). We discuss general properties of the method, such as consistency and coherence. Furthermore, in addition to the original, essentially asymptotic justifications of the fractional Bayes factor, we provide further finite-sample motivations for its use. Connections and comparisons to other automatic methods are discussed and several issues of robustness with respect to priors and data are considered. Finally, we focus on some open problems in the fractional Bayes factor approach, and outline some possible answers and directions for future research. /// Dans l'approche Bayesienne relative {\`{a}} la s{\'{e}}lection d'un model et {\`{a}} la v{\'{e}}rification d'une hypoth{\`{e}}se, le facteur de Bayes joue une r{\^{o}}le fondamental. Toutefois le facteur de Bayes est tr{\`{e}}s sensible aux distributions {\`{a}} priori des param{\`{e}}tres. Ceci constitue un probl{\`{e}}me surtout en pr{\'{e}}sence d'une faible information {\`{a}} priori en ce qui concerne les param{\`{e}}tres des models. La cons{\'{e}}quence la plus radical de ce fait est que le facteur de Bayes est undetermin{\'{e}} quand les distributions {\`{a}} priori non informatives sont utilis{\'{e}}es. Cepandant, il est important d'{\'{e}}largir l'approche non informative de l'analyse Bayesienne {\`{a}} l'effet soit de d{\'{e}}terminer la s{\'{e}}lection d'un model que de v{\'{e}}rifier une hypoth{\`{e}}se. La necessit{\'{e}} de d{\'{e}}velopper des m{\'{e}}thodes automatiques et robustes pour la comparaison des models, a amen{\'{e}} {\`{a}} l'introduction des plusieurs facteurs de Bayes alternatifs. Cette {\'{e}}tude prend en consideration les resultats principaux relatifs {\`{a}} une de ces methodes, {\`{a}} savoir le facteur de Bayes fractionnaire. Nous analysons les caracteristique g{\'{e}}n{\'{e}}rales de cette methode telles que sa consistance et sa coh{\'{e}}rence. De plus en sus des justifications asyntotiques donn{\'{e}}es {\`{a}} l'origine au facteur fractionnaire de Bayes nous apportons d'autres raisons qui demontrent le bien fond{\'{e}} de son utilisation dans le domaine d'un {\'{e}}chantillonage fini. Nous prenons aussi en consideration par comparaison d'autres methodes automatiques et nous examinons d'autres caracteristiques telles que la robustesse par rapport aux les distributions {\`{a}} priori et aux donn{\'{e}}es. En conclusion, nous attirons l'attention sur certains probl{\`{e}}mes non encore resolus et proposons des solutions qui peuvent {\`{e}}tre explor{\'{e}}es d'avantage.},
author = {Santis, Fulvio De and Spezzaferri, Fulvio},
doi = {10.1055/s-0031-1292046},
file = {:home/markg/Downloads/Santis{\_}et{\_}al-1999-International{\_}Statistical{\_}Review.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
keywords = {bayes factors,bayesian inference,fractional bayes factor,model comparison},
number = {3},
pages = {267--286},
pmid = {22048938},
title = {{Methods for Default and Robust Bayesian Model Comparison: The Fractional Bayes Factor Approach}},
volume = {67},
year = {1999}
}
@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George andMcCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa- rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for pos- terior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probabil- ity. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I and Mcculloch, Robert E},
doi = {10.1.1.211.4871},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Mcculloch - 1997 - Approaches for Bayesian Variable Selection.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,cal models,conjugate prior,gibbs sampling,gray code,hierarchi-,markov chain monte carlo,metropolis-hastings algorithms,mixtures,normal,normalization constant,regression,simulation},
pages = {339--373},
title = {Approaches for Bayesian Variable Selection},
volume = {7},
year = {1997}
}
@article{Butler2002,
abstract = {In this paper we present Laplace approximations for two functions of matrix argument: the Type I confluent hypergeometric function and the Gauss hypergeometric function. Both of these functions play an important role in distribution theory in multivariate analysis, but ...},
author = {Butler, R.W. and a.T.a. Wood},
file = {:home/markg/Documents/euclid.aos.1031689021.pdf:pdf},
issn = {0090-5364},
journal = {Annals of statistics},
number = {4},
pages = {1155--1177},
title = {{Laplace approximations for hypergeometric functions with matrix argument}},
volume = {30},
year = {2002}
}
@article{Gordy1998,
abstract = {This paper introduces the "compound confluent hypergeometric" (CCH) distribution. The CCH unifies and generalizes three recently introduced generalizations of the beta distributions: the Gauss hypergeometric (GH) distribution of Armero and Bayarri (1994), the generalized beta (GB) distribution of McDonald and Xu (1995), and the confluent hypergeometric (CH) distribution of Gordy (forthcoming). In addition to greater flexibility in fitting data, the CCH offers two useful properties. Unlike the beta, GB and GH, the CCH allows for conditioning on explanatory variables in a natural and convenient way. The CCH family is conjugate for gamma distribution signals, and so may also prove useful in Bayesian analysis. Application of the CCH is demonstrated with two measures of household liquid assets. In each case, the CCH yields a statistically significant improvement in fit over the more restrictive alternatives.},
author = {Gordy, Michael B},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gordy - 1998 - A generalization of generalized beta distributions.pdf:pdf},
number = {202},
pages = {1--28},
title = {{A generalization of generalized beta distributions}},
year = {1998}
}
 
@article{JOFP:rethink,
author = {{Atkins David C.; Gallop}, Robert J},
doi = {10.1037/0893-3200.21.4.726},
journal = {Journal of Family Psychology},
keywords = {Poisson regression,count models,zero-inflated models},
number = {4},
pages = {726--735},
publisher = {American Psychological Association},
title = {{Rethinking How Family Researchers Model Infrequent Outcomes: A Tutorial on Count Regression and Zero-Inflated Models.}},
volume = {21},
year = {2007}
}
 



@article{Spiegelhalter2016,
author = {Spiegelhalter, David J and Carlin, Bradley P},
file = {:home/markg/Downloads/3088806.pdf:pdf},
number = {4},
pages = {583--639},
title = {{Bayesian Measures of Model Complexity and Fit Author ( s ): David J . Spiegelhalter , Nicola G . Best , Bradley P . Carlin and Angelika van der Linde Source : Journal of the Royal Statistical Society . Series B ( Statistical Methodology ), Vol . 64 , Publ}},
volume = {64},
year = {2016}
}
@article{Leamer1978,
abstract = {The computation and selection of constrained regressions may be motivated$\backslash$nby prior information and, if so, a regression selection strategy$\backslash$nreveals the implicit prior. The selection strategies of principal$\backslash$ncomponent regression, stepwise regression, and imposing equality$\backslash$nconstraints are connected with prior densities which are uniform$\backslash$non spheres, hyperbolas, and cones, respectively. Omitting variables$\backslash$nin a predetermined order reveals lexicographic priors.},
author = {Leamer, Edward E},
doi = {10.1080/01621459.1978.10480058},
file = {:home/markg/Documents/2286604.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Regression; Constrained regression; Bayesian infer},
number = {363},
pages = {580--587},
title = {{{\{}R{\}}egression selection strategies and revealed priors}},
volume = {73},
year = {1978}
}
@article{Summation1993,
author = {Summation, Point},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Summation - 1993 - The accuracy.pdf:pdf},
keywords = {1,ams subject classifications,and all,floating point summation,inner products,introduction,means,norms,numbers are ubiquitous in,orderings,primary 65g05,puting,rounding error analysis,scientific com-,secondary 65b10,sums of floating point,they occur when evaluating,variances},
number = {4},
pages = {783--799},
title = {{The accuracy}},
volume = {14},
year = {1993}
}
@article{Foster1994,
author = {Foster, Dean P. and George, Edward I.},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foster, George - 1994 - Risk{\_}Inflation.Pdf.pdf:pdf},
journal = {Annals of Statistics},
number = {4},
pages = {1947--1975},
title = {{The Risk inflation criterion for multiple regression}},
volume = {22},
year = {1994}
}
@article{Casella2006,
abstract = {A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis–Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples.},
author = {Casella, George and Moreno, El{\'{i}}as},
doi = {10.1198/016214505000000646},
file = {:home/markg/Documents/ObjectiveBayes.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hastings algorithm,intrinsic prior,methods,metropolis,monte carlo markov chain,normal linear regression},
number = {473},
pages = {157--167},
title = {{Objective Bayesian Variable Selection}},
volume = {101},
year = {2006}
}
@article{Min01042005,
abstract = {For count responses, the situation of excess zeros (relative to what standard models
allow) often occurs in biomedical and sociological applications. Modeling repeated
measures of zero-inflated count data presents special challenges. This is because in
addition to the problem of extra zeros, the correlation between measurements upon
the same subject at different occasions needs to be taken into account. This article
discusses random effect models for repeated measurements on this type of response
variable. A useful model is the hurdle model with random effects, which separately
handles the zero observations and the positive counts. In maximum likelihood model
fitting, we consider both a normal distribution and a nonparametric approach for the
random effects. A special case of the hurdle model can be used to test for zero
inflation. Random effects can also be introduced in a zero-inflated Poisson or
negative binomial model, but such a model may encounter fitting problems if there is
zero deflation at any settings of the explanatory variables. A simple alternative
approach adapts the cumulative logit model with random effects, which has a single
set of parameters for describing effects. We illustrate the proposed methods with examples.},
author = {Min, Yongyi and Agresti, Alan},
doi = {10.1191/1471082X05st084oa},
journal = {Statistical Modelling},
number = {1},
pages = {1--19},
title = {{Random effect models for repeated measures of zero-inflated count data}},
volume = {5},
year = {2005}
}
@article{DeLeeuw1992,
abstract = {The problem of estimating the dimensionality of a model occurs in various forms in applied statistics: estimating the number of factors in factor analysis, estimating the degree of a polynomial describing the data, selecting the variables to be introduced in a multiple regression equation, estimating the order of an AR or MA time series model, and so on.},
author = {DeLeeuw, J.},
doi = {10.1016/0049-3848(86)90167-2},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeLeeuw - 1992 - Introduction to Akaike (1973) Information Theory and an Extension of the Maximum Likelihood Principle.pdf:pdf},
isbn = {0172-7397},
issn = {00493848},
journal = {Breakthroughs in Statistics Volume I: Foundations and Basic Theory},
number = {May},
pages = {599--609},
pmid = {217},
title = {{Introduction to Akaike (1973) Information Theory and an Extension of the Maximum Likelihood Principle}},
year = {1992}
}
@article{Zucchini2000,
abstract = {This paper is an introduction to model selection intended for nonspecialists who have knowledge of the statistical concepts covered in a typical first (occasionally second) statistics course. The intention is to explain the ideas that generate frequentist methodology for model selection, for example the Akaike information criterion, bootstrap criteria, and cross-validation criteria. Bayesian methods, including the Bayesian information criterion, are also mentioned in the context of the framework outlined in the paper. The ideas are illustrated using an example in which observations are available for the entire population of interest. This enables us to examine and to measure effects that are usually invisible, because in practical applications only a sample from the population is observed. The problem of selection bias, a hazard of which one needs to be aware in the context of model selection, is also discussed. Copyright 2000 Academic Press.},
author = {Zucchini, W},
doi = {10.1006/jmps.1999.1276},
file = {:home/markg/Documents/241801c342dbdcf4cb212eef4634797c15b9.pdf:pdf},
isbn = {00222496},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
number = {1},
pages = {41--61},
pmid = {10733857},
title = {{An Introduction to Model Selection}},
volume = {44},
year = {2000}
}
@article{Wang2015a,
abstract = {Variable selection is a challenging issue in statistical applications when the number of predictors {\$}p{\$} far exceeds the number of observations {\$}n{\$}. In this ultra-high dimensional setting, the sure independence screening (SIS) procedure was introduced to significantly reduce the dimensionality by preserving the true model with overwhelming probability, before a refined second stage analysis. However, the aforementioned sure screening property strongly relies on the assumption that the important variables in the model have large marginal correlations with the response, which rarely holds in reality. To overcome this, we propose a novel and simple screening technique called the high-dimensional ordinary least-squares projection (HOLP). We show that HOLP possesses the sure screening property and gives consistent variable selection without the strong correlation assumption, and has a low computational complexity. A ridge type HOLP procedure is also discussed. Simulation study shows that HOLP performs competitively compared to many other marginal correlation based methods. An application to a mammalian eye disease data illustrates the attractiveness of HOLP.},
archivePrefix = {arXiv},
arxivId = {1506.01782},
author = {Wang, Xiangyu and Leng, Chenlei},
eprint = {1506.01782},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Leng - 2015 - High-dimensional Ordinary Least-squares Projection for Screening Variables.pdf:pdf},
keywords = {consistency,forward regression,generalized inverse,high dimensionality,lasso,marginal correlation,moore-penrose inverse,ordinary least squares,screening,sure independent,variable selection},
pages = {1--47},
title = {{High-dimensional Ordinary Least-squares Projection for Screening Variables}},
year = {2015}
}
@article{Wand2002,
abstract = {Many statisitcal operations benefit from differential calculus. Examples include optimization of likelihood functions and calculation of information matrices. For multiparameter models differential calculuc suited to vector argument functions is usually the most efficient means of performing the required calculations. We present a primer on vector differential calculus and demonstrate its application to statistics through several worked examples.},
author = {Wand, M. P},
doi = {10.1198/000313002753631376},
file = {:home/markg/Downloads/Wand02.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {best linear prediction,generalized linear,generalized linear mixed model,information matrix,matrix differential calculus,maximum likelihood estimation,model,penalized quasi-likelihood,score equation},
number = {1},
pages = {55--62},
title = {{Vector Differential Calculus in Statistics}},
volume = {56},
year = {2002}
}
@article{Wood2008,
abstract = {Existing computationally efficient methods for penalized likelihood generalized additive model fitting employ iterative smoothness selection on working linear models (or working mixed models). Such schemes fail to converge for a non-negligible proportion of models, with failure being particularly frequent in the presence of concurvity. If smoothness selection is performed by optimizing ‘whole model' criteria these problems disappear, but until now attempts to do this have employed finite-difference-based optimization schemes which are computationally inefficient and can suffer from false convergence. The paper develops the first computationally efficient method for direct generalized additive model smoothness selection. It is highly stable, but by careful structuring achieves a computational efficiency that leads, in simulations, to lower mean computation times than the schemes that are based on working model smoothness selection. The method also offers a reliable way of fitting generalized additive mixed models.},
archivePrefix = {arXiv},
arxivId = {0709.3906},
author = {Wood, Simon N.},
doi = {10.1111/j.1467-9868.2007.00646.x},
eprint = {0709.3906},
file = {:home/markg/Downloads/j.1467-9868.2007.00646.x.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Akaike's information criterion,Generalized additive mixed models,Generalized additive models,Generalized approximate cross-validation,Generalized cross-validation,Penalized likelihood,Penalized regression splines,Stable computation},
number = {3},
pages = {495--518},
title = {{Fast stable direct fitting and smoothness selection for generalized additive models}},
volume = {70},
year = {2008}
}
@article{Tan2014,
abstract = {Mixtures of linear mixed models (MLMMs) are useful for clustering grouped data and can be estimated by likelihood maximization through the Expectation–Maximization algorithm. A suitable number of components is then determined conventionally by comparing different mixture models using penalized log-likelihood criteria such as Bayesian information criterion. We propose fitting MLMMs with variational methods, which can perform parameter estimation and model selection simultaneously. We describe a variational approximation for MLMMs where the variational lower bound is in closed form, allowing for fast evaluation and develop a novel variational greedy algorithm for model selection and learning of the mixture components. This approach handles algorithm initialization and returns a plausible number of mixture components automatically. In cases of weak identifiability of certain model parameters, we use hierarchical centering to reparameterize the model and show empirically that there is a gain in efficiency in ...},
archivePrefix = {arXiv},
arxivId = {1112.4675},
author = {Tan, Siew Li and Nott, David J.},
doi = {10.1080/10618600.2012.761138},
eprint = {1112.4675},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Nott - 2014 - Variational Approximation for Mixtures of Linear Mixed Models.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Hierarchical centering,Mixture models,Model selection,Variational Bayes},
number = {January 2015},
pages = {564--585},
title = {{Variational Approximation for Mixtures of Linear Mixed Models}},
volume = {23},
year = {2014}
}
@article{Amari1998b,
author = {Amari, Shun-ichi and Amari, Shun-ichi},
file = {:home/markg/Downloads/Amari1998a.pdf:pdf},
journal = {Neural Computation},
pages = {251--276},
title = {{Natural Gradient Works Ef ciently in Learning}},
volume = {276},
year = {1998}
}
@article{Wilson2015,
archivePrefix = {arXiv},
arxivId = {1511.02222},
author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1511.02222},
file = {:home/markg/Dropbox/Downloads/1611.00336.pdf:pdf},
journal = {Artificial Intelligence and Statistics (AISTATS)},
number = {Nips},
pages = {1--19},
title = {{Deep Kernel Learning}},
volume = {51},
year = {2015}
}
@article{Amari1998,
abstract = {Gradient adaptation is a useful technique for adjusting a set of parameters to minimize a cost function. While often easy to implement, the convergence speed of gradient adaptation can be slow when the slope of the cost function varies widely for small changes in the parameters. In this paper, we outline an alternative technique, termed natural gradient adaptation, that overcomes the poor convergence properties of gradient adaptation in many cases. The natural gradient is based on differential geometry and employs knowledge of the Riemannian structure of the parameter space to adjust the gradient search direction. Unlike Newton's method, natural gradient adaptation does not assume a locally-quadratic cost function. Moreover, for maximum likelihood estimation tasks, natural gradient adaptation is asymptotically Fisher-efficient. A simple example illustrates the desirable properties of natural gradient adaptation.},
author = {Amari, S and Douglas, Sc},
doi = {10.1109/ICASSP.1998.675489},
isbn = {0-7803-4428-6},
issn = {1520-6149},
journal = {{\ldots} , 1998. Proceedings of the 1998 IEEE {\ldots}},
pages = {1213--1216},
title = {{Why natural gradient?}},
volume = {9},
year = {1998}
}
@article{Nickisch2008,
abstract = {We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.},
author = {Nickisch, Hannes and Rasmussen, Carl Edward},
file = {:home/markg/Downloads/nickisch08a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with Statistics,Theory {\&} Algorithms},
pages = {2035--2078},
title = {{Approximations for Binary Gaussian Process Classification}},
volume = {9},
year = {2008}
}

@article{You2014,
author = {You, Chong and Ormerod, John T},
file = {:home/markg/Dropbox/Downloads/linearmodelv7.pdf:pdf},
keywords = {akaike information criterion,bayesian information criterion,consistency,deviance information criterion,markov chain monte carlo},
number = {2},
title = {{On Variational Bayes Estimation and Variational Bayes Information Criteria for Linear Regression Models}},
volume = {61},
year = {2014}
}
 
@article{Tarr2015,
abstract = {The mplot package provides an easy to use implementation of model stability and variable inclusion plots (M$\backslash$"uller and Welsh 2010; Murray, Heritier, and M$\backslash$"uller 2013) as well as the adaptive fence (Jiang, Rao, Gu, and Nguyen 2008; Jiang, Nguyen, and Rao 2009) for linear and generalised linear models. We provide a number of innovations on the standard procedures and address many practical implementation issues including the addition of redundant variables, interactive visualisations and approximating logistic models with linear models. An option is provided that combines our bootstrap approach with glmnet for higher dimensional models. The plots and graphical user interface leverage state of the art web technologies to facilitate interaction with the results. The speed of implementation comes from the leaps package and cross-platform multicore support.},
archivePrefix = {arXiv},
arxivId = {1509.07583},
author = {Tarr, Garth and M{\"{u}}ller, Samuel and Welsh, Alan},
eprint = {1509.07583},
file = {:home/markg/Documents/1509.07583v1.pdf:pdf},
keywords = {generalised linear,linear models,mixed models,model selection,variable selection},
number = {1996},
title = {{mplot: An R Package for Graphical Model Stability and Variable Selection Procedures}},
year = {2015}
}
@article{Vatsa2014,
author = {Vatsa, Richa and Wilson, Simon},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vatsa, Wilson - 2014 - Variational Bayes Approximation for Inverse Non-Linear Regression.pdf:pdf},
issn = {2278-2273},
keywords = {1,2278-2273,2348-7909,e-issn,earch,inverse non-linear,journal of statistics,p-issn,reviews,special issue on recent,statistical methodologies and applications,variational bayes approximation for,vol},
number = {JANUARY 2014},
pages = {76--84},
title = {{Variational Bayes Approximation for Inverse Non-Linear Regression}},
year = {2014}
}
@article{Journal2017,
author = {Journal, Source and Statistical, Royal and Series, Society and Methodology, B Statistical},
file = {:home/markg/Documents/3088868.pdf:pdf},
number = {2},
pages = {413--428},
title = {{Modelling and Smoothing Parameter Estimation with Multiple Quadratic Penalties Author ( s ): S . N . Wood Published by : Wiley for the Royal Statistical Society Stable : Modelling and smoothing parameter estimation with multiple quadratic penalties}},
volume = {62},
year = {2017}
}
@article{McGrory2007,
abstract = {Variational methods, which have become popular in the neural computing/machine learning literature, are applied to the Bayesian analysis of mixtures of Gaussian distributions. It is also shown how the deviance information criterion, (DIC), can be extended to these types of model by exploiting the use of variational approximations. The use of variational methods for model selection and the calculation of a DIC are illustrated with real and simulated data. The variational approach allows the simultaneous estimation of the component parameters and the model complexity. It is found that initial selection of a large number of components results in superfluous components being eliminated as the method converges to a solution. This corresponds to an automatic choice of model complexity. The appropriateness of this is reflected in the DIC values. ?? 2006 Elsevier B.V. All rights reserved.},
author = {McGrory, C. A. and Titterington, D. M.},
doi = {10.1016/j.csda.2006.07.020},
file = {:home/markg/Documents/1-s2.0-S0167947306002362-main.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Bayesian analysis,Deviance information criterion (DIC),Mixtures,Variational approximations},
number = {11},
pages = {5352--5367},
title = {{Variational approximations in Bayesian model selection for finite mixture distributions}},
volume = {51},
year = {2007}
}


@article{GelmanEtal1996,
	author = {Andrew Gelman and Xiao-Li Meng and Hal Stern},
	journal = {Statistica Sinica},
	number = {4},
	pages = {733-760},
	title = {POSTERIOR PREDICTIVE ASSESSMENT OF MODEL FITNESS VIA REALIZED DISCREPANCIES},
	volume = {6},
	year = {1996}
}

@article{Berk1966,
	author = {Berk, R.}, 
	year = {1966},
	title = {Limiting behavior of posterior distributions when the model is incorrect},
	journal = {Annals of Mathematical Statistics}, 
	volume = {37}, 
	page = {51--58}
}



@incollection{Dmochowski1996,
	author = {Dmochowski, J.},
	editor = {J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith},
	title = {{Intrinsic priors via Kullback-Leibler geometry}},
	booktitle = {Bayesian Statistics 5},
	publisher = {Oxford University Press},
	pages = {543--549},
	year = {1996}
}


@TechReport{BenjaminEtal2017,
	author={Daniel Benjamin and James Berger and Magnus Johannesson and Brian Nosek and E. Wagenmakers and Richard Berk and Kenneth Bollen and Bjorn Brembs and Lawrence Brown and Colin Camerer and David Cesarini and Christopher Chambe},
	title={{Redefine Statistical Significance}},
	year=2017,
	institution={The Field Experiments Website},
	type={Artefactual Field Experiments},
	number={00612},
	abstract={We propose to change the default P-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005.},
	keywords={},
	doi={},
}

@article{Naaman2016,
	author = {Naaman, M.},
	title = {{Almost sure hypothesis testing and the resolution of the Jeffreys-Lindey paradox}},
	journal = {Electronic Journal of Statistics},
	year = {2016},
	volume = {19},
	pages = {1526--1550}
}

 
@Article{Aitkin1991,
	author = {Aitkin, Murray},
	title = {{Posterior Bayes factors (with discussion).}},
	journal = {Journal of the Royal Statistical Society, Series B},
	volume = {53},
	number = {1},
	pages = {111--142},
	year = {1991}
}

@Article{vaart98,
	author = {van der Vaart, A. W.}, 
	title = {{Asymptotic statistics, volume 3 of Cambridge Series in Statistical and Probabilistic Mathematics}},
	publisher = {Cambridge University Press, Cambridge}, 
	year = {1998}
}  

@Article{vuong89,
	author = {Vuong, Quang H.}, 
	title = {Likelihood ratio tests for model selection and nonnested hypotheses}, 
	journal = {Econometrica},
	volume = {57},
	number = {2},
	pages = {307--333}, 
	year = {1989}
}



@Article{Aitkin1997,
	author={Aitkin, Murray},
	title={{The calibration of P-values, posterior Bayes factors and the AIC from the posterior distribution of the likelihood}},
	journal={Statistics and Computing},
	year={1997},
	volume={7},
	number={4},
	pages={253--261}
}

@Article{Barbieri2004,
	author = {Barbieri, Maria Maddalena and Berger, James O.},
	journal = {The Annals of Statistics},
	number = {3},	
	pages = {870--897},
	title = {Optimal predictive model selection},
	volume = {32},
	year = {2004}
}

@Article{Bayarri2012,
	author = {Bayarri, M. J. and Berger, J. O. and Forte, A. and Garc\'ia-Donato, G.},
	journal = {The Annals of Statistics},
	number = {3},
	pages = {1550--1577},
	title = {{Criteria for Bayesian model choice with application to variable selection}},
	volume = {40},
	year = {2012}
}

@article{Berger1996,
	author = {Berger, J. O. and  Pericchi, L. R.},
	journal = {Journal of the American Statistical Association},
	number = {433},
	pages = {109--122},
	title = {{The intrinsic Bayes factor for model selection and prediction}},
	volume = {91},
	year = {1996}
}
@article{Berger1998,
	author = {Berger, J. O. and Pericchi, L. R. and Varshavsky, J. A.},
	journal = {Sankhy\={a}: The Indian Journal of Statistics, Series A},
	number = {3},
	pages = {307--321},
	publisher = {Springer},
	title = {Bayes factors and marginal distributions in invariant situations},
	volume = {60},
	year = {1998}
}

@article{Berger1987,
	author = {Berger, J. O. and Sellke, T.},
	journal = {Journal of the American Statistical Association},
	number = {397},
	pages = {112--122},
	title = {{Testing a point null hypothesis: The irreconcilability of $p$-values and evidence}},
	volume = {82},
	year = {1987}
}

@article{Bernardo1999,
	Author = {Bernardo, Jose M.},
	Title = {{Nested hypothesis testing: The Bayesian reference criterion}},
	Journal = {Bayesian Statistics},
	Vol = {6},
	Year = {1999},
	Pages = {101--130},
}

@article{Bove2011,
	author = {Bov\'e, D. S. and Held, L.},
	journal = {Bayesian Analysis},
	number = {3},
	pages = {387--410},
	title = {Hyper-$g$ priors for generalized linear models},
	volume = {6},
	year = {2011}
}

@article{Chen2003,
	author = {Chen, M. -H. and Ibrahim, J. G.},
	journal = {Statistica Sinica},
	number = {2},
	pages = {461-476},
	title = {Conjugate priors for generalized linear models},
	volume = {13},
	year = {2003}
}

@article{Fernandez2001,
	author = {Fern\'andez, C. and Ley, E. and Steel, M. F. J.},
	title = {{Benchmark priors for Bayesian model averaging}},
	journal = {Journal of Econometrics},
	volume = {100},
	number = {2},
	pages = {381 - 427},
	year = {2001}
}
@article{Fouskakis2009,
	author = {Fouskakis, D. and Ntzoufras, I. and Draper, D.},
	journal = {The Annals of Applied Statistics},
	number = {2},
	pages = {663--690},
	title = {{Bayesian variable selection using cost-adjusted BIC, with application to cost-effective measurement of quality of health care}},
	volume = {3},
	year = {2009}
}
@book{Gelman2013,
	title={{Bayesian Data Analysis, 3rd Edition}},
	author={Gelman, A. and Carlin, J. B. and Stern, H. S. and Dunson, D. B. and Vehtari, A. and Rubin, D. B.},
	series={Chapman \& Hall/CRC Texts in Statistical Science},
	year={2013},
	publisher={Taylor \& Francis}
}
@article{Hansen2001,
	author = {Hansen, M. H. and Yu, B.},
	title = {{Model selection and the principle of minimum description length}},
	journal = {Journal of the American Statistical Association},
	volume = {96},
	number = {454},
	pages = {746-774},
	year = {2001},
}
@article{Johnson2012,
	author = {Valen E. Johnson and David Rossell},
	title = {Bayesian Model Selection in High-Dimensional Settings},
	journal = {Journal of the American Statistical Association},
	volume = {107},
	number = {498},
	pages = {649-660},
	year = {2012},
}
@article{Krishna2009,
	Author = {Krishna, Arun and Bondell, Howard D. and Ghosh, Sujit K.},
	Title = {Bayesian variable selection using an adaptive powered correlation prior},
	Journal = {Journal of Statistical Planning and Inference},
	Volume = {139},
	Number = {8},
	Year = {2009},
	Pages = {2665--2674},
}
 

@book{lehmann2004,
	title={Elements of Large-Sample Theory},
	author={Lehmann, E.L.},
	isbn={9780387985954},
	lccn={98034429},
	series={Springer Texts in Statistics},
	year={2004},
	publisher={Springer New York}
}


@article{Lindley1968,
	author = {D. V. Lindley},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {31-66},
	title = {The Choice of Variables in Multiple Regression},
	volume = {30},
	year = {1968}
}
@article{Lindley1972,
	author = {D. V. Lindley and A. F. M. Smith},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	pages = {1-41},
	title = {Bayes estimates for the linear model},
	volume = {34},
	year = {1972}
}
 
@article{Narisetty2014,
	author = "Narisetty, Naveen Naidu and He, Xuming",
	journal = "The Annals of Statistics",
	number = "2",
	pages = "789--817",
	publisher = "The Institute of Mathematical Statistics",
	volume = "42",
	year = "2014"
}
@article{OHagan1995,
	author = {O'Hagan, A.},
	journal = {Journal of the Royal Statistical Society, Series B},
	number = {1},	
	pages = {99-138},
	title = {{Fractional Bayes factors for model comparison}},
	volume = {57},
	year = {1995}
}
@article{Pettit1992,
	Author = {Pettit, Lawrence I.},
	Title = {Bayes factors for outlier models using the device of imaginary observations},
	Journal = {Journal of the American Statistical Association},
	Volume = {87},
	Year = {1992},
	Pages = {541--545},
}
@book{Shao2003,
	title={Mathematical Statistics},
	author={Shao, J.},
	isbn={9780387953823},
	series={Springer Texts in Statistics},
	year={2003},
	publisher={Springer}
}
@article{Spanos2013,
	pages = {73--93},
	volume = {80},
	year = {2013},
	number = {1},
	author = {Aris Spanos},
	journal = {Philosophy of Science},
	title = {{Who should be afraid of the Jeffreys-Lindley paradox?}}
}
@article {Spiegelhalter2014,
	author = {Spiegelhalter, D. J. and Best, N. G. and Carlin, B. P. and van der Linde, A.},
	title = {The deviance information criterion: 12 years on},
	journal = {Journal of the Royal Statistical Society, Series B},
	volume = {76},
	number = {3},
	pages = {485--493},
	year = {2014},
}

@techreport{Zellner1980a,
	author  = {Zellner, A.},
	title   = {{On Bayesian regression analysis with $g$ prior distributions}},
	year    = "1980",
	note    = "Working paper",
}

 

@book{Abramowitz1972,
	author = {Abramowitz, M. and Stegun, I. A.},
	title = {{Handbook of Mathematical Functions with Formulas,
	Graphs, and Mathematical Tables}},
	publisher = {New York: Dover Publications},
	year = {1972}
}





@article{Madigan1994,
	author = {Madigan, D. and Raftery, A. E.},
	title = {{Model selection and accounting for model uncertainty in graphical models using Occam's window}},
	journal = {Journal of the American Statistical Association},
	volume = {89},
	pages = {1335--1346},
	year = {1994}
}
@article{Ishwaran2005,
	author = {Ishwaran, Hemant and Rao, J. Sunil},
	journal = "The Annals of Statistics",	
	number = "2",
	pages = "730--773",
	title = {{Spike and slab variable selection: Frequentist and Bayesian strategies}},
	volume = "33",
	year = "2005"
}
@article{George2000,
	author = {George, E. I. and Foster, D. P.},
	title = {{Calibration and empirical Bayes variable selection}},
	journal = {Biometrika},
	volume = {87},
	number = {4},
	pages = {731},	
	year = {2000}
}
@article{Yang2005,
	author = {Yang, Yuhong},
	title = {{Can the strengths of AIC and BIC be shared? A conflict between model indentification and regression estimation}},
	journal = {Biometrika},
	volume = {92},
	number = {4},
	pages = {937},
	year = {2005}
}
@techreport{Li2015,
	author  = "Li, Y. and Clyde, M. A.",
	title   = "Mixture of g-priors in generalized linear models.",
	year    = "2015",
	note    = "ArXiv e-prints",
}
@article{Kass1995b,
	author = {Robert E. Kass  and  Larry   Wasserman},
	title = {{A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion}},
	journal = {Journal of the American Statistical Association},
	volume = {90},
	number = {431},
	pages = {928--934},
	year = {1995}
}


@TECHREPORT{Gordy1998,
	title = {A generalization of generalized beta distributions},
	author = {Gordy, Michael},
	year = {1998},
	institution = {Board of Governors of the Federal Reserve System (U.S.)},
	type = {Finance and Economics Discussion Series},
	number = {1998-18}
}
@article{Wilks1938,
	author = "Wilks, S. S.",
	journal = "The Annals of Mathematical Statistics",
	number = "1",
	pages = "60--62",
	title = "The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses",
	volume = "9",
	year = "1938"
}


@article{Nadarajah2015,
	author = {Saralees Nadarajah},
	title = {{On the computation of Gauss hypergeometric functions}},
	journal = {The American Statistician},
	volume = {69},
	number = {2},
	pages = {146--148},
	year = {2015}
}





@article{Zellner1983,
	author = {Zellner, A.},
	journal = {The Statistician},
	number = {1},
	pages = {23--24},
	title = {{Applications of Bayesian Analysis in Econometrics}},
	volume = {32},
	year = {1983}
}
@article{Robert1993,
	author = {Robert, C. P.},
	journal = {Statistica Sinica},
	pages = {601--608},
	title = {{A note on Jeffreys-Lindley paradox}},
	volume = {3},
	year = {1993}
}
@article{OHara2009,
	author = {O'Hara, R. B. and Sillanp{\"{a}}{\"{a}}, M. J.},
	title = {Bayesian Analysis},
	number = {1},
	pages = {85--118},
	title = {{A review of bayesian variable selection methods: What, how and which}},
	volume = {4},
	year = {2009}
}
@article{Nott2005,
	author = {Nott, D. J. and Kohn, R.},
	journal = {Biometrika},
	number = {4},
	pages = {747--763},
	title = {{Adaptive sampling for Bayesian variable selection}},
	volume = {92},
	year = {2005}
}
 
@article{Lindley1997,
	author = {Lindley, D. V.},
	journal = {Journal of Statistical Planning and Inference},
	pages = {181--189},
	title = {{Discussion forum: Some comments on Bayes factors}},
	volume = {61},
	year = {1997}
}
@article{Lindley1962,
	author = {Lindley, D. V.},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	pages = {285--287},
	title = {{Discussion on Professor Steins paper}},
	volume = {24},
	year = {1962}
}
@article{Lindley1957,
	author = {Lindley, D. V.},
	journal = {Biometrika},
	pages = {187--192},
	title = {{A statistical paradox}},
	volume = {44},
	year = {1957}
}
@article{Kass1995,
	author = {Kass, R. E. and Raftery, A.},
	journal = {Journal of the American Statistical Association},
	number = {6},
	pages = {773--795},
	title = {{Bayes factors}},
	volume = {91},
	year = {1995}
}

@inbook{KassEtal1990,
	author = {Kass, R. E. and Tierney, L. and Kadane, J. B.},
	title = {{The validity of posterior expansions based on Laplace's method}}, 
	booktitle = {{Essays in Honor of George Bernard}},
	editor = {S. Geisser, J.S. Hodges, S.J. Press, and A. Zellner},
	publisher = {North Holland},
	location = {Amsterdam},
	pages = {473-488},
	year = {1990}
}

@Article{Pearson2017,
	author="Pearson, John W.
	and Olver, Sheehan
	and Porter, Mason A.",
	title="Numerical methods for the computation of the confluent and Gauss hypergeometric functions",
	journal="Numerical Algorithms",
	year="2017",
	volume="74",
	number="3",
	pages="821--866"
}



@book{Jeffreys1961,
	author = {Jeffreys, H.},
	booktitle = {Theory of Probability},
	pages = {1--40},
	title = {{Theory of Probability}},
	publisher = {Oxford University Press},
	volume = {2},
	year = {1961}
}
@book{Berger1985,
	address = {New York},
	author = {Berger, J. O.},
	edition = {2nd edition},
	publisher = {Springer-Verlag},
	title = {{Statistical Decision Theory and Bayesian Analysis}},
	year = {1985}
}
@article{Berger1987,
	author = {Berger, J. O. and Delampady, M.},
	journal = {Statistical Science},
	number = {3},
	pages = {348--352},
	title = {{Testing Precise Hypotheses}},
	volume = {2},
	year = {1987}
}
@article{Bartlett1957,
	author = {Bartlett, M. S.},
	journal = {Biometrika},
	number = {3},
	pages = {533--534},
	title = {{A comment on D.V. Lindley's statistical paradox}},
	volume = {44},
	year = {1957}
}
@incollection{Akaike1973,
	author = {Akaike, H.},
	booktitle = {Second International Symposium on Information Theory},
	pages = {267--281},
	title = {{Information theory and an extension of the maximum likehood principle}},
	year = {1973}
}
@article{Aitkin1992,
	author = {Aitkin, M.},
	journal = {Mathematical Scientist},
	pages = {15--25},
	title = {{Evidence and the posterior Bayes factor}},
	volume = {17},
	year = {1992}
}
@article{Berger2001,
	author = {Berger, J. O. and Pericchi, L. R.},
	journal = {IMS Lecture Notes - Monograph Series},
	keywords = {Mathematical statistics,Statistics,methods,statistic},
	pages = {135--207},
	title = {{Objective Bayesian methods for model selection: Introduction and comparison}},
	volume = {38},
	year = {2001}
}
@article{Bernardo1979,
	author = {Bernardo, J. M.},
	booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {2},
	pages = {113--147},
	title = {{Reference Posterior Distributions for Bayesian Inference}},
	volume = {41},
	year = {1979}
}
@article{Feldkircher2009,
	author = {Feldkircher, M. and Zeugner, S.},
	journal = {IMF Working Papers},
	number = {202},
	pages = {1},
	title = {{Benchmark Priors Revisited:on Adaptive Shrinkage and the Supermodel Effect in Bayesian Model Averaging}},
	volume = {09},
	year = {2009}
}
@article{Fouskakis2016,
	author = {Fouskakis, D. and Ntzoufras, I.},
	journal = {Journal of Computational and Graphical Statistics},
	title = {{Power-Conditional-Expected Priors : Using g -priors with Random Imaginary Data for Variable Selection}},
	volume = {25},
	issue = {3},
	year = {2016}
}
@article{George1993,
	author = {George, E. I. and McCulloch, R. E.},
	journal = {Journal of the American Statistical Association},
	pages = {881--889},
	title = {{Variable selection via Gibbs sampling}},
	volume = {88},
	number = {423},
	year = {1993}
}
@article{Chen2008,
	author = {Chen, M. H. and Huang, L. and Ibrahim, J. G. and Kim, S.},
	journal = {Bayesian Analysis},
	number = {3},
	pages = {585--614},
	title = {{Bayesian variable selection and computation for generalized linear models with conjugate priors}},
	volume = {3},
	year = {2008}
}

@article{Bernardo2012,
	author = {Bernardo, J. M. and Pericchi, L.},
	journal = {In Bayesian Statistics 9: Proceedings of the Ninth Valencia Meeting},
	pages = {1--68},
	title = {{Integrated Objective Bayesian Estimation and Hypothesis Testing}},
	year = {2012}
}
@article{Schwarz1978,
	author = {Schwarz, G.},
	journal = {The Annals of Statistics},
	number = {2},
	pages = {461--464},
	title = {{Estimating the dimension of a model}},
	volume = {6},
	year = {1978}
}
@article{Gupta2009,
	author = {Gupta, M. and Ibrahim, J. G.},
	journal = {Statistica Sinica},
	number = {4},
	pages = {1641--1663},
	title = {{An information matrix prior for Bayesian analysis in generalized linear models with high dimensional data.}},
	volume = {19},
	year = {2009}
}
@article{Li2010,
	author = {Li, F. and Zhang, N. R.},
	journal = {Journal of the American Statistical Association},
	number = {491},
	pages = {1202--1214},
	title = {{Bayesian Variable Selection in Structured High-Dimensional Covariate Spaces With Applications in Genomics}},
	volume = {105},
	year = {2010}
}
@article{Tierney1989,
	author = {Tierney, L. and Kass, R. E. and Kadane, J. B.},
	journal = {Journal of the American Statistical Association},
	number = {407},
	pages = {710--716},
	title = {{Fully exponential Laplace approximations to expectations and variances of nonpositive functions}},
	volume = {84},
	year = {1989}
}
@article{Spiegelhalter2002,
	author = {David J. Spiegelhalter and Nicola G. Best and Bradley P. Carlin and Angelika van der Linde},
	number = {4},
	pages = {583--639},
	title = {{Bayesian measures of model complexity and fit}},
	journal = {Journal of the Royal Statistical Society, Series B},
	volume = {64},
	year = {2002}
}

@article{Jahn1987,
	author = {Jahn, R. G. and Dunne, B. J. and Nelson, R. D.},
	number = {1},
	pages = {583--639},
	journal = {Journal of Scientific Exploration},
	title = {{Engineering anomalies research}},
	volume = {1},
	year = {1987}
}

 


@book{PressEtal2007,
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	title = {Numerical Recipes in C (3rd Ed.): The Art of Scientific Computing},
	year = {2007},
	publisher = {Cambridge University Press},
	address = {New York, NY, USA}
} 


@article{Bayarri2007,
	author = {M. J. Bayarri and Gonzalo Garc\'{i}a-Donato},
	journal = {Biometrika},
	number = {1},
	pages = {135-152},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Extending Conventional Priors for Testing General Hypotheses in Linear Models},
	volume = {94},
	year = {2007}
}



@article {Jeffreys1946,
	author = {Jeffreys, Harold},
	title = {An Invariant Form for the Prior Probability in Estimation Problems},
	volume = {186},
	number = {1007},
	pages = {453--461},
	year = {1946},
	publisher = {The Royal Society},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences}
}

@article{Jeffreys1935,
	title = "Some Tests of Significance, Treated by the Theory of Probability",
	publisher = "Cambridge University Press",
	address = "Cambridge, UK",
	author = "Harold Jeffreys",
	volume = "31",
	number = "2",
	pages = "203--222",
	year = "1935",
	journal = "Mathematical Proceedings of the Cambridge Philosophical Society"
}




@article{Robert2014,
	author = {Christian P. Robert},
	title = {{On the Jeffreys-Lindley paradox}},
	journal = {Philosophy of Science},
	volume = {81},
	number = {2},
	pages = {216--232},
	year = {2014}
}


@article{DeGroot1973,
	author = {DeGroot, M. H.},
	title = {Doing what comes naturally: Interpreting a tail area as a poterior probability or as a likelihood ratio},
	journal = {Journal of the American Statistical Association},
	volume = {68},
	pages = {966--969},
	year = {1973}
}


@Article{Bernardo1980,
	author="Bernardo, Jos{\'e} M.",
	title={{A Bayesian analysis of classical hypothesis testing}},
	journal="Trabajos de Estadistica Y de Investigacion Operativa",
	year="1980",
	volume="31",
	number="1",
	pages="605--647"
}



@article{Smith1980,
	author = {Smith, A. F. M. and Spiegelhalter, D. J.},
	title = {Bayes factors and choice criteria for linear models},
	journal = {Journal of the Royal Statistical Society, Series B},
	volume = {42},
	number = {2},
	year = {1980},
	pages = {213--220}
}

@article{Spiegelhalter1982,
	author = {Spiegelhalter, D. J. and Smith, A. F. M.},
	title = {Bayes factors for linear and log-linear models with vague prior information},
	journal = {Journal of the Royal Statistical Society, Series B},
	volume = {44},
	number = {3},
	year = {1982},
	pages = {377--387}
}

@article{OHagan1997,
	author = {O'Hagan, A.},
	title = {{Properties of intrinsic and fractional Bayes factors}},
	journal = {Test},
	volume = {6},
	year = {1997},
	pages = {101--118}
}

@article{Good1952,
	author = {Good, I. J.},
	title = {{Rational decisions}},
	journal = {Journal of the Royal Statistical Society, Series B},
	volume = {14},
	number = {1},
	year = {1952},
	pages = {107--114}
}

@Article{Zellner1980b,
	author="Zellner, A.
	and Siow, A.",
	title="Posterior odds ratios for selected regression hypotheses",
	journal="Trabajos de Estadistica Y de Investigacion Operativa",
	year="1980",
	volume="31",
	number="1",
	pages="585--603",
}


@book{Marin2007,
	author = {Marin, J. -M. and Robert, C. P.},
	title = {Bayesian Core: A Practical Approach to Computational Bayesian Statistics},
	year = {2007},
	publisher = {Springer},
	location = {New York}
}

@Article{Hanson2014,
	author="Hanson, T. E. and Branscum, A. J. and Johnson, W. O.",
	title="Informative g-Priors for Logistic Regression",
	journal="Bayesian Analysis",
	year="2014",
	volume="9",
	number="3",
	pages="597--612",
}

@Article{Wang2007,
	author="Wang, X. and George, E. I.",
	title={{Adaptive Bayesian criteria in variable selection for generalized linear models}},
	journal="Statistics Sinica",
	year="2007",
	volume="17",
	pages="667--690",
}


@article{Sprenger2013,
	author = {Jan Sprenger},
	title = {{Testing a precise null hypothesis: The case of Lindley's paradox}},
	journal = {Philosophy of Science},
	volume = {80},
	number = {5},
	pages = {733--744},
	year = {2013}
}

@article{Inglot2010,
	Author = {Inglot, Tadeusz},
	Title = {Inequalities for quantiles of the chi-square distribution},
	Journal = {Probability and Mathematical Statistics},
	Volume = {30},
	Number = {2},
	Year = {2010},
	Pages = {339--351}
}

@article{Laurent2000,
	author = "Laurent, B. and Massart, P.",
	journal = "The Annals of Statistics",
	month = "10",
	number = "5",
	pages = "1302--1338",
	publisher = "The Institute of Mathematical Statistics",
	title = "Adaptive estimation of a quadratic functional by model
	selection",
	volume = "28",
	year = "2000"
}

@book{PrudnikovEtal1986,
	author = {Prudnikov, A. P. and Brychkov, Y. A. and Marichev, O. I.},
	year = {1986},
	title = {Integrals and Series (Vols. 1–3)},
	publisher = {Gordon and Breach},
	location = {Amsterdam}
}




@Manual{Weisstein2009,
	author = {Weisstein, E. W.},
	year = {2009},
	title = {Appell Hypergeometric Function},
	OPTorganization = {MathWorld–A Wolfram Web
	Resource}

}

@article{Hastie2015,
abstract = {Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of l1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso. In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling.},
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
doi = {10.1201/b18401-1},
file = {:home/markg/Downloads/SLS.pdf:pdf},
isbn = {978-1-4987-1217-0},
issn = {0306-7734},
journal = {Crc},
pages = {362},
title = {{Statistical Learning with Sparsity: The Lasso and Generalizations}},
year = {2015}
}

@article{Hahn2015,
abstract = {Selecting a subset of variables for linear models remains an active area of research. This paper reviews many of the recent contributions to the Bayesian model selection and shrink- age prior literature. A posterior variable selection summary is proposed, which distills a full posterior distribution over regression coefficients into a sequence of sparse linear predictors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.0464v1},
author = {Hahn, P. Richard and Carvalho, Carlos M.},
doi = {10.1080/01621459.2014.993077},
eprint = {arXiv:1408.0464v1},
file = {:home/markg/Downloads/1408.0464.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Decision theory,Linear regression,Loss function,Model selection,Parsimony,Shrinkage prior,Sparsity,Variable selection},
number = {509},
pages = {435--448},
title = {{Decoupling shrinkage and selection in bayesian linear models: A posterior summary perspective}},
volume = {110},
year = {2015}
}

@article{Chipman2014,
author = {Chipman, Hugh and George, Edward I and Mcculloch, Robert E and Clyde, M and Foster, Dean P and Stine, Robert},
doi = {10.1214/lnms/1215540964},
file = {:home/markg/Downloads/ims.pdf:pdf},
isbn = {0-940600-52-8},
issn = {0749-2170},
journal = {Lecture Notes-Monograph Series},
number = {2001},
pages = {65--134},
title = {{The Practical Implementation of Bayesian Model Selection}},
volume = {38},
year = {2014}
}

@article{Breheny2011,
abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
archivePrefix = {arXiv},
arxivId = {1104.2748},
author = {Breheny, Patrick and Huang, Jian},
doi = {10.1214/10-AOAS388},
eprint = {1104.2748},
file = {:home/markg/Downloads/1104.2748.pdf:pdf},
isbn = {19326157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Coordinate descent,Lasso,MCP,Optimization,Penalized regression,SCAD},
number = {1},
pages = {232--253},
pmid = {22081779},
title = {{Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection}},
volume = {5},
year = {2011}
}

@article{Breheny2011,
abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
archivePrefix = {arXiv},
arxivId = {1104.2748},
author = {Breheny, Patrick and Huang, Jian},
doi = {10.1214/10-AOAS388},
eprint = {1104.2748},
file = {:home/markg/Downloads/1104.2748.pdf:pdf},
isbn = {19326157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Coordinate descent,Lasso,MCP,Optimization,Penalized regression,SCAD},
number = {1},
pages = {232--253},
pmid = {22081779},
title = {{Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection}},
volume = {5},
year = {2011}
}

@article{Breheny2011,
abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
archivePrefix = {arXiv},
arxivId = {1104.2748},
author = {Breheny, Patrick and Huang, Jian},
doi = {10.1214/10-AOAS388},
eprint = {1104.2748},
file = {:home/markg/Downloads/1104.2748.pdf:pdf},
isbn = {19326157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Coordinate descent,Lasso,MCP,Optimization,Penalized regression,SCAD},
number = {1},
pages = {232--253},
pmid = {22081779},
title = {{Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection}},
volume = {5},
year = {2011}
}

@article{Breheny2011,
abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
archivePrefix = {arXiv},
arxivId = {1104.2748},
author = {Breheny, Patrick and Huang, Jian},
doi = {10.1214/10-AOAS388},
eprint = {1104.2748},
file = {:home/markg/Downloads/1104.2748.pdf:pdf},
isbn = {19326157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Coordinate descent,Lasso,MCP,Optimization,Penalized regression,SCAD},
number = {1},
pages = {232--253},
pmid = {22081779},
title = {{Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection}},
volume = {5},
year = {2011}
}

@article{Chen2008,
abstract = {The ordinary Bayesian information criterion is too liberal for model selection when the model space is large. In this paper, we re-examine the Bayesian paradigm for model selection and propose an extended family of Bayesian information criteria, which take into account both the number of unknown parameters and the complexity of the model space. Their consistency is established, in particular allowing the number of covariates to increase to infinity with the sample size. Their performance in various situations is evaluated by simulation studies. It is demonstrated that the extended Bayesian information criteria incur a small loss in the positive selection rate but tightly control the false discovery rate, a desirable property in many applications. The extended Bayesian information criteria are extremely useful for variable selection in problems with a moderate sample size but with a huge number of covariates, especially in genome-wide association studies, which are now an active area in genetics research.},
author = {Chen, Jiahua and Chen, Zehua},
doi = {10.1093/biomet/asn034},
file = {:home/markg/Downloads/asn034.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
keywords = {Bayesian paradigm,Consistency,Genome-wide association study,Tournament approach,Variable selection},
number = {3},
pages = {759--771},
title = {{Extended Bayesian information criteria for model selection with large model spaces}},
volume = {95},
year = {2008}
}

@book{Van_Rijsbergen1979,
 author = {Rijsbergen, C. J. Van},
 title = {Information Retrieval},
 year = {1979},
 isbn = {0408709294},
 edition = {2nd},
 publisher = {Butterworth-Heinemann},
 address = {Newton, MA, USA},
} 

@article{Fan2001,
abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coef? cients simultaneously. Hence they enable us to construct con? dence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on 401ˆ5, and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
file = {:home/markg/Downloads/penlike.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hard thresholding,lasso,nonnegative garrote,oracle estimator,penalized likelihood,scad,soft thresholding},
number = {456},
pages = {1348--1360},
pmid = {21796725},
title = {{Variable Selection via Nonconcave Penalized}},
volume = {96},
year = {2001}
}

@book{Zhang2010,
archivePrefix = {arXiv},
arxivId = {1002.4734},
author = {Zhang, Cun Hui},
booktitle = {Annals of Statistics},
doi = {10.1214/09-AOS729},
eprint = {1002.4734},
file = {:home/markg/Downloads/euclid.aos.1266586618.pdf:pdf},
isbn = {9040210063},
issn = {00905364},
keywords = {Correct selection,Degrees of freedom,Least squares,Mean squared error,Minimax,Model selection,Nonconvex minimization,Penalized estimation,Risk estimation,Selection consistency,Sign consistency,Unbiasedness,Variable selection},
number = {2},
pages = {894--942},
title = {{Nearly unbiased variable selection under minimax concave penalty}},
volume = {38},
year = {2010}
}

@article{Redmond2002,
abstract = {Police departments utilize information technology for combating crime, however, mostly for tactical purposes. This paper presents an Artificial-Intelligence software, Crime Similarity System (CSS) that helps police departments develop a strategic viewpoint toward decision-making. CSS utilizes socioeconomic, crime and enforcement profiles of cities to generate a list of communities that are best candidates to cooperate and share experiences. By providing a list of relevant similar communities from whom past experience and learnings can be shared, this tool offers the potential for proactive management. CSS provides a user-friendly front-end enabling easy usage. Camden, NJ and Philadelphia, PA police departments were partners in this development effort. Feedback from these two police departments has validated the benefit of this software in uncovering opportunities for police departments to cooperate. An evaluation using human subjects showed that the CSS software provided significantly better support than a conventional database. The modeling framework developed in this work is versatile, potentially useful for applications beyond law enforcement. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Redmond, Michael and Baveja, Alok},
doi = {10.1016/S0377-2217(01)00264-8},
file = {:home/markg/Downloads/1-s2.0-S0377221701002648-main.pdf:pdf},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Artificial intelligence,Case-based reasoning,Policing,Societal problem analysis},
number = {3},
pages = {660--678},
title = {{A data-driven software tool for enabling cooperative information sharing among police departments}},
volume = {141},
year = {2002}
}

@article{Xu2007,
abstract = {The genetic variance of a quantitative trait is often controlled by the segregation of multiple interacting loci. Linear model regression analysis is usually applied to estimating and testing effects of these quantitative trait loci (QTL). Including all the main effects and the effects of interaction (epistatic effects), the dimension of the linear model can be extremely high. Variable selection via stepwise regression or stochastic search variable selection (SSVS) is the common procedure for epistatic effect QTL analysis. These methods are computationally intensive, yet they may not be optimal. The LASSO (least absolute shrinkage and selection operator) method is computationally more efficient than the above methods. As a result, it has been widely used in regression analysis for large models. However, LASSO has never been applied to genetic mapping for epistatic QTL, where the number of model effects is typically many times larger than the sample size. In this study, we developed an empirical Bayes method (E-BAYES) to map epistatic QTL under the mixed model framework. We also tested the feasibility of using LASSO to estimate epistatic effects, examined the fully Bayesian SSVS, and reevaluated the penalized likelihood (PENAL) methods in mapping epistatic QTL. Simulation studies showed that all the above methods performed satisfactorily well. However, E-BAYES appears to outperform all other methods in terms of minimizing the mean-squared error (MSE) with relatively short computing time. Application of the new method to real data was demonstrated using a barley dataset.},
author = {Xu, Shizhong},
doi = {10.1111/j.1541-0420.2006.00711.x},
file = {:home/markg/Downloads/Xu-2007-Biometrics.pdf:pdf},
isbn = {0006-341X},
issn = {0006341X},
journal = {Biometrics},
keywords = {LASSO,Maximum likelihood,Mixed model,QTL mapping,Shrinkage},
number = {2},
pages = {513--521},
pmid = {17688503},
title = {{An empirical Bayes method for estimating epistatic effects of quantitative trait loci}},
volume = {63},
year = {2007}
}

@article{Karkkainen2012,
abstract = {Population-based association analyses are more powerful than within-family analyses in identifying genetic loci associated with a phenotype of interest. However, if the population or sample structure is omitted from the model, population stratification and cryptic relatedness may lead to false positive and negative signals caused by relatedness between individuals, rather than association due to close linkage of the marker and the trait loci. Therefore it is important to correct or account for these confounders in population-based association analyses. However, there is cumulative evidence that when fitting a multilocus association model, the genetic relationships between the individuals can be captured by the markers themselves, bringing about a possibility to use the models without an additional correction for the population or sample structure. In this work we have further investigated this possibility in the Bayesian multilocus association model context using the extended Bayesian LASSO and the indicator-based variable selection. In particular, we have studied whether these multilocus models benefit from an insertion of an additional polygenic term representing the genetic variation not captured by the markers and taking account of the residual dependencies between the individuals. We have found that although the models may benefit from the insertion of the polygenic component, omitting the component does not damage the model performance severely.},
author = {K{\"{a}}rkk{\"{a}}inen, Hanni P. and Sillanp{\"{a}}{\"{a}}, Mikko J.},
doi = {10.1111/j.1469-1809.2012.00729.x},
file = {:home/markg/Downloads/K-rkk-inen{\_}et{\_}al-2012-Annals{\_}of{\_}Human{\_}Genetics.pdf:pdf},
isbn = {1469-1809},
issn = {00034800},
journal = {Annals of Human Genetics},
keywords = {Association model,Cryptic relatedness,Extended Bayesian Lasso,Polygene},
number = {6},
pages = {510--523},
pmid = {22971009},
title = {{Robustness of Bayesian Multilocus Association Models to Cryptic Relatedness}},
volume = {76},
year = {2012}
}

@book{MacKay:2002:ITI:971143,
address = {New York, NY, USA},
author = {MacKay, David J C},
isbn = {0521642981},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference {\&} Learning Algorithms}},
year = {2002}
}

@book{hastie01statisticallearning,
address = {New York, NY, USA},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
keywords = {ml statistics},
publisher = {Springer New York Inc.},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
year = {2001}
}

@book{Murphy:2012:MLP:2380985,
author = {Murphy, Kevin P},
isbn = {0262018020, 9780262018029},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012}
}

@article{Cox2005,
abstract = {The broad distinctions between the frequentist and Bayesian approaches to statistical inference are outlined and some brief historical background given. The advantages and disadvantages of the frequentist discussion are sketched and then two contrasting Bayesian views given. The difficulties with the notion of a flat or uninformative prior distribution are discussed.},
author = {Cox, Dr},
doi = {10.1142/9781860948985_0001},
file = {:home/markg/Downloads/10.1.1.451.6137.pdf:pdf},
isbn = {9781860946493},
journal = {Proceedings of the Statistical Problems in Particle {\ldots}},
pages = {8--11},
title = {{Frequentist and Bayesian statistics: a critique}},
year = {2005}
}

@book{ruppert_wand_carroll_2003,
author = {Ruppert, David and Wand, M P and Carroll, R J},
doi = {10.1017/CBO9780511755453},
publisher = {Cambridge University Press},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Semiparametric Regression}},
year = {2003}
}

@article{DeBoor1972,
abstract = {In computational dealings with splines, the question of representation is of primary importance. For splines of fixed order on a fixed partition, this is a question of choice of basis, since such splines form a linear space. Only three kinds of bases for spline spaces have actually been given serious attention; those consisting of truncated power functions, of cardinal splines, and of B-splines. Truncated power bases are known to be open to severe ill- conditioning, while cardinal splines are difficult to calculate. By contrast, bases consisting of B-splines are well-conditioned, at least for orders {\textless} 20. Such bases are also local in the sense that at every point only a fixed number (equal to the order) of B-splines is nonzero. B-splines are also evaluated quite easily, using their definition as a divided difference of the truncated power function. Unfortunately, such calculations are ill-conditioned, particularly for partitions of widely varying interval lengths, as is indicated by the fact that special measures have to be taken in case of coincident knots. In this note, a different way of evaluating B-splines is discussed which is very well conditioned yet efficient, and which needs no special adjustments in case of coincident knots. It is also shown that the condition of the B-spline basis increases exponentially with the order.},
author = {de Boor, Carl},
doi = {10.1016/0021-9045(72)90080-9},
file = {:home/markg/Downloads/deboor.pdf:pdf},
issn = {10960430},
journal = {Journal of Approximation Theory},
number = {1},
pages = {50--62},
title = {{On calculating with B-splines}},
volume = {6},
year = {1972}
}

@book{Press:2007:NRE:1403886,
address = {New York, NY, USA},
author = {Press, William H and Teukolsky, Saul A and Vetterling, William T and Flannery, Brian P},
edition = {3},
isbn = {0521880688, 9780521880688},
publisher = {Cambridge University Press},
title = {{Numerical Recipes 3rd Edition: The Art of Scientific Computing}},
year = {2007}
}

@article{Hoerl1970,
abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X'X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X'X to obtain biased estimates with smaller mean square error.},
author = {Hoerl, Arthur E and Kennard, Robert W},
doi = {10.1080 / 00401706.2000.10485983},
file = {:home/markg/Downloads/d31ef5532dcbcf0bd01a980b1f79b9086fca.pdf:pdf},
journal = {Technometrics},
number = {1},
pages = {55--67},
title = {{Ridge Regression: Biased Estimation for Problems Nonorthogonal}},
volume = {12},
year = {1970}
}

@article{Minka2001,
abstract = {One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, ``Expectation Propagation,'' unifies and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction---propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Minka, Thomas P},
doi = {10.1016/j.conb.2011.12.004},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Minka - 2001 - A family of algorithms for approximate bayesian inference.pdf:pdf},
isbn = {9788578110796},
issn = {09594388},
journal = {Ph.D. Thesis},
keywords = {Belief Propagation,MRF Inference,Markov Random Fields},
pages = {1--482},
pmid = {2688543},
title = {{A family of algorithms for approximate Bayesian inference}},
year = {2001}
}

@misc{Tierney1986,
abstract = {This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.},
author = {Tierney, Luke and Kadane, Joseph B.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1986.10478240},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Kadane - 1986 - Accurate approximations for posterior moments and marginal densities.pdf:pdf},
isbn = {0162-1459},
issn = {1537274X},
keywords = {Asymptotic expansions,Bayesian inference,Computation of integrals,Laplace method},
number = {393},
pages = {82--86},
title = {{Accurate approximations for posterior moments and marginal densities}},
volume = {81},
year = {1986}
}

@article{Rue2009,
abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (gener- alized) additive models, smoothing spline models, state space models, semiparametric regres- sion, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models.We consider approximate Bayesian inference in a popular subset of struc- tured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables.The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis.We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its gen- erality, which makes it possible to performBayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0913v5},
author = {Rue, H{\aa}vard and Martino, Sara and Chopin, Nicolas},
doi = {10.1111/j.1467-9868.2008.00700.x},
eprint = {arXiv:1405.0913v5},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rue, Martino, Chopin - 2009 - Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximation.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
number = {2},
pages = {319--392},
title = {{Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations}},
volume = {71},
year = {2009}
}

@article{Minka2013,
abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
archivePrefix = {arXiv},
arxivId = {1301.2294},
author = {Minka, Thomas P.},
eprint = {1301.2294},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Minka - 2013 - Expectation Propagation for approximate Bayesian inference.pdf:pdf},
isbn = {1-55860-800-1},
title = {{Expectation Propagation for approximate Bayesian inference}},
year = {2013}
}

@article{Barber2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08337v1},
author = {Barber, Rina Foygel and Drton, Mathias and Tan, Kean Ming},
doi = {10.1007/978-3-319-27099-9_2},
eprint = {arXiv:1503.08337v1},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barber, Drton, Tan - 2016 - Laplace approximation in high-dimensional Bayesian regression.pdf:pdf},
isbn = {9783319270975},
issn = {21978549},
journal = {Abel Symposia},
keywords = {and phrases,bayesian inference,generalized linear models,laplace ap-,logistic regression,model selection,proximation,variable selection},
number = {2012},
pages = {15--36},
title = {{Laplace approximation in high-dimensional Bayesian regression}},
volume = {11},
year = {2016}
}

@article{KimWand2017,
author = {Kim, Andy S. I. Kim and Wand, M. P.},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Wand - 2017 - On Expectation Propagation for Generalized , Linear and Mixed Models.pdf:pdf},
keywords = {bayesian computing,factor graphs,infer,mean field variational bayes,net},
pages = {1--27},
title = {{On Expectation Propagation for Generalized , Linear and Mixed Models}},
year = {2017}
}

@article{Sinica2017,
author = {Hall, P and Ormerod, Jt and Wand, Mp},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall, Ormerod, Wand - 2011 - Theory of Gaussian variational approximation for a Poisson mixed model.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,asymptotic theory,generalized linear mixed models,kullback-,liebler divergence,longitudinal data analysis,maximum likelihood estimation},
number = {1},
pages = {369--389},
title = {{Theory of Gaussian variational approximation for a Poisson mixed model}},
volume = {21},
year = {2011}
}

@book{Golub:1996:MC:248979,
address = {Baltimore, MD, USA},
author = {Golub, Gene H and {Van Loan}, Charles F},
isbn = {978-1-4214-0794-4},
publisher = {Johns Hopkins University Press},
title = {{Matrix Computations (4th Ed.)}},
year = {2013}
}

@book{trefethen97,
author = {Trefethen, Lloyd N and Bau, David},
isbn = {0898713617},
keywords = {eigenvalues linear.algebra matrix numerical numeri},
publisher = {SIAM},
title = {{Numerical Linear Algebra}},
year = {1997}
}

@article{Carpenter2016,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matt and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
file = {:home/markg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carpenter et al. - 2016 - Journal of Statistical Software Stan A Probabilistic Programming Language.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {probabilistic programming, Bayesian inference, alg},
number = {Ii},
title = {{Journal of Statistical Software Stan : A Probabilistic Programming Language}},
volume = {VV},
year = {2016}
}

@misc{StanDevelopmentTeam2016,
annote = {R package version 2.14.1},
author = {{Stan Development Team}},
title = {{{\{}RStan{\}}: the {\{}R{\}} interface to {\{}Stan{\}}}},
year = {2016}
}

@article{Long1990,
author = {Long, J Scott},
year = {1990},
month = {06},
pages = {},
title = {The Origins of Sex Differences in Science},
volume = {68},
booktitle = {Social Forces}
}

@book{zuur_mixed_2009,
address = {New York, NY},
author = {Zuur, Alain F and Ieno, Elena N and Walker, Neil and Saveliev, Anatoly A and Smith, Graham M},
isbn = {978-0-387-87457-9},
keywords = {imported},
publisher = {Springer New York},
series = {Statistics for Biology and Health},
title = {Mixed effects models and extensions in ecology with R},
year = {2009}
}

@article{pitman_1936,
author = {Pitman, E J G},
doi = {10.1017/S0305004100019307},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {4},
pages = {567--579},
publisher = {Cambridge University Press},
title = {{Sufficient statistics and intrinsic accuracy}},
volume = {32},
year = {1936}
}

@article{Koopman1935,
author = {Koopman, B O},
file = {:home/markg/Downloads/S0002-9947-1936-1501854-3.pdf:pdf},
journal = {Transactions of the American Mathematical Society},
pages = {399--409},
title = {{On Distributions Admitting a Sufficient Statistics}},
volume = {222},
year = {1935}
}

@misc{Jordan2010,
author = {Jordan, Michael},
title = {STAT260/CS 294 Bayesian Modeling and Inference lecture notes},
url = {https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/},
year = {2010}
}

@book{butler_2007,
author = {Butler, Ronald W},
doi = {10.1017/CBO9780511619083},
publisher = {Cambridge University Press},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Saddlepoint Approximations with Applications}},
year = {2007}
}

@article{OSullivan1986,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {O'Sullivan, Finbarr},
doi = {10.1214/ss/1177013604},
eprint = {arXiv:1011.1669v3},
file = {:home/markg/Downloads/euclid.ss.1177013525.pdf:pdf},
isbn = {0412343908},
issn = {0883-4237},
journal = {Statistical Science},
number = {4},
pages = {505--527},
pmid = {20948974},
title = {{A Statistical Perspective on Ill-Posed Inverse Problems}},
volume = {1},
year = {1986}
}

@article{Gandomi2015,
abstract = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95{\%} of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.},
author = {Gandomi, Amir and Haider, Murtaza},
doi = {10.1016/j.ijinfomgt.2014.10.007},
file = {:home/markg/Downloads/Beyond{\_}the{\_}hype.pdf:pdf},
isbn = {02684012},
issn = {02684012},
journal = {International Journal of Information Management},
keywords = {Big data analytics,Big data definition,Predictive analytics,Unstructured data analytics},
number = {2},
pages = {137--144},
pmid = {1655112214},
publisher = {Elsevier Ltd},
title = {{Beyond the hype: Big data concepts, methods, and analytics}},
volume = {35},
year = {2015}
}

@book{Claeskens:1251912,
address = {Leiden},
author = {Claeskens, Gerda and Hjort, Nils Lid},
publisher = {Cambridge Univ. Press},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Model selection and model averaging}},
year = {2008}
}

@book{Bishop:2006:PRM:1162264,
address = {Berlin, Heidelberg},
author = {Bishop, Christopher M},
isbn = {0387310738},
publisher = {Springer-Verlag},
title = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
year = {2006}
}

@article{Schelldorfer2010,
archivePrefix = {arXiv},
arxivId = {1002.3784},
author = {Schelldorfer, J{\"{u}}rg and B{\"{u}}hlmann, Peter and van de Geer, Sara},
doi = {10.1111/j.1467-9469.2011.00740.x},
eprint = {1002.3784},
file = {:home/markg/Downloads/1002.3784.pdf:pdf},
issn = {03036898},
keywords = {adaptive lasso,coordinate gradient descent,coordinatewise optimiza-,lasso,random-effects model,tion,variable selection,variance components},
number = {20},
pages = {1--30},
title = {{Estimation for High-Dimensional Linear Mixed-Effects Models Using $L_1$-Penalization}},
volume = {2},
year = {2010}
}

@article{NengjunYi2013,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Nengjun Yi}, Himel Mallick},
doi = {10.4172/2155-6180.S1-005},
eprint = {NIHMS150003},
file = {:Users/markg/Downloads/bayesian-methods-for-high-dimensional-linear-models-2155-6180.S1-005.pdf:pdf},
isbn = {6176321972},
issn = {21556180},
journal = {Journal of Biometrics {\&} Biostatistics},
keywords = {Bayesian hierarchical models,Bayesian model selection,Bayesian variable selection,High dimensional linear models,MCMC,Nonlocal priors: Bayesian subset regression,Penalized regression,Posterior consistency,Regularization,Shrinkage methods,bayesian hierarchical models,bayesian model selection,bayesian subset regression,bayesian variable selection,high dimensional linear models,mcmc,nonlocal priors,penalized regression,posterior consistency,regularization,shrinkage methods},
pmid = {1000000221},
title = {{Bayesian Methods for High Dimensional Linear Models}},
year = {2013}
}

@article{Johnstone2009,
abstract = {Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue.},
author = {Johnstone, Iain M and Titterington, D Michael and Adragni, K. P. and Cook, R. D. and Banks, D. L. and House, L. and Killhoury, K. and Barber, D. and Beal, M. J. and Ghahramani, Z. and Belabbas, M-A. and Wolfe, P. J. and Belkin, M. and Niyogi, P. and Benjamini, Y. and Hochberg, Y. and Benjamini, Y. and Heller, R. and Yekutieli, D. and Bickel, P. and Bickel, P. J. and Levina, E. and Bickel, P. J. and Ritov, Y. and Tsybakov, A. B. and Bickel, P. J. and Brown, J. B. and Huang, H. and Li, Q. and Bishop, C. M. and Breiman, L. and Buja, A. and Cook, D. and Asimov, D. and Hurley, D. and Buja, A. and Cook, D. and Hofmann, H. and Lawrence, M. and Lee, E.-K. and Swayne, D. F. and Wickham, H. and Cand{\`{e}}s, E. J. and Tao, T. and Cand{\`{e}}s, E. and Tao, T. and Chipman, H. A. and George, E. I. and McCulloch, R. and Cook, R. D. and Dawid, A. P. and Dettling, M. and Donoho, D. L. and Donoho, D. L. and Grimes, C. and Donoho, D. L. and Jin, J. and Donoho, D. and Jin, J. and Donoho, D. and Tanner, J. and Karoui, N. El and Fan, J. and Lv, J. and Graunt, J. and Hamilton, W. C. and Hastie, T. and Tibshirani, R. and Hastie, T. and Tibshirani, R. and Friedman, J. H. and Hoerl, A. E. and Kennard, R. W. and Huber, P. J. and Ingster, Y. I. and Pouet, C. and Tsybakov, A. B. and Jin, J. and Johnstone, I. M. and Johnstone, I. M. and Johnstone, I. M. and Lu, A. Y. and Jolliffe, I. T. and Lindsay, B. G. and Kettenring, J. and Siegmund, D. O. and Nadler, B. and Onatski, A. and Ravikumar, P. and Liu, H. and Lafferty, J. and Wasserman, L. and Roweis, S. T. and Saul, L. K. and Tenenbaum, J. and DeSilva, V. and Langford, J. and Tibshirani, R. and Titterington, D. M. and Wainwright, M. J. and Wegman, E. J. and Wegman, E. J. and Solka, J. L.},
doi = {10.1098/rsta.2009.0159},
file = {:Users/markg/Downloads/4237.full.pdf:pdf},
issn = {1364-503X},
journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
keywords = {bayesian analysis,classification,cluster analysis,high-dimensional data,regression,sparsity},
number = {1906},
pages = {4237--53},
pmid = {19805443},
title = {{Statistical challenges of high-dimensional data.}},
volume = {367},
year = {2009}
}

@article{Gershman2012,
abstract = {Variational methods are widely used for ap- proximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approx- imations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to im- prove an approximate lower bound on the marginal likelihood of the data. Unlike most other variational approximations, using mul- tiple kernels allows the approximation to cap- ture multiple modes of the posterior. We demonstrate the efficacy of the nonparamet- ric approximation with a hierarchical logistic regression model and a nonlinear matrix fac- torization model. We obtain predictive per- formance as good as or better than more spe- cialized variational methods and MCMC ap- proximations. The method is easy to apply to graphical models for which standard vari- ational methods are difficult to derive.},
archivePrefix = {arXiv},
arxivId = {1206.4665},
author = {Gershman, Samuel J and Hoffman, Matthew D and Blei, David M},
doi = {10.1162/089976699300016331},
eprint = {1206.4665},
file = {:Users/markg/Library/Application Support/Mendeley Desktop/Downloaded/Gershman, Hoffman, Blei - 2012 - Nonparametric Variational Inference.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {0899-7667},
journal = {International Conference on Machine Learning},
pages = {1--8},
pmid = {80990000001},
title = {{Nonparametric Variational Inference}},
year = {2012}
}

@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
file = {:Users/markg/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
number = {Ml},
pages = {1--14},
pmid = {23459267},
title = {{Auto-Encoding Variational Bayes}},
year = {2013}
}

@Manual{CiteR,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2017},
    url = {https://www.R-project.org/},
  }

@article{Li2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06913v1},
author = {Li, Yingbo and Clyde, Merlise A},
eprint = {arXiv:1503.06913v1},
file = {:Users/markg/Library/Application Support/Mendeley Desktop/Downloaded/Li, Clyde - 2015 - Mixtures of g -priors in Generalized Linear Models.pdf:pdf},
journal = {arXiv},
title = {{Mixtures of g -priors in Generalized Linear Models}},
volume = {1503.06913},
year = {2015}
}

@article{Kachman2000,
abstract = {Linear mixed models provide a powerful means of predicting breeding values. However, for many traits of economic importance the assumptions of linear responses, constant variance, and normality are questionable. Generalized linear mixed models provide a means of modeling these deviations from the usual linear mixed model. This paper will examine what constitutes a generalized linear mixed model, issues involved in constructing a generalized linear mixed model, and the modifications necessary to convert a linear mixed model program into a generalized linear mixed model program.},
author = {Kachman, Stephen D},
doi = {10.2307/2532526},
file = {:Users/markg/Downloads/AN INTRODUCTION TO GENERALIZED LINEAR MIXED MODELS.pdf:pdf},
isbn = {1-58488-165-8},
issn = {0006341X},
journal = {Proceedings of a symposium at the organizational},
pages = {59--63},
title = {{an Introduction To Generalized Linear Mixed Models}},
year = {2000}
}

@article{Kleinman2004,
abstract = {Since the intentional dissemination of anthrax through the US postal system in the fall of 2001, there has been increased interest in surveillance for detection of biological terrorism. More generally, this could be described as the detection of incident disease clusters. In addition, the advent of affordable and quick geocoding allows for surveillance on a finer spatial scale than has been possible in the past. Surveillance for incident clusters of disease in both time and space is a relatively undeveloped arena of statistical methodology. Surveillance for bioterrorism detection, in particular, raises unique issues with methodological relevance. For example, the bioterrorism agents of greatest concern cause initial symptoms that may be difficult to distinguish from those of naturally occurring disease. In this paper, the authors propose a general approach to evaluating whether observed counts in relatively small areas are larger than would be expected on the basis of a history of naturally occurring disease. They implement the approach using generalized linear mixed models. The approach is illustrated using data on health-care visits (1996-1999) from a large Massachusetts managed care organization/multispecialty practice group in the context of syndromic surveillance for anthrax. The authors argue that there is great value in using the geographic data.},
author = {Kleinman, Ken and Lazarus, Ross and Platt, Richard},
doi = {10.1093/aje/kwh029},
file = {:Users/markg/Downloads/A generalized linear mixed models approach for detecting incident.pdf:pdf},
isbn = {0002-9262 (Print)$\backslash$n0002-9262 (Linking)},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {Bioterrorism,Communicable diseases,Epidemiologic methods,Generalized linear mixed model,Population surveillance,Spatial analysis},
number = {3},
pages = {217--224},
pmid = {14742279},
title = {{A Generalized Linear Mixed Models Approach for Detecting Incident Clusters of Disease in Small Areas, with an Application to Biological Terrorism}},
volume = {159},
year = {2004}
}

@article{Lo2015,
abstract = {Linear mixed-effect models (LMMs) are being increasingly widely used in psychology to analyse multi-level research designs. This feature allows LMMs to address some of the problems identified by Speelman and McGann (2013) about the use of mean data, because they do not average across individual responses. However, recent guidelines for using LMM to analyse skewed reaction time (RT) data collected in many cognitive psychological studies recommend the application of non-linear transformations to satisfy assumptions of normality. Uncritical adoption of this recommendation has important theoretical implications which can yield misleading conclusions. For example, Balota et al. (2013) showed that analyses of raw RT produced additive effects of word frequency and stimulus quality on word identification, which conflicted with the interactive effects observed in analyses of transformed RT. Generalized linear mixed-effect models (GLMM) provide a solution to this problem by satisfying normality assumptions without the need for transformation. This allows differences between individuals to be properly assessed, using the metric most appropriate to the researcher's theoretical context. We outline the major theoretical decisions involved in specifying a GLMM, and illustrate them by reanalysing Balota et al.'s datasets. We then consider the broader benefits of using GLMM to investigate individual differences.},
author = {Lo, Steson and Andrews, Sally},
doi = {10.3389/fpsyg.2015.01171},
file = {:Users/markg/Downloads/fpsyg-06-01171.pdf:pdf},
isbn = {1664-1078(Electronic)},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {RT transformations, generalized linear mixed-effect models, mental chronometry, interaction effects, additive factors,generalized linear mixed-effect models,interaction effects,mental chronometry,rt transformations},
number = {August},
pages = {1--16},
pmid = {26300841},
title = {{To transform or not to transform: using generalized linear mixed models to analyse reaction time data}},
url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.01171/abstract},
volume = {6},
year = {2015}
}

