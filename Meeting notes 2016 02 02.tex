\documentclass{amsart}
\title{Meeting with Dr John Ormerod - 02/02/2016}

\input{Definitions.tex}

\begin{document}

\maketitle

Get access to Dropxbox.

Block inverse formula.

$R^2 = \frac{\vy^\top \mX (\mX^\top \mX)^{-1} \mX^\top \vy}{\vy^\top \vy}$

$O(np^2 + p^3)$

\section{Rank one update}

Let $\mA = (\mX^\top \mX)^{-1}$, $O(p^3)$

$\mX \to [\mX, \vx]$
$n \times p, n \times 1$

\begin{equation*}
\begin{array}{lll}
R^2_{new} &= \frac{
\begin{bmatrix}
\vy^\top \mX \\
\vy^\top \vx
\end{bmatrix}
\begin{bmatrix}
\mX^\top \mX & \mX^\top \vx \\
\vx^\top \mX & \vx^\top \vx
\end{bmatrix}^{-1}
\begin{bmatrix}
\mX^\top \vy \\
\vx^\top \vy
\end{bmatrix}
}{\vy^\top \vy} & \text{(Use the block inverse formula.)} \\
&= \frac{
\begin{bmatrix}
\vy^\top \mX \\
\vy^\top \vx
\end{bmatrix}
\begin{bmatrix}
\mA + b \mA \mX^\top \vx \vx^\top \mX \mA & -\mA \mX^\top \vx b \\
-b \vx^\top \mX \mA & b
\end{bmatrix}
\begin{bmatrix}
\mX^\top \vy \\
\vx^\top \vy
\end{bmatrix}
}{\vy^\top \vy}
\end{array}
\end{equation*}

where $b = 1/(\vx^\top \vx - \vx^\top \mX \mA \mX^\top \vx)$. $O(np + p^2)$

If n large, big data case.

This calculation can be sped up by pre-calculating every possible b for all covariates.
Pre-calculate $\mZ^\top \mZ$, $O(nq^2)$, the main diagonal can be computed in $nq$ time.

$q = 100,000$

Fix a subset of covariates, do combinatorial search on subset.
Subset highly dependent in the posterior suggests its' worth doing a combinatorial search.
$q = 40$ should be achievable on the computational hardware that we have available.

\begin{equation*}
\begin{array}{ll}
p(\vy | \text{model}) & \\
p(\vy) &= \sum_{\text{models}} p*\vy | \text{model}) p(\text{model})
\end{array}
\end{equation*}

Pre-allocate matrices of all sizes.
Can be numerically stable provided $\vx^\top \vx - \vx^\top \mX \mA \mX^\top \vx$ doesn't get small.
$b$ is guaranteed to be positive definite.
If $b > \text{threshold}$, then calculate the full inverse.
Numerical inaccuracies accumulate, so calculate full inverse occasionally.

\section{Rank one downdate}

Let
\begin{equation*}
\begin{array}{ll}
\mA &= \begin{bmatrix}
\mA_{11} & \va_{12} \\
\va_{21} & \va_{22}
\end{bmatrix} \\
&= \begin{bmatrix}
\mX^\top \mX & \mX^\top \vx \\
\vx^\top & \vx^\top \vx
\end{bmatrix}^{-1}
\end{array}
\end{equation*}

We want $\mA_{\text{new}} = (\mX^\top \mX)^{-1} = \mA_{11} - \va_{22}^{-1} \va_{12} \va_{21}$.

Greycode

$G \in R^{2^r \times r}$. No need to store the entire matrix, generate as you need them. Can you get which
variables are set? q variables, what if q really large? Correct representation, integer flow if indexing. Bit
vector.

We should put all of this useful code into an R package.

\section{A cunning plan \ldots}
\begin{itemize}
\item Fully Bayesian
\item Combinatorial search, soe subset fixed
\item Collapsed VB next, integrates out subsets of parameters, just left with model indicators, some number 
			between $[0, 1]$. Fully factorised VB, same sort of algebra, but changing bits of $\mA$.
\item Initialisation strategies.
\item Branch and bound if there's time, lowest priority. It's just Mark's idea.
\end{itemize}
\end{document}