\documentclass{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\input{include.tex}

\title{Variational approximations to zero-inflated count models}
\author{Mark Greenaway}

\mode<presentation>
{ \usetheme{boxes} }

\begin{document}
% 1. Front slide
\begin{frame}
\titlepage
% Details about myself here?
\end{frame}

% 2. Intro
\begin{frame}
\frametitle{Introduction}
Zero inflated data arises in many areas of application, such as physical
activity data, number of hospital visits and number of insurance claims per
year.

We will work with zero-inflated count data. I encountered this sort of data while
analysing the data arising from the Get Healthy project.
\end{frame}
% 3. Univariate model
\begin{frame}
\frametitle{Univariate model formulation}
We start by building a relatively simple zero-inflated count model.

\begin{align*}{ll}
X_i &= R_i Y_i \\
R_i &\sim \text{Bernoulli}(\rho) \\
Y_i &\sim \text{Poisson}(\lambda) \\
\rho &\sim \text{Beta}(a_\rho, b_\rho) \\
\lambda &\sim \text{Gamma}(a_\lambda, b_\lambda)
\end{align*}

\end{frame}

% 4. \rho = 9/10, \lambda = 5
% Example data 0 0 0 5 10
\begin{frame}[fragile]
\frametitle{Example data}
Take, for example, $\rho = \frac{1}{2}, \lambda = 5$.

\begin{verbatim}
0 7 3 4 5 3 2 6 5 0 0 1
0 0 5 0 2 3 6 4 0 5 4 0
7 0 0 0 7 0 6 6 0 3 0 5
0 4 0 0 0 2 3 0 3 4 5 0
8 0
\end{verbatim}

% Histogram
\includegraphics[width=50mm, height=50mm]{code/univariate_data_histogram.pdf}
% Density
\end{frame}

% 5. How to fit, and advantages and disadvantages of each approach
% - Maximum likelihood
% - MCMC
% - VB
\
\begin{frame}
\frametitle{Comparison of fitting techniques}
\begin{tabular}{p{2cm}p{3.75cm}p{3.75cm}}
Technique & Pro & Con \\
\hline
MLE & Standard optimisation techniques can be used & Biased for mixed models \\
& & \\ %Frequentist \\
\hline
MCMC & Bayesian & Slow \\
	& Very accurate &  May not converge at all \\
\hline
Variational Bayes & Bayesian & May lose accuracy, or underestimate variance \\
& Fast  & Solution may be intractable \\ 
& Still quite accurate & \\
\hline
\end{tabular}

\end{frame}

% 6. Overview of Variational Bayes
\begin{frame}
\frametitle{An overview of Variational Bayes}
\begin{itemize}
\item \emph{Idea:} Approximate the full posterior $p(\vx, \theta)$ with a simpler approximation $q(\theta)$
\item Fit $q(\theta)$ to the data by minimising the KL divergence between $p(\vx, \theta)$ and $q(\theta)$
\item Theory guarantees that $q(\theta) < p(\vx, \theta)$ and that $q(\theta)$ will
increase with each iteration
\item If you use conjugate priors, a factored approximation can be used, and mean field updates can be used on
each parameter in turn until convergence is reached
\end{itemize}
% - Algorithm
We iteratively update the parameters of each approximate distribution
in turn until the lower bound of the approximation converges.

% Check this.
This could be thought of as a generalisation of Expectation Maximisation, where each parameter is thought of as a latent
variable and estimated according to the expectations of the other parameters.
\end{frame}

% 7a. Variational Bayes solution to ZIP
% - q-densities
\begin{frame}
Choose a factored approximation of the form
$$
q(\theta) = q(\lambda) q(\rho) \prod_{i=1}^n q(r_i)
$$
where
<<<<<<< HEAD
\begin{align*}{ll}
q(\lambda) &= \text{Gamma}(a_{q(\lambda)}, b_{q(\lambda)}) \\
q(\rho) &= \text{Beta}(a_{q(\rho)}, b_{q(\rho)}) \\
q(r_i) &= \text{Bernoulli}(p_i)
\end{align*}

=======
$$
q(\lambda) = \text{Gamma}(a_\lambda_*, b_\lambda_*)
q(\rho) = \text{Beta}(a_{q(\rho)}, b_{q(\rho))
q(r_i) = Bernoulli(p_i)
$$

% - Algorithm
We iteratively update the parameters of each approximate distribution
in turn until the lower bound of the approximation converges.

This could be thought of as a generalisation of Expectation Maximisation,
where each parameter is thought of as the unobserved parameter and maximised
relative to the other parameters in turn..
% FIXME - Check this.
>>>>>>> 7f0afd0b3cb08f3f0f262113abe12ccacdc6fc05
\end{frame}


% 8. Results
% - Lower bound convergence
% - Accuracies
% 7b. Define accuracy
\begin{frame}
\frametitle{Mean field updates}
% Are they going to be happy with that?
As we are using conjugate priors in our model, the mean field updates can be derived.
\begin{align*}{ll}
q(\lambda) &= \text{Gamma}(\alpha_\lambda + \vone^T \vx, \beta_\lambda + \vone^T\vp) \\
q(\rho) &= \text{Beta}(\alpha_\rho + \vone^T\vr, \beta_\rho + \vone^T(\vone - \vr)) \\
q(r_i) &= \text{Bernoulli}(\text{expit}(\eta_i))
\end{align*}
where
$$
\eta_i = - \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + \Psi(\alpha_{q(\rho)}) - \Psi(\beta_{q(\rho)})
$$
\end{frame}

\begin{frame}
\frametitle{Results/Accuracy}
% Definition of accuracy
Accuracy is defined as the difference in $L_1$ norm between the true posterior distribution and
the approximate posterior distribution of the variational approximation. This was calculated
using the kernel density estimate of the posterior distribution from MCMC minus the approximate
distribution for each parameter of interest.

Excellent accuracy for the univariate approximation, over 99\% in all of the cases that I looked at.
% Graph of lower bound
%\includegraphics{univariate_lower_bound_convergence.pdf}
\end{frame}

% 9. Extension to linear model
% 10. Overview GVA
% 11. Algorithm
% 12. Results?
% 13. What next
% 14. Conclusion
% 15. References
\begin{frame}
\frametitle{Extension to multivariate/regression models}
\begin{itemize}
\item Univariate models are a nice proof of concept.
\item Most applied statisticians want to build regression models
\item Applied statisticians love mixed models.
\item There is a need for better approaches to fitting zero-inflated mixed models.
\item For example, MCMC with existing software can take minutes to
converge, or not converge at all.
\item That might not sound so bad to you, but how do you
do model selection? Applied statisticians and biostatisticians rarely know the true model.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Model formulation}
We use a zero-inflated Poisson regression model
\begin{align*}{ll}
p(y_i|\vbeta, \vu, \vr, \mX, \mZ) &= \exp{(y_i^T\mR(\mX_i \vbeta + \mZ_i \vu) - \vr^Te^{\mX_i \vbeta + \mZ_i \vu} - \log{(y_i !)})} \\
\vbeta &\sim N(\vmu_\vbeta, \Sigma_\vbeta) \\
\vu|\sigma_u^2 &\sim N(\vzero, \sigma_u^2 I) \\
\sigma_u^2 &\sim IG(\alpha_{\sigma_u^2}, \beta_{\sigma_u^2}) \\
\end{align*}
\end{frame}
\end{document}

\begin{frame}
\frametitle{Form of the multivariate approximation}
Form of approximation
$$
q(\theta) = q(\beta, \vu) q(\sigma_u^2) q(\rho) \prod_{i=1}^n q(r_i)
$$
where
\begin{align*}{ll}
q(\beta) &\sim N(\mu, \Sigma) \\
q(\sigma_u^2) &\sim IG(\alpha_{\sigma_u^2}, \beta_{\sigma_u^2}) \\
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Gaussian and Laplacian Variational Approximations}
\begin{itemize}
\item Lack of conjugacy means mean field updates won't be analytically tractable for the regression parameters.
\item We try Gaussian Variational Approximations instead, assuming that
$$
\begin{pmatrix}
\beta \\
\vu
\end{pmatrix}
\sim N(\vmu, \Sigma)
$$
and approximate as closely as we can
\item For each iteration, we use Newton-Raphson style optimisation to find
$$
\begin{pmatrix}
\beta \\
\vu
\end{pmatrix}
$$
and then perform mean field updates on the other parameters
\item Computation - a work in progress
\item Initial signs are that this approach will work. The correct parameters are estimated for simulated test cases
that we have tried.
\item Initial signs are that this approach will work. Parameters are correctly estimated for simulated data.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{What next?}
\begin{itemize}
\item Continue working on the multivariate approximation
\item Extensions - random slopes, splines, and measurement error can all be accomodated within a mixed model
framework
\item Check accuracy against random walk Metropolis-Hastings approximation of the true posterior
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{References}
\begin{itemize}
\item Explaining variational approximations, J. T. Ormerod, M. P. Wand, American Statistician, 2010
\item Gaussian Variational Approximations, J. T. Ormerod, M. P. Wand, 2012
\item General Design Mixed Models, J. T. Ormerod, M. P. Wand, 2006
\end{itemize}
\end{frame}

\end{document}
