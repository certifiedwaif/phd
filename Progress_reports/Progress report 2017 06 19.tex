\documentclass{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{ulem}
\input{../include.tex}
\input{../Definitions.tex}

\usefonttheme{serif}

\title{Progress report - 19/06/2017}
\author{Mark Greenaway\\PhD candidate\\markg@maths.usyd.edu.au}

\mode<presentation>
{ \usetheme{boxes} }

\begin{document}
\begin{frame}
\maketitle
\end{frame}

\begin{frame}{Writing}
\begin{itemize}
\item 80 rough pages of thesis writing so far
\item Aiming to submit by the end of the year
\item John and I are very close to submitting a paper on normal linear model selection using g-priors
to Bayesian Analysis. Hopefully this week
\item Marek asked me to prepare a one-page document on IT problems
\end{itemize}
\end{frame}

\begin{frame}{Zero-inflated Poisson}
\begin{itemize}
\item{Biochemist example}
\begin{align*}
y_i &= R_i e^{\vbeta_1 + \vbeta_2 \text{women} + \vbeta_3 \text{married} + \vbeta_4 \text{children} + \vbeta_5 \text{PhD}} \\
R_i &\sim \Bernoulli(\rho) \\
\rho &\sim \Beta(\alpha, \beta) \\
\vbeta &\sim \N(0, \sigma_\vbeta^2)
\end{align*}
I fit this model, and the accuracy was \emph{terrible}
\item Wrong parameterisation \\
It turns out our variational approximation was using the seperate parameterisation
\begin{align*}
P(y_i = 0) &= 1 - \rho \\
P(y_i = y) &= \rho \Poisson(y; e^{\mX \vbeta}), y \ne 0
\end{align*}
while Stan was using the overlapping representation
\begin{align*}
P(y_i = 0) &= 1 - \rho + \rho \Poisson(0; e^{\mX \vbeta})\\
P(y_i = y) &= \rho \Poisson(y; e^{\mX \vbeta}), y \ne 0
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{When this was fixed, accuracy was good}
\begin{figure}
\includegraphics[scale=0.3]{../code/results/accuracy_plots_application_biochemists_GVA_inv_param-nup.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Collapsed variational approximation}
Suppose that
\[
	q(\vgamma) = \sum_{k=1}^K w_k \I(\vgamma = \vgamma_k)
\]
where
\[
	w_k = p(\vy, \vgamma_k) / \sum_{j=1}^K p(\vy, \vgamma_j).
\]
A variational lower bound is given by
\[
\log \underline{p}(\vy; \vw, \Gamma) = \sum_{k=1}^K w_k \log p(\vy, \vgamma_k) - w_k \log w_k.
\]
\end{frame}

\begin{frame}{Algorithm}
We can fit this model using the following algorithm.
\begin{itemize}
\item Initialise population of K bitstrings, $\vgamma$ \\
Loop until convergence\\
\quad for $k = 1,  \ldots, K$\\
\quad \quad for $j = 1, \ldots, p$\\
\quad \quad \quad $\vgamma_{jk} = \begin{cases}
1, \text{if } p(\vy, \vgamma_{jk}^{(1)}) > p(\vy, \vgamma_{jk}^{(0)}) \\
0, \text{otherwise}
\end{cases}$\\
\quad for $k = 1, \ldots, K$\\
\quad \quad $w_k = p(\vy, \vgamma_k) / \sum_{j=1}^K p(\vy, \vgamma_j)$
\item I've had some time to experiment now, and we're getting some interesting results
\end{itemize}
\end{frame}

\begin{frame}{Model selection}

\begin{figure}
\includegraphics[scale=0.45]{../code/correlation/cva_more_dimensions.png}
\end{figure}
\end{frame}

\begin{frame}{Covariate selection}
\begin{figure}
\includegraphics[scale=0.45]{../code/correlation/covariate_inclusion_probabilities.pdf}
\end{figure}
\end{frame}

\end{document}
