\documentclass{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{subcaption}
\usepackage{algorithm,algorithmic}
\usepackage{natbib}
\usepackage{cancel}
\input{../include.tex}
\input{../Definitions.tex}

\usefonttheme{serif}

\title{Normal linear model selection}
\author{Mark Greenaway, Dr John Ormerod}

\mode<presentation>
{ \usetheme{boxes} }


\begin{document}
% 1. Front slide
\begin{frame}
	\titlepage
	% Details about myself here?
\end{frame}
			
% Only have ten minutes
% Introduce problem
% Introduce model
\begin{frame}
	\frametitle{Model}
	Consider the linear model
	\begin{align*}
		\vy | \vbeta, \sigma^2 \sim \N_n(\mX \vbeta, \sigma^2 \mI) 
	\end{align*}
	with priors
	\begin{align*}
		\alpha & \propto 1 \\
		\vbeta | \sigma^2, g & \sim \N_p(\vzero, g \sigma^2 (\mX^T \mX)^{-1}),                     \\
		p(\sigma^2)          & = (\sigma^2)^{-1} \I(\sigma^2 > 0), \text{ and }                    \\
		p(g)                 & = \frac{g^b (1 + g)^{-(a + b + 2)}}{\Beta(a + 1, b + 1)} \I(g > 0), 
	\end{align*}
	choosing $a = -3/4$ and $b = (n-p)/2 - a - 2$, following \citep{Maruyama2011}.
\end{frame}

% Discuss Bartlett's and Information Paradox
\begin{frame}
	\frametitle{Bartlett's Paradox and the Information Paradox}
	The Bayes Factor for comparing a non-null model to the null model is
	\[
		\text{BF}(\mathcal{M}_\vgamma : \mathcal{M}_0) = \frac{(1 + g)^{(n - p_\vgamma - 1)/2}}{[1 + g (1 - R_\vgamma^2)]^{(n - 1)/2}}
	\]
	But this Bayes Factor exhibits some paradoxes.
	\begin{itemize}
		\item Problem 1) Bartlett's paradox: $p(\vzero | \vy) \to 1$ as $g \to \infty$ \\
		\item Problem 2) Information paradox: $p(\vgamma^* | \vy) \cancel{\to} 1$ as $n \to \infty$
	\end{itemize}

\end{frame}

% Mixture of g, choice of g prior
\begin{frame}
	\frametitle{Mixture of $g$}
	\citep{Liang2008} suggested we avoid these paradoxes by using a mixture of $g$.
	We choose a Beta-Prime prior on $g$, following \citep{Maruyama2011}.
	\[
		p(g) = \frac{g^b (1 + g)^{-(a + b + 2)}}{\Beta(a + 1, b + 1)} \I(g > 0)
	\]
\end{frame}

% Things we derived
\begin{frame}
	\frametitle{Things we derived}
	$$\log p(\vy, \vgamma) = \int p(\vy | \alpha, \vbeta, \sigma^2) p(\alpha) p(\vbeta) p(\sigma^2) d \alpha d \vbeta d \sigma^2$$
	\begin{itemize}
		\item We choose our priors so that this integral is tractable. Each choice of prior like a fancy version of BIC
		\item Liang et al. 2008 hyper-$g$ prior \citep{Liang2008}
			$p(\vy | \vgamma) = \frac{K(n) (a - 2)}{p_\vgamma + a  - 2} {}_2 F_1 \left( \frac{n-1}{2}, 1; \frac{p_\vgamma + a}{2}; R_\vgamma^2 \right)$
		\item Liang et al. 2008 hyper-$g/n$ prior \citep{Liang2008}
			$p(\vy | \vgamma) = \frac{K(n) (a - 2)}{n (p_\vgamma + a  - 2)} F_1 \left( 1, \frac{a}{2}, \frac{n-1}{2}; \frac{p_\vgamma + a}{2}; 1 - \frac{1}{n}, R_\vgamma^2 \right)$
		\item Robust Bayarri et al. (2012) \citep{Bayarri2012}
			$p(\vy | \vgamma) = K(n) \left(\frac{n + 1}{1 + p_\vgamma}\right)^{-p_\vgamma/2} \frac{(\hat{\sigma_\vgamma^2})^{-(n-1)/2}}{p_\vgamma + 1} {}_2 F_1 \left[ \frac{n-1}{2}, \frac{p_\vgamma+1}{2}; \frac{p_\vgamma + 3}{2}; \frac{(1 - 1/\hat{\sigma_\vgamma^2})(p_\vgamma + 1)}{1 + n} \right]$
	\end{itemize}
\end{frame}

% Results
\begin{frame}
	\frametitle{Results}
	$\tau_g$ is a function of $n$, $p$ and $R^2$.
\end{frame}
% Look at past presentations as a starting point

\begin{frame}
	\frametitle{References / Contact details}
	\begin{itemize}
		\item Mark Greenaway
		\item markg@maths.usyd.edu.au
		\item Twitter: @certifiedwaif	
	\end{itemize}
	\bibliographystyle{elsarticle-harv}
	\bibliography{../references_mendeley}
\end{frame}

\end{document}
