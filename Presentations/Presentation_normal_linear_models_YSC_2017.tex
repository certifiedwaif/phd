\documentclass[notes=only]{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{subcaption}
\usepackage{algorithm,algorithmic}
\usepackage{natbib}
\usepackage{cancel}
\input{../include.tex}
\input{../Definitions.tex}

\usefonttheme{serif}

\title{Exact expressions for parameter posteriors in Bayesian linear model selection}
\author{Mark Greenaway, Dr John Ormerod}

\mode<presentation>
{ \usetheme{boxes} }


\begin{document}
% 1. Front slide
\begin{frame}
	\titlepage
	% Details about myself here?
\end{frame}

\note{Hello everyone, I'm Mark Greenaway and I'm a PhD candidate from the University of Sydney.
			I'll be presenting on exact expressions for parameter posteriors
			in Bayesian linear model selection. This is joint work with Dr John Ormerod.}
			
% Only have ten minutes
% Introduce problem
\begin{frame}
	\frametitle{Posterior parameters in Bayesian linear model selection}
	\begin{itemize}
		\item Motivation: The problem of selecting linear models is well-studied, in the frequentist and Bayesian
					contexts.
		\item A lot of work has been done on parameter priors for Bayesian model selection.
		\item Less work has been done on the parameter posteriors, so we chose to explore this area in our
					research.
		\item We derive several new exact closed form expressions for the posterior parameters using special
					functions. These expressions are numerically well-behaved.
	\end{itemize}
\end{frame}

\note{Linear model selection is a well-studied problem. A lot of work has been done on studying parameter priors
			for model selection. The parameter posteriors are less well-studied, so we chose to explore this area in
			our research. We derive several new exact closed form expressions for the posterior parameters using
			special functions. These expressions are numerically well-behaved, and can be evaluated with widely 
			available	libraries.}

\begin{frame}
	\frametitle{Notation}
	\begin{itemize}
		\item Let $\vy$ be the vector of responses of length $n$, and $\mX$ be the $n \times p$ covariate matrix.
		\item We denote our models by $\vgamma$, vectors of length $p$ where
					\[
						\vgamma_i =
						\begin{cases}
							1 & \text{ if the $i$th column of $\mX$ is included in the model} \\
							0 & \text{otherwise}.
						\end{cases}
					\]
		e.g. $\vgamma = (1, 0, 0, 1, 1)^\top$ includes the first, fourth and fifth covariates from $\mX$.
		\item Then $\mX_\vgamma$ is the matrix formed by taking the columns for which $\vgamma_i = 1$ from $\mX$,
					and $p_\vgamma$ is the number of covariates in the model $\vgamma$.
	\end{itemize}
\end{frame}

\note{I want to introduce some notation. Let $\vy$ denote the vector of responses, and $\mX$ be the
			$n \times p$ covariate matrix as usual. We dnote our models by $\vgamma$, vectors of length $p$ where
			each element indicates whether that covariate is included or not. So a model $\vgamma$ equals
			$(1, 0, 0, 1, 1)^\top$ contains the first, fourth and fifth covariates, but excludes the second and
			third. $\mX_\vgamma$ is the matrix formed by taking the columns for which $\vgamma_i = 1$ from $\mX$,
			and $p_\vgamma$ is the number of covariates in the model $\vgamma$.}

% Introduce model
\begin{frame}
	\frametitle{Model selection using $g$}
	\begin{itemize}
		\item Consider the linear model
		\begin{align*}
			\vy | \alpha, \vbeta, \sigma^2, \vgamma \sim \N_n(\vone_n \alpha + \mX_\vgamma \vbeta, \sigma^2 \mI) 
		\end{align*}
		with priors
		\begin{align*}
			\alpha \quad & \propto \quad 1, \\
			\vbeta | \sigma^2, g, \vgamma \quad & \sim \quad \N_p(\vzero, g \sigma^2 (\mX_\vgamma^\top \mX_\vgamma)^{-1}),                     \\
			p(\sigma^2)          \quad & = \quad (\sigma^2)^{-1} \I(\sigma^2 > 0), \text{ and }                    \\
			p(g)                 \quad & = \quad \text{unspecified for now}.
		\end{align*}
		\item We introduce a hyperparameter $g \in [0, \infty)$, as a shrinkage parameter.
					$g$ shrinks $\vbeta$ according to the sample size $n$, the number of covariates in the model $p_\vgamma$
					and the quality of the model fit $R^2_\vgamma$.
					$g$ deserves special consideration.
		\item This prior structure on $\vbeta$ is called the Zellner-Siow prior, introduced in \cite{Zellner1980}.
		% that controls the mixing between the null and full models.
		% \item If $g$ is $0$, we
		% choose the null model. If $g$ is $\infty$ we choose $\hat{\vbeta}$.
	\end{itemize}
\end{frame}

\note{Consider a linear model with standard normal priors. We introduce a new shrinkage parameter $g$.
			$g$ shrinks $\vbeta$ according to $n$, $p_\vgamma$ and $R_\vgamma^2$.}

% Discuss Bartlett's and Information Paradox
\begin{frame}
	\frametitle{How should we choose $g$?}
	There are a few approaches we could take.
	\begin{itemize}
		\item Approach 1: We could simply let $g \to \infty$.
		\item \cite{Liang2008} showed that\\
		$p(\vgamma = \vzero | \vy) \to 1$ as $g \to \infty$ (Bartlett's paradox).
		\item Approach 2: We could do Empirical Bayes on $g$, or otherwise make a fixed choice of $g$.
		\item \cite{Liang2008} showed that this was not model selection consistent. \\
					$p(\vgamma = \vgamma^* | \vy) \cancel{\to} 1$ as $n \to \infty$,
					where $\vgamma^*$ is the true model (Information Paradox)
		\item So we need another approach.
	\end{itemize}
\end{frame}

\note{So how should we choose $g$? There are a few approaches we could take. We could let $g$ go to infinity.
			But Liang showeed in 2008 that if you do this, you'll select the null model with probability $1$, which
			is an example of Bartlett's Paradox.
			We could use Empirical Bayes to choose $g$, but Liang showed that this was not model selection
			consistent, which is an example of the Information Paradox.
			So we need another approach.}

% Mixture of g, choice of g prior
\begin{frame}
	\frametitle{We should choose a mixture of $g$}
	\cite{Liang2008} argued that we avoid should these paradoxes by using a mixture of $g$. \\
	Several choices of prior for $g$	have been proposed.
	\small
	\begin{itemize}
		\item hyper-$g$ prior, \cite{Liang2008}\\
		$p(g) = \frac{a-2}{2} (1 + g)^{-a/2} I(g > 0)$, $a > 2$
		\item hyper-$g/n$ prior, \cite{Liang2008}\\
		$p(g) = \frac{a-2}{2n} (1 + g/n)^{-a/2} I(g > 0)$, $a > 2$ 
		\item Bayarri's robust prior on $g$, \cite{Bayarri2012} \\
		$p(g) = \frac{1}{2} \left(  \frac{1 + n}{1 + p_\vgamma} \right)^{1/2} (1 + g)^{-3/2} I(g > (1 + n)/(1 + p_\vgamma) - 1)$
		\item Beta-Prime prior on $g$, \cite{Maruyama2011} \\
		$p(g) = \frac{g^b (1 + g)^{-(a + b + 2)}}{\Beta(a + 1, b + 1)} \I(g > 0)$
	\end{itemize}
\end{frame}

\note{Liang argued that we should avoid these paradoxes by using a mixture of $g$. Several choices of prior
			for $g$ have been proposed in the literature.
			These all place most of the prior probability on lower values of $g$, adjusting the shrinkage according
			to the sample size and number of covariates in the model.}

% Things we derived
\begin{frame}
	\frametitle{Calculating $p(\vy | \vgamma)$}
	We want to calculate $p(\vy | \vgamma)$ so that we can calculate $p(\vgamma | \vy)$ using Bayes' Rule and rank models against	one another. \\
	To do this, we need to calculate
	$$p(\vy | \vgamma) = \int p(\vy | \alpha, \vbeta, \sigma^2, \vgamma) p(\alpha) p(\vbeta | g, \vgamma) p(\sigma^2) p(g) d \alpha d \vbeta d \sigma^2 d g.$$
	We choose our priors so that this integral is tractable, with help from \cite{Gradshteyn1988}.
	The marginal likelihoods behave like BIC. \\
	We found the following closed form expressions.
	\small
	\begin{itemize}
		\item Liang et al. 2008 hyper-$g$ prior \cite{Liang2008}
			$p(\vy | \vgamma) = \frac{K(n) (a - 2)}{p_\vgamma + a  - 2} {}_2 F_1 \left( \frac{n-1}{2}, 1; \frac{p_\vgamma + a}{2}; R_\vgamma^2 \right)$
		\item Liang et al. 2008 hyper-$g/n$ prior \cite{Liang2008}
			$p(\vy | \vgamma) = \frac{K(n) (a - 2)}{n (p_\vgamma + a  - 2)} F_1 \left( 1, \frac{a}{2}, \frac{n-1}{2}; \frac{p_\vgamma + a}{2}; 1 - \frac{1}{n}, R_\vgamma^2 \right)$
	\end{itemize}
	These expressions are numerically stable to evaluate.
\end{frame}

\note{We want to be able to calculate the marginal likelihood of the data given $\vgamma$, so that we can
			calculate the posterior probability of $\vgamma$ and rank models against one another. To do this, we
			need to calculate the following integral.
			We choose our priors so that this integral is tractable, with help from the book Table of Integrals and
			Series by Gradshteyn. The marginal likelihoods behave like BIC.
			We found closed form expressions for the marginal likelihood for the hyper-$g$ and hyper-$g/n$ priors.}

\begin{frame}
	\frametitle{Calculating $p(\vy | \vgamma)$ for Bayarri's robust prior}
	\begin{itemize}
		\item Bayarri calculated the marginal likelihood given $\vgamma$ for the Robust Bayarri prior
		 	in \cite{Bayarri2012} 
			\tiny
			$p(\vy | \vgamma) = K(n) \left(\frac{n + 1}{1 + p_\vgamma}\right)^{-p_\vgamma/2} \frac{(1 - R^2_\vgamma)^{-(n-1)/2}}{p_\vgamma + 1} {}_2 F_1 \left[ \frac{n-1}{2}, \frac{p_\vgamma+1}{2}; \frac{p_\vgamma + 3}{2}; \frac{[1 - 1/(1 - R^2_\vgamma)](p_\vgamma + 1)}{1 + n} \right]$.
		\small
		\item But this expression is not numerically well-behaved, because the second argument of the
					${}_2 F_1$ function is greater than $1$.
		\item Using Euler's transformation on ${}_2 F_1$ and the fact that ${}_2 F_1(., 1; .; .)$ is numerically
					well-behaved, we derived the new expression
			\tiny
			$p(\vy | \vgamma) = K(n) \left(\frac{n + 1}{1 + p_\vgamma}\right)^{(n-p_\vgamma-1)/2} \frac{[1 + L (1 - R^2_\vgamma)]^{-(n-1)/2}}{p_\vgamma + 1} {}_2 F_1 \left[ \frac{n-1}{2}, 1; \frac{p_\vgamma + 3}{2}; 
			\frac{R^2_\vgamma}{1 + L(1 - R^2_\vgamma)} \right]$ \\
			\small
			where $L = (1 + n)/(1 + p_\vgamma) - 1$.
	\end{itemize}
\end{frame}

\note{Bayarri calculated the marginal likelihood given $\vgamma$ for the Robust Bayarri prior.
			But this expression is not numerically well-behaved.
			We used Euler's transformation and properties of the hypergeometric function to derive a new,
			numerically stable function.}

% Exact posterior distributions using special functions
\begin{frame}
	\frametitle{Exact posterior distributions of $\alpha$, $u$ and $\sigma^2$ using special functions}
	When deriving exact posterior distributions on the parameters of the model,
	we focused on the Beta-Prime prior on $g$, as it is the most numerically well-behaved. \\
	We were able to derive closed form expressions for the posterior distributions.
	\small
	\begin{align*}
		p(u | \vy) &= \frac{(1 - R_\vgamma^2)^{b + 1}}{\text{Beta}(b + 1, d + 1)} u^b (1 - u)^d (1 - u R_\vgamma^2)^{-(b + d + 2)} \\
		p(\alpha | \vy) &= \frac{\Gamma(c + \frac{1}{2}) (1 - R_\vgamma^2)^{b + 1}}{\Gamma(c) \sqrt{\pi}} (1 + \alpha^2)^{-n/2}
		{}_2 F_1 \left(b + 1, c + \frac{1}{2}; c; \frac{R_\vgamma^2}{1 + \alpha^2}\right) \\
		p(\sigma^2 | \vy) &= \frac{\frac{n}{2} (1 - R_\vgamma^2)^{b + 1}}{\Gamma(c)} (1 - R_\vgamma^2)^{-(c + 1)}
												\exp{\left( -\frac{n}{2 \sigma^2} \right)} {}_1 F_1 \left(b + 1; c; \frac{n R_\vgamma^2}{2 \sigma^2}\right)
	\end{align*}
	where $u = g/(1 + g)$, $a = 1$ and $b = p_\vgamma$.
\end{frame}

\note{We derived exact expressions for the posteriors of $\alpha$, $u$ which is a transformation of $g$ and
			$\sigma^2$ under the Beta-Prime prior. We focused on the Beta-Prime prior, because it is numerically
			stable.}

\begin{frame}
	\frametitle{Exact first and second moments of $\vbeta$}
	% We weren't able to find a closed form expression for $\vbeta | \vy$, but 
	We weren't able to find a closed form expression for $\vbeta | \vy$, but we were able to find closed form
	expressions for the first and second moments of $\vbeta | \vy$.
	\small
	\begin{align*}
		\E[\vbeta | \vy] &= M_1 \hat{\vbeta} \\
		\Cov[\vbeta | \vy] &= \frac{n}{n - 3} (M_1 - M_2 R^2)(\mX^\top \mX)^{-1} + (M_2 - M_1^2) \hat{\vbeta} \hat{\vbeta}^\top
	\end{align*}
	where
	\small
	\begin{align*}
		M_1 &= \frac{b + 1}{c} {}_2 F_1 (d + 1, 1; c + 1; R^2) \text{ and } \\
		M_2 &= \frac{(b + 1)(b + 2)}{c(c + 1)} {}_2 F_1 (d + 1, 2; c + 2; R^2).
	\end{align*}
\end{frame}

\note{We weren't able to find a closed form expression for the posterior of $\vbeta$, so instead we focused on
			the first and second moments.}

% Say sopmething about the Rao-Blackwellisation?

% Graphs of posterior functions
\begin{frame}
	\frametitle{Comparing exact results to Monte Carlo sampling}
	We checked our results by comparing against Monte Carlo sampling and found that they matched closely.
	\begin{figure}[htp]
		\centering
		\begin{center}$
			\begin{array}{ll}
				% \caption{$u | \vy$}
				\includegraphics[width=.5\textwidth]{uGivenY.pdf} &
				% \caption{$\sigma^2 | \vy$}
				\includegraphics[width=.5\textwidth]{sigma2GivenY.pdf} \\
				% \caption{$g | \vy$}
				\includegraphics[width=.5\textwidth]{gGivenY.pdf} &
				% \caption{$\alpha | \vy$}
				\includegraphics[width=.5\textwidth]{alphaGivenY.pdf}
			\end{array}$
		\end{center}
		\includegraphics[width=.5\textwidth]{uGivenY.pdf}
		\includegraphics[width=.5\textwidth]{sigma2GivenY.pdf}
		\includegraphics[width=.5\textwidth]{gGivenY.pdf}
		\includegraphics[width=.5\textwidth]{alphaGivenY.pdf}
	\end{figure}
\end{frame}

\note{We checked that our exact expressions were correct by checking them against a Monte Carlo sampling
			scheme that we developed. We found that they matched very closely for a range of parameter settings.}

\begin{frame}
	\frametitle{Application: US Crime data}
	\begin{figure}
		\caption{Example model fit to the US Crime data}
		\includegraphics[scale=.33]{BetaUSCrime.pdf}
	\end{figure}
\end{frame}

\note{We applied our model fitting software to the US Crime data set, and found that our
			first and second moments fit well.}

\begin{frame}
	\frametitle{References / Contact details}
	\begin{itemize}
		\item Mark Greenaway
		\item markg@maths.usyd.edu.au
		\item Twitter: @certifiedwaif	
	\end{itemize}

	\small
	\bibliographystyle{elsarticle-harv}
	\bibliography{../references_mendeley}
\end{frame}

\note{Thank you for your attention! Any questions?}

\end{document}
