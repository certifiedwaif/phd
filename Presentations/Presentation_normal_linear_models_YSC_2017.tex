\documentclass{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{subcaption}
\usepackage{algorithm,algorithmic}
\usepackage{natbib}
\usepackage{cancel}
\input{../include.tex}
\input{../Definitions.tex}

\usefonttheme{serif}

\title{Exact expressions for parameter posteriors in Bayesian linear model selection}
\author{Mark Greenaway, Dr John Ormerod}

\mode<presentation>
{ \usetheme{boxes} }


\begin{document}
% 1. Front slide
\begin{frame}
	\titlepage
	% Details about myself here?
\end{frame}

\note{Hello everyone, I'm Mark Greenaway and I'm a PhD candidate from the University of Sydney.
			I'll be presenting on exact expressions for parameter posteriors
			in Bayesian linear model selection. This is joint work with Dr John Ormerod.}
			
% Only have ten minutes
% Introduce problem
\begin{frame}
	\frametitle{Posterior parameters in Bayesian linear model selection}
	\begin{itemize}
		\item Motivation: The problem of selecting linear models is well-studied, in the frequentist and Bayesian
					contexts.
		\item A lot of work has been done on parameter priors for Bayesian model selection.
		\item Less work has been done on the parameter posteriors, so we chose to explore this area in our
					research.
		\item We derive several new exact closed form expressions for the posterior parameters using special
					functions. These expressions are numerically well-behaved.
	\end{itemize}
\end{frame}

\note{Linear model selection is a well-studied problem. A lot of work has been done on studying parameter priors
			for model selection. The parameter posteriors are less well-studied, so we chose to explore this area in
			our research. We derive several new exact closed form expressions for the posterior parameters using
			special functions. These expressions are numerically well-behaved, and can be evaluated with widely 
			available	libraries.

			People have looked at particular combinations of models and priors, calculating the marginal
			likelihoods.
			The next step in the process would be to look at the posteriors of individual parameters.
			We also want to look at the numerical properties of these expressions, as we want them to be numerically
			stable and easy to evaluate.}

\begin{frame}
	\frametitle{Notation}
	\begin{itemize}
		\item Let $\vy$ be the vector of responses of length $n$, and $\mX$ be the $n \times p$ covariate matrix.
		\item We denote our models by $\vgamma$, vectors of length $p$ where
					\[
						\vgamma_i =
						\begin{cases}
							1 & \text{ if the $i$th column of $\mX$ is included in the model} \\
							0 & \text{otherwise}.
						\end{cases}
					\]
		e.g. $\vgamma = (1, 0, 0, 1, 1)^\top$ includes the first, fourth and fifth covariates from $\mX$.
		\item Then $\mX_\vgamma$ is the matrix formed by taking the columns for which $\vgamma_i = 1$ from $\mX$,
					and $p_\vgamma$ is the number of covariates in the model $\vgamma$.
	\end{itemize}
\end{frame}

\note{I want to introduce some notation. Let $\vy$ denote the vector of responses, and $\mX$ be the
			$n \times p$ covariate matrix as usual. $\vgamma$ represents a particular model choice, where
			each element of the vector indicates whether that covariate is included or not. So a model $\vgamma$ equals
			$(1, 0, 0, 1, 1)^\top$ contains the first, fourth and fifth covariates, but excludes the second and
			third. $\mX_\vgamma$ is the matrix formed by taking the columns for which $\vgamma_i = 1$ from $\mX$,
			and $p_\vgamma$ is the number of covariates in the model $\vgamma$.}

% Introduce model
\begin{frame}
	\frametitle{Model selection using $g$}
	\begin{itemize}
		\item We adopt the prior structure on $\vbeta$ is called the Zellner-Siow prior, introduced in \cite{Zellner1980}.
		\item Model
		\begin{align*}
			\vy | \alpha, \vbeta, \sigma^2, \vgamma \sim \N_n(\vone_n \alpha + \mX_\vgamma \vbeta, \sigma^2 \mI) 
		\end{align*}
		with priors
		\begin{align*}
			\alpha \quad & \propto \quad 1, \\
			\vbeta | \sigma^2, g, \vgamma \quad & \sim \quad \N_p(\vzero, g \sigma^2 (\mX_\vgamma^\top \mX_\vgamma)^{-1}),                     \\
			p(\sigma^2)          \quad & = \quad (\sigma^2)^{-1} \I(\sigma^2 > 0), \text{ and }                    \\
			p(g)                 \quad & = \quad \text{unspecified for now}.
		\end{align*}
		\item We introduce a parameter $g$ which controls the shrinkage: if $g$ is high there is not much shrinkage,
					and if $g$ is low there is a lot of shrinkage.
		% that controls the mixing between the null and full models.
		% \item If $g$ is $0$, we
		% choose the null model. If $g$ is $\infty$ we choose $\hat{\vbeta}$.
	\end{itemize}
\end{frame}

\note{We adopt a linear model with the following structure.
			This structure is chosen with great care so that the the parameters can be integrated out analytically.
			We introduce a parameter $g$ which controls the shrinkage: if $g$ is high there is not much shrinkage,
			and if $g$ is low there is a lot of shrinkage.}

% Discuss Bartlett's and Information Paradox
\begin{frame}
	\frametitle{How should we choose $g$?}
	There are a few approaches we could take.
	\begin{itemize}
		\item We could simply let $g \to \infty$.
		\item \cite{Liang2008} discussed that\\
		$p(\vgamma = \vzero | \vy) \to 1$ as $g \to \infty$ (Bartlett's paradox).
		\item We could make a fixed choice of $g$. \\
		\item \cite{Liang2008} discussed that \\
					$p(\vgamma = \vgamma^* | \vy) \cancel{\to} 1$ as $R_\vgamma^2 \to 1$, \\
					where $\vgamma^*$ is the true model (Information Paradox)
		\item We could choose $g$ adaptively, using Empirical Bayes
		\item \cite{Liang2008} showed that this was not model selection consistent if the true model is the null
					model.
		\item So we need another approach.
	\end{itemize}
\end{frame}

\note{So how should we choose $g$?
			We could let $g$ go to infinity.
			If we have $g$ large, we have a diffuse prior on the coefficients. This places a lot of weight on the
			null model, which is an example of Bartlett's Paradox.
			We could make a fixed choice of $g$, but then we don't necessarily select the true model, even if the
			sample size goes to infinity, which is an example of the Information Paradox.
			We could use Empirical Bayes to choose $g$ adaptively, but Liang showed that this was not model selection
			consistent
			So we need another approach.}

% Mixture of g, choice of g prior
\begin{frame}
	\frametitle{We should choose a mixture of $g$}
	\cite{Liang2008} argued that we avoid should these paradoxes by using a mixture of $g$. \\
	Several choices of prior for $g$	have been proposed.
	\small
	\begin{itemize}
		\item hyper-$g$ prior, \cite{Liang2008}\\
		$p(g) = \frac{a-2}{2} (1 + g)^{-a/2} I(g > 0)$, $a > 2$
		\item hyper-$g/n$ prior, \cite{Liang2008}\\
		$p(g) = \frac{a-2}{2n} (1 + g/n)^{-a/2} I(g > 0)$, $a > 2$ 
		\item Beta-Prime prior on $g$, \cite{Maruyama2011} \\
		$p(g) = \frac{g^b (1 + g)^{-(a + b + 2)}}{\Beta(a + 1, b + 1)} \I(g > 0)$
		\item Bayarri's robust prior on $g$, \cite{Bayarri2012} \\
		$p(g) = \frac{1}{2} \left(  \frac{1 + n}{1 + p_\vgamma} \right)^{1/2} (1 + g)^{-3/2} I(g > (1 + n)/(1 + p_\vgamma) - 1)$
	\end{itemize}
\end{frame}

\note{In order to avoid the problems in the previous slide, Liang argued we should put a prior on $g$.
			Several choices of prior for $g$ have been proposed in the literature.
			These all place most of the prior probability on lower values of $g$, adjusting the shrinkage according
			to the sample size and number of covariates in the model.}

\begin{frame}
	\frametitle{Special functions}
	\begin{itemize}
		\item When $|z| < 1$, the hypergeometric function is defined by the power series
					\[
						{}_2 F_1 (a, b; c; z) = \sum_{n=0}^\infty \frac{(a)_n (b)_n}{(c)_n} \frac{z^n}{n!}
					\]
					where
					$(q)_n = \begin{cases}
						1 & n = 0 \\
						q (q + 1) \ldots (q + n - 1) & n > 0
					\end{cases}$
		\item The confluent hypergeometric function is defined by the power series
					\[
						{}_1 F_1 (a; b; z) = \sum_{n=0}^\infty \frac{(a)_n}{(b)_n} \frac{z^n}{n!}
					\]
		\item These functions can be difficult to numerically evaluate in practice, \cite{Press:2007:NRE:1403886}.
					We must be careful!
	\end{itemize}
\end{frame}

\note{At this point, I'll to introduce the hypergeometric function, which is defined by
			the following power series within the unit circle.
			The confluent hypergeometric function is defined by the somewhat simpler power series at the bottom of
			the slide.
			The series is divergent as $z \to 1$, which is going to come back to haunt us.
			There is no known numerical routine that will evaluate these functions for every possible parameter, and
			so we need to take care.}

% Exact posterior distributions using special functions
\begin{frame}
	\frametitle{Exact posterior distributions of $\alpha$, $u$ and $\sigma^2$ using special functions}
	% When deriving exact posterior distributions on the parameters of the model,
	We focused on the Beta-Prime prior on $g$, as it is the most numerically well-behaved.
	We were able to derive closed form expressions for the posterior distributions.
	\small
	\begin{align*}
		p(g | \vy) &= \frac{(1 - R_\vgamma^2)^{b + 1} g^b [1 + g (1 - R_\vgamma^2)]^{-(b + d + 2)}}{\text{Beta}(b + 1, d + 1)} I(g > 0)\\
		% p(u | \vy) &= \frac{(1 - R_\vgamma^2)^{b + 1}}{\text{Beta}(b + 1, d + 1)} u^b (1 - u)^d (1 - u R_\vgamma^2)^{-(b + d + 2)} \\
		p(\alpha | \vy) &= \frac{\Gamma(c + \frac{1}{2}) (1 - R_\vgamma^2)^{b + 1}}{\Gamma(c) \sqrt{\pi}} (1 + \alpha^2)^{-n/2}
		{}_2 F_1 \left(d + 1, -\frac{1}{2}; c; \frac{R_\vgamma^2}{1 + \alpha^2}\right) \\
		p(\sigma^2 | \vy) &= \frac{\frac{n}{2} (1 - R_\vgamma^2)^{b + 1}}{\Gamma(c)} (1 - R_\vgamma^2)^{-(c + 1)}
												\exp{\left( -\frac{n}{2 \sigma^2} \right)} {}_1 F_1 \left(b + 1; c; \frac{n R_\vgamma^2}{2 \sigma^2}\right)
	\end{align*}
	where $a = 1$, $b = p_\vgamma$, $c=(n - 1)/2$ and $d = c - b - 2 = p_\vgamma / 2 + a$.
\end{frame}

\note{We focused on the Beta-Prime prior, because it is numerically stable.
			We derived exact expressions for the posteriors of $g$, $\alpha$ and $\sigma^2$ under the Beta-Prime
			prior.
			$g | \vy$ is a generalised Beta-Prime distribution.
			We came up with two expressions for $\alpha | \vy$. I've presented the second of these which is using the 
			Euler identity so that the first two arguments are small in magnitude. This causes the series to converge quickly. This looks complicated, but is actually a fancy $t$-distribution.
			$\sigma^2 | \vy$ is pretty close to an inverse gamma distribution.
			These expressions depend only on $n$, $p_\vgamma$ and $R_\vgamma^2$.}

\begin{frame}
	\frametitle{Exact first and second moments of $\vbeta$}
	% We weren't able to find a closed form expression for $\vbeta | \vy$, but 
	We were able to find closed form
	expressions for the first and second moments of $\vbeta | \vy$.
	\small
	\begin{align*}
		\E[\vbeta | \vy] &= M_1 \hat{\vbeta} \\
		\Cov[\vbeta | \vy] &= \frac{n}{n - 3} (M_1 - M_2 R^2)(\mX^\top \mX)^{-1} + (M_2 - M_1^2) \hat{\vbeta} \hat{\vbeta}^\top
	\end{align*}
	where
	\small
	\begin{align*}
		M_1 &= \frac{b + 1}{c} {}_2 F_1 (d + 1, 1; c + 1; R_\vgamma^2) \text{ and } \\
		M_2 &= \frac{(b + 1)(b + 2)}{c(c + 1)} {}_2 F_1 (d + 1, 2; c + 2; R_\vgamma^2).
	\end{align*}
	Using the delta method, we were able to get the approximation
	\begin{align*}
		\vbeta | \vy \stackrel{approx}{\sim} t_{n-1} \left[M_1 \vbeta_\vgamma, \left(\frac{n}{n - 1}\right) M_1 (1 - M_1 R_\vgamma^2)(\mX_\vgamma^\top \mX_\vgamma)^{-1}\right].
	\end{align*}
\end{frame}

\note{We weren't able to find a closed form expression for the posterior of $\vbeta$, so instead we focused on
			the first and second moments. The mean and covariance are given in terms of $M_1$ and $M_2$ and these
			can be expressed in terms of the hypergeometric function.}

% Say sopmething about the Rao-Blackwellisation?

% Graphs of posterior functions
\begin{frame}
	\frametitle{Comparing exact results to Monte Carlo sampling}
	We checked our results by comparing against Monte Carlo sampling and found that they matched closely.
	\begin{figure}[htp]
		\centering
		\begin{center}$
			\begin{array}{ll}
				% \caption{$u | \vy$}
				% \includegraphics[width=.5\textwidth]{uGivenY.pdf} &
				% \caption{$\sigma^2 | \vy$}
				\includegraphics[width=.5\textwidth]{gGivenY.pdf} \\
				% \caption{$g | \vy$}
				\includegraphics[width=.5\textwidth]{alphaGivenY.pdf} &
				% \caption{$\alpha | \vy$}
				\includegraphics[width=.5\textwidth]{sigma2GivenY.pdf}
			\end{array}$
		\end{center}
	\end{figure}
\end{frame}

\note{We checked that our exact expressions were correct by checking them against a Rao-Blackwellisation sampling
			scheme that we developed, which can be considered the gold standard.
			We found that they matched very closely for a range of parameter settings. The inexactness of the Monte
			Carlo densities came about because of sampling error. Our expressions are much faster to evaluate and
			more accurate. In the event that we run into numerical problems, we can fall back on the MC scheme.}

\begin{frame}
	\frametitle{Application: US Crime data}
	\begin{figure}
		\caption{Example model fit to the US Crime data}
		\includegraphics[scale=.33]{BetaUSCrime.pdf}
	\end{figure}
\end{frame}

\note{We applied our model fitting software to the US Crime data set, and found that our
			first and second moments fit well.}

\begin{frame}
	\frametitle{References / Contact details}
	\begin{itemize}
		\item Mark Greenaway
		\item markg@maths.usyd.edu.au
		\item Twitter: @certifiedwaif	
	\end{itemize}

	\small
	\bibliographystyle{elsarticle-harv}
	\bibliography{../references_mendeley}
\end{frame}

\note{Thank you for your attention! Any questions?}

\end{document}
