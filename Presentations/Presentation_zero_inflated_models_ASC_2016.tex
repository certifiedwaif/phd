\documentclass{beamer}

\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{subcaption}
\usepackage{algorithm,algorithmic}
\input{include.tex}
\input{Definitions.tex}

\usefonttheme{serif}

\title{Gaussian Variational Bayes approximations to zero--inflated mixed models}
\author{Mark Greenaway, John Ormerod}

\mode<presentation>
{ \usetheme{boxes} }

\begin{document}
% 1. Front slide
\begin{frame}
	\titlepage
	% Details about myself here?
\end{frame}

% 2. Intro
\begin{frame}
	\frametitle{Introduction}
	\begin{itemize}
		\item Zero--inflated data arises in many areas of application.
		\item Examples include physical activity data, number of hospital
		      visits per year per person and number of insurance claims per year per person.
		\item I encountered this sort of data  while analysing physical
		      activity data arising from the Get Healthy project.
		\item Get Healthy project is run by the NSW Ministry of Health
		      where participants can be coached via phone to change their diet and maintain exercise levels in order
		      to lose weight.
	\end{itemize}		
\end{frame}

% 4. \rho = 9/10, \lambda = 5
% Example data 0 0 0 5 10
\begin{frame}[fragile]
	\frametitle{Example data}
	Consider the following counts:
	\begin{verbatim}
0 7 3 4 5 3 2 6 5 0 0 1 0 0 5 0 2 3 6 4 0 5 4 0
7 0 0 0 7 0 6 6 0 3 0 5 0 4 0 0 0 2 3 0 3 4 5 0
8 0
	\end{verbatim}
	%Take, for example, $\rho = \frac{1}{2}, \lambda = 5$.
							
	% To Do - rewrite this. It's almost impossible to understand.
			
	\noindent Note that if this data were generated by a Poisson distribution, the number of expected
	zeros would be approximately $3.8$, yet we observe $21$ zeros. This suggests a Poisson model is not suitable.
							
	% Histogram
	\begin{figure}
		% \includegraphics[width=50mm, height=50mm]{code/results/univariate_data_histogram.pdf}
		% pdf("~/Dropbox/phd/poisson_does_not_fit.pdf")
		% hist(x, , prob=TRUE)
		% points(0:8 + .5, dpois(0:8, lambda_hat), col = "red")
		% dev.off()
		\includegraphics[width=60mm, height=60mm]{poisson_does_not_fit.pdf}
	\end{figure}% Density
\end{frame}


% 3. Univariate model
\begin{frame}
	\frametitle{Latent variable formulation of zero--inflated model}
							
	\begin{columns}
		\begin{column}{0.7 \textwidth}
																			
			Suppose that we observe
			$$
			Y_i = R_i X_i, \quad 1\le i\le n,
			$$
																			
			\noindent where for $1\le i\le n$,
			\begin{align*} 
				R_i | \rho    & \sim \text{Bernoulli}(\rho) \text{ and} \\
				X_i | \lambda & \sim \text{Poisson}(\lambda)            
			\end{align*}
																			
			\noindent for parameters $\rho$ and $\lambda$,
			we use the priors:
			\begin{align*} 
				\rho    & \sim \text{Beta}(a_\rho, b_\rho)         \\
				\lambda & \sim \text{Gamma}(a_\lambda, b_\lambda). 
			\end{align*}
		\end{column}
												
		\begin{column}{0.2 \textwidth}
			\begin{figure}
				\begin{tikzpicture}[node distance=1.5cm, minimum size=0.8cm]
					\node[draw, circle, double] (y) {$\vy$};
					\node[draw, circle, below left of=y] (x) {$\vx$};
					\node[draw, circle, below of=x] (lambda) {$\lambda$};
					\node[draw, circle, below right of=y] (r) {$\vr$};
					\node[draw, circle, below of=r] (rho) {$\rho$};
					\draw[<-] (y) -- (x);
					\draw[<-] (x) -- (lambda);
					\draw[<-] (y) -- (r);
					\draw[<-] (r) -- (rho);
				\end{tikzpicture}
				\caption{Latent variable formulation of zero--inflated model}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}
	\frametitle{Extending to a multivariate mixed effects model}
	We extend this model to a multivariate mixed model formulation, allowing
	us to fit random intercept and slope models and splines. \cite{Zhao2006}

	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.4 \textwidth}
			\centering
			\begin{tikzpicture}[node distance=1.5cm, minimum size=0.8cm]
				\node[draw, circle, double] (y) {$\vy$};
				\node[draw, circle, below left of=y] (x) {$\vx$};
				\node[draw, circle, below of=x] (lambda) {$\lambda$};
				\node[draw, circle, below right of=y] (r) {$\vr$};
				\node[draw, circle, below of=r] (rho) {$\rho$};
				\draw[<-] (y) -- (x);
				\draw[<-] (x) -- (lambda);
				\draw[<-] (y) -- (r);
				\draw[<-] (r) -- (rho);
			\end{tikzpicture}
			\caption{Latent variable formulation of zero--inflated model}
		\end{subfigure}
		\begin{subfigure}[b]{0.4 \textwidth}
			\centering
			\begin{tikzpicture}[node distance=1.5cm, minimum size=0.8cm]
				\node[draw, circle, double] (y) {$\vy$};
				\node[draw, circle, below left of=y] (beta) {$\vbeta$};
				\node[draw, circle, below of=y] (u) {$\vu$};
				\node[draw, circle, below right of=y] (r) {$\vr$};
				\node[draw, circle, below of=u] (sigma2) {$\sigma^2$};
				\node[draw, circle, below of=r] (rho) {$\rho$};
				\draw[<-] (y) -- (beta);
				\draw[<-] (y) -- (u);
				\draw[<-] (y) -- (r);
				\draw[<-] (u) -- (sigma2);
				\draw[<-] (r) -- (rho);
			\end{tikzpicture}
			\caption{Mixed model extension of zero--inflated model}
		\end{subfigure}
	\end{figure}
						
\end{frame}

% 9. Extension to linear model
% 10. Overview GVA
% 11. Algorithm
% 12. Results?
% 13. What next
% 14. Conclusion
% 15. References
% \begin{frame}
% 	\frametitle{Extension to multivariate/regression models}
% 	\begin{itemize}
% 		\item Univariate models are a nice proof of concept.
% 		\item Most applied statisticians want to build regression models.
% 		\item Applied statisticians love mixed models.
% 		\item There is a need for better approaches to fitting zero-inflated mixed models.
% 		\item For example, MCMC with existing software can take minutes to hours 
% 		      to converge, or not converge at all.
% 		\item Not practical for many applications.
% 	\end{itemize}
% \end{frame}



% \begin{frame}
% 	\frametitle{Linear mixed model set-up}
% 	% Let $\vone$ be a vector of $1$s, and $\vone_n$ be a vector of $1$s of
% 	% length $n$.
	
% 	% TODO: Where are the prior specifications. Is this entirely accurate? What about for the random
% 	% slopes model?
		
% 	Consider a linear random intercept model with
% 	$$
% 	\begin{array}{ll}
% 		\vy|\vbeta,\vu,\sigma_y^2 & \sim \N(\mX \vbeta + \mZ \vu, \sigma_\vy^2 \mI)  \\[2ex]
% 		\vbeta                    & \sim \N(\vzero, \sigma_\vbeta^2 \mI) \text{ and} \\[2ex]
% 		\vu|\sigma_{\vu}^2        & \sim \N(\vzero, \sigma_{\vu}^2 \mI).             
% 	\end{array}
% 	$$
% 	\noindent where
% 	$$
% 	\begin{array}{rll}
% 		            & \vy    & =                                 
% 		\begin{pmatrix}
% 		y_1 \\
% 		\vdots \\
% 		y_n
% 		\end{pmatrix}, \\[2ex]
% 		            & \mX    & = [\vone_n, \vx_1, \cdots, \vx_p] \\
% 		            & \mZ    & =                                 
% 		\begin{bmatrix}
% 		\vone_{n_1} &        &                                   \\
% 		            & \ddots &                                   \\
% 		            &        & \vone_{n_m}                       
% 		\end{bmatrix} \\
% 		\text{and}  & \mC    & = [\mX, \mZ].                     
% 	\end{array}
% 	$$
% \end{frame}




% 5. How to fit, and advantages and disadvantages of each approach
% - Maximum likelihood
% - MCMC
% - VB


\begin{frame}
	\frametitle{Multivariate model formulation}
	We modify a Poisson random effects model to include the vector of non--zero indicators from
	before.
							
	\medskip
							
	Suppose that we observe a vector of responses $\vy$ of length n, and matrices
	of covariates $\mX$ and $\mZ$ of dimensions $n \times p$ and $n \times m$,
	respectively.	

	\begin{align*}
		p(\vy|\vbeta, \vu, \vr)       & = \exp{\left[\vy^\top \mR(\mX \vbeta + \mZ \vu) - \vr^\top e^{\mX \vbeta + \mZ \vu} - \vone^\top \log{\Gamma(\vy + \vone)}\right]}, \\
		\vr_i|\rho                    & \stackrel{\mbox{\tiny iid}}{\sim} \text{Bernoulli}(\rho), \ \{i \colon \vy_i=0\},                                                   \\
		\text{and }\vu|\sigma_{\vu}^2 & \sim \N({\bf 0}, \mSigma)
	\end{align*}
	\noindent where $\mR =\mbox{diag}(\vr)$.
	We use conjugate priors for $\mSigma$ and $\rho$, and a normal prior for $\vbeta$.
\end{frame}


\begin{frame}
	\frametitle{Comparison of fitting techniques}
	\begin{tabular}{p{2cm}p{3.5cm}p{4.5cm}}
		Technique         & Pro                                                             & Con                                                 \\
		\hline
		MLE               & EM could be used to fit these models, with $R_i$ as latent data & Not flexible enough for mixed models               \\
		                  &                                                                 &                                                     \\ %Frequentist \\
		\hline
		MCMC              & Bayesian                                                        & Slow                                                \\
		                  & Very accurate                                                   &                                                     \\
		\hline
		Variational Bayes & Bayesian                                                        & May lose accuracy, or \mbox{underestimate} variance \\
		                  & Fast                                                            &                                                     \\ %Solution may be intractable \\ 
		                  & Can be quite \mbox{accurate}                                    &                                                     \\
		\hline
	\end{tabular}
							
\end{frame}

% 6. Overview of Variational Bayes
\begin{frame}
	\frametitle{An overview of Variational Bayes}
	\begin{itemize}
		\item \emph{Idea:} Approximate the full posterior $p(\theta|\vy)$ with a simpler approximation $q(\vtheta)$
					\cite{Ormerod2010}.
		      		      		      		      		      		      		      
		\item Fit $q(\vtheta)$ to the data by minimising the KL divergence between $p(\vtheta|\vy)$ and $q(\vtheta)$.

		\item This turns statistical inference into an optimisation problem.
		      		      		      		      		      		      		      
		\item We iteratively update the parameters of each approximate distribution
		      in turn until the lower bound of the approximation converges.
		      
		      % \item Theory guarantees that $\log p(\vy)\ge 
		      %       \log \underline{p}(\vy;q)$ and that 
		      %       $$
		      %       \log \underline{p}(x;q) = \int q(\vtheta) \left\{ \frac{p(\vy,\vtheta)}{q(\vtheta)} \right\} d \vtheta
		      %       $$ 
		      		      		      		      		      		      		      
		      %       \noindent will
		      %       increase with each iteration.
		      		      		      		      		      		      		      
		\item If you use a factored approximation with conjugate distributions, these mean field
			  updates will either have closed forms/be analytically tractable.
	\end{itemize}
\end{frame}

% \begin{frame}
% 	\frametitle{An overview of Variational Bayes - Continued}
% 	% - Algorithm
% 	We iteratively update the parameters of each approximate distribution
% 	in turn until the lower bound of the approximation converges.
						
% 	\bigskip 
% 	This could be thought of as a generalisation of Expectation Maximisation, where each parameter is thought of as a latent
% 	variable and estimated according to the expectations of the other parameters.
% \end{frame}


\begin{frame}
	\frametitle{Form of the multivariate approximation}
	We choose an approximation of the form
	$$
	q(\theta) = q(\vbeta, \vu) q(\sigma_u^2) q(\rho) \prod_{ \{i : \vy_i = 0\} } q(r_i)
	$$
	where
	\begin{align*}
		q(\vbeta, \vu) & \text{ is a } \text{N}(\vmu, \mLambda) \text{ distribution},                                  \\
		q(\sigma_u^2)  & \text{ is a } \text{IG}(\alpha_{\sigma_u^2}, \beta_{\sigma_u^2}) \text{ distribution},        \\
		q(\rho)        & \text{ is a } \text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)}) \text{ distribution} \text{ and} \\
		q(r_i)         & \text{ is a } \text{Bernoulli}(p_i) \text{ distribution}, \ \ 1 \leq i \leq n.                
	\end{align*}
\end{frame}

\begin{frame}
	\frametitle{Gaussian and Laplace's Method Variational Approximations}
	\begin{itemize}
		\item Lack of conjugacy means mean field updates won't be analytically tractable for the regression parameters.
		\item We try Laplace's method and Gaussian Variational Approximations (GVA \cite{Ormerod2012}) instead, assuming that
		      $$
		      \begin{pmatrix}
		      	\vbeta \\
		      	\vu    
		      \end{pmatrix}
		      \sim \N(\vmu, \mLambda)
		      $$
		      and approximate as closely as we can.
		\item For each iteration, we optimise to find
		      $\begin{pmatrix}
		      	\vbeta \\
		      	\vu    
		      \end{pmatrix}
		      $ and $\mLambda$,
		      and then perform mean field updates on the other parameters.
	\end{itemize}
\end{frame}

% \begin{frame}
% 	\frametitle{Mean field updates}
% 	\begin{align*}
% 		q(\vbeta, \vu)    & = \N(\vmu, \mLambda),                                                                                                                                                                                 \\
% 		q(\sigma_{\vu}^2) & = \IG\left(\alpha_{q(\sigma_{\vu}^2)} = \alpha_{\sigma_u^2} + \frac{m}{2}, \beta_{q(\sigma_{\vu}^2)} = \beta_{\sigma_u^2} + \frac{\|\vmu_\vu\|^2}{2} + \frac{\text{tr}(\mLambda_{\vu\vu})}{2}\right), \\
% 		q(\rho)           & = \Beta(\alpha_{q(\rho)} = \alpha_\rho + \vone^\top \vp, \beta_{q(\rho)} = \beta_\rho + \vone^\top(\vone - \vp))\text{ and}                                                                           \\
% 		q(r_i)            & \sim \Bernoulli{(p_i)}, \ \ \ 1 \leq i \leq n                                                                                                                                                         
% 	\end{align*}
% 	where
% 	$$
% 	p_i = \expit{\left [\Psi{(\alpha_{q(\rho)})} - \Psi{(\beta_{q(\rho)})} - \exp{\left(\vc_i^\top \vmu + \half \vc_i^\top  \mLambda \vc_i\right)} \right ]}.
% 	$$
% \end{frame}

\begin{frame}
	\frametitle{Fitting algorithm}
		
	\begin{algorithm}[H]
		\caption[Algorithm 1]{Iterative scheme for obtaining the parameters in the
			optimal densities $q^*(\vmu, \mLambda)$, $q^*(\mSigma_{\vu \vu})$ and $q^*(\rho)$}
		\label{alg:algorithm_one}
		\begin{algorithmic}
			\REQUIRE{$p_{q(\mSigma_{\vu \vu})} \leftarrow p + 1$} \\[1ex]
			\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
			% \vmu, \mLambda
			\STATE Optimise $\vmu$ and $\mLambda$ using $\vy, \mC, \vp$ and $\mSigma_{\vu \vu}$ \\[1ex]
			% 			% \vp
			\STATE $\alpha_{q(\rho)} \leftarrow \alpha_\rho + \vone^\top\vp$
			\STATE $\beta_{q(\rho)} \leftarrow \beta_\rho + n - \vone^\top\vp$ \\[1ex]
			\FOR{$i=1$ to $n$}
				\STATE $\eta_i \leftarrow \vy_i (\mC \vmu)_i - \exp \left [ \mC \vmu + \half (\mC\mLambda\mC^\top)_{ii} \right ] + \psi{(\alpha_{q{(\rho)}})} - \psi{(\beta_{q{(\rho)}})}$ \\[1ex]
				\STATE $\vp_i \leftarrow \expit{(\eta_i)}$ \\[1ex]
			\ENDFOR
			% \mSigma_{\vu \vu}
			\STATE $\mPsi_{q(\mSigma_{\vu \vu})} \leftarrow \mPsi + \sum_{i=1}^m (\vmu_i \vmu_i^\top + \mLambda_{\vu_i \vu_i})$ \\[1ex]
			\STATE $\mSigma_{\vu\vu} \leftarrow [\mPsi_{q(\mSigma_{\vu \vu})}/(v - d - 1)]^{-1}$
			\ENDWHILE
			\end{algorithmic}
		\end{algorithm}
						
	\end{frame}
			
	%\begin{frame}
	%\frametitle{Progress so far}
	%\begin{itemize}
	%\item Computation - a work in progress. The multivariate model is
	%implemented by combining my univariate VB code with John's GVA code.
	%\item Initial signs are that this approach will work.
	%\item The correct parameters are estimated for simulated test cases
	%that we have tried.
	%\item I've calculated the lower bound, but have not yet checked that it
	%always increases for my test cases.
	%\item Accuracy will be assessed against the random walk Metropolis-Hastings approximation of the true posterior.
	%\end{itemize}
	%\end{frame}
			
	%\begin{frame}
	%\frametitle{What next?}
	%\begin{itemize}
	%\item Continue working on the multivariate approximation.
	%\item Extensions - random slopes, splines, and measurement error can all be 
	%accomodated within a mixed model framework.
	%\item Check accuracy against random walk Metropolis-Hastings approximation
	%of the true posterior.
	%\item Apply the mixed model fitting code to my physical activity data.
	%\item Write this all up into a paper.
	%\item Release an R package.
	%\end{itemize}
	%\end{frame}
			
			
	%\begin{frame}
	%\frametitle{Gaussian variational approximation}
	%% Details from last time: Gaussian variational approximation
	%For generalised linear models, there is no tractable factored 
	%approximation which works well, so we attempt to approximate
	%the GLM with a multivariate Gaussian (reference relevant
	%Ormerod paper).
	%\end{frame}
			
	% \begin{frame}
	% 	\frametitle{Laplace's method  of approximation}
	% 	We Taylor expand the log likelihood $\log{p(\vtheta)}$ around the mode
	% 	$\vtheta_*$:
	% 	\tiny
	% 	\begin{align*}
	% 		\log{p(\vtheta)} & \approx \log{p(\vtheta_*)} + \log{p'(\vtheta_*)}^\top (\vtheta - \vtheta^*) + \half (\vtheta - \vtheta^*)^\top \log{p''(\vtheta_*)}(\vtheta - \vtheta^*) \\
	% 		                 & = \log{p(\vtheta_*)} + \half (\vtheta - \vtheta^*)^\top \log{p''(\vtheta_*)}(\vtheta - \vtheta^*)                                                        
	% 	\end{align*}
	% 	\small
	% 	as $\log{p^{'}(\vtheta_*)}(\vtheta - \vtheta^*) = 0$ at the mode. This has
	% 	the same form as a Gaussian density
	% 	\begin{align*}
	% 		\log{N(\vtheta|\vmu, \eta^{-1})} & = \half \log{\eta} - \frac{p}{2} \log{2 \pi} - \frac{\eta}{2} (\theta-\mu)^\top (\theta-\mu) \\
	% 		                                 & = \half \log{\frac{\eta}{2\pi}} + \half (-\eta)(\theta-\mu)^\top (\theta-\mu)                
	% 	\end{align*}
					
	% 	and so we have an approximate posterior $q(\vtheta) = \N(\vtheta|\mu, \eta^{-1})$ with
	% 	\begin{align*}
	% 		\mu  & = \theta_*              & \text{mode of the log-posterior, and}  \\
	% 		\eta & = -\log{p''(\vtheta_*)} & \text{negative curvature at the mode.} \\
	% 	\end{align*}
	% \end{frame}
			
	\begin{frame}
		\frametitle{Optimising the Gaussian Variational Approximation}
		The true likelihood $\log p(\vbeta, \mLambda)$ is hard to optimise due 
		to the high dimensional integral involved:
		\begin{align*}
			\log p(\vbeta, \mSigma | \vr) & = \vy^\top \mR \mX \vbeta + \vone^\top c(\vy) + \frac{m}{2} \log |\mSigma| - \frac{mK}{2} \log{(2 \pi)}    \\
			                               & \quad + \log  \int_{\mathbb{R}^{m}} \exp \big[ \vy^\top \mR \mZ \vu - \mR^\top b( \mX \vbeta + \mZ \vu)   \\
			                               & \quad \quad \quad \quad \quad \quad \quad - \half {\vu^\top \mSigma^{-1} \vu } \big] d \vu + \text{const.} 
		\end{align*}
		where $b(x) = e^x$ and $c(x) = -\log \Gamma(x + 1)$. But the Gaussian Variational lower bound $\log
		\underline{p}(\vmu, \mLambda)$ can be optimised using quasi-Newton Raphson style algorithms 
		\cite{Nocedal2006}.
		\begin{align*}		                                   
			\log \underline{p}(\vmu, \mLambda) & = \vy^\top \mP \mC \vmu - \vp^\top \exp{\left(\mC \vmu + \half \dg{(\mC \mLambda \mC^\top)}\right)}                         \\
			                                   & \quad - \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mLambda \mSigma^{-1})} + \half \log{|\mSigma^{-1}\mLambda|} + \text{const.} 
			%&= \mR\mC^T(\vy - B^{(1)}(\vmu, \sigma^2_u)) \\
			%& \quad - \vmu^T \mSigma^{-1} \vmu - \half \tr{(\mLambda \mSigma^{-1})} + \half \log{|\mSigma^{-1}\mLambda|} + \text{const.} \\
		\end{align*}
		where $\vnu = (\vbeta, \vu)^\top$, $\mC = [\mX, \mZ]$, $\vp = \E_q[\vr]$ and $\mP = \diag(\vp)$.
	\end{frame}
			
	\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
			
	\begin{frame}
		\frametitle{Parameterisations to fit model using GVA}
		\begin{itemize}
			% Detail what progress has been made, and what results have been obtained
			\item There are several \mbox{alternative} parameterisations we've implemented
			      to fit this model:
			      % Point out the advantages of each
			      % \begin{enumerate}
			      % 	\item Optimise variational lower bound with L-BFGS, $\mLambda = \mR \mR^T$.\\
			      % 	      Advantage: Accurate.
			      % 	\item Optimise variational lower bound with L-BFGS, $\mLambda = (\mR \mR^T)^{-1}$. \\
			      % 	      Advantage: Accurate, and very fast.
			      % 	\item Newton-Raphson optimisation on the variational lower bound, using block inverses. \\
			      % 	      Advantage: Straightforward to implement, and very fast. \\
			      % 	      Disadvantage: Unstable.
			      % \end{enumerate}
			      \begin{tabular}{|l|ccc|}
			      	\hline
			      	Method                    & Accuracy   & Speed      & Stability  \\
			      	\hline
			      	L-BFGS, covariance param. & \checkmark &            & \checkmark \\
			      	L-BFGS, precision param.  & \checkmark & \checkmark & \checkmark \\
			      	Newton-Raphson iteration  & \checkmark & \checkmark &            \\
			      	\hline
			      \end{tabular}	
			\item These algorithms optimise the Gaussian Variational lower bound with respect to
			      $\vmu$ and $\mLambda$.
			\item They should all fit the same model to the same data
			      within numerical tolerances.
			\item The crucial part is how we parameterise $\mLambda$.
		\end{itemize}
	\end{frame}
			
	\begin{frame}
		\frametitle{Parameterising $\mLambda$ using its' Cholesky factors}
		\begin{itemize}
			\item Any symmetric matrix $\mSigma$ can be written $\mSigma = \mR \mR^\top$
			      where $\mR$ is lower triangular. $\mR$ is unique if $r_{ii} \geq 0$. 
			      \begin{align*}
			      	&\begin{pmatrix}
			      	r_{11}        & 0                            & 0                               \\
			      	r_{21}        & r_{22}                       & 0                               \\
			      	r_{31}        & r_{32}                       & r_{33}                          
			      	\end{pmatrix}
			      	\begin{pmatrix}
			      	r_{11}        & r_{21}                       & r_{31}                          \\
			      	0             & r_{22}                       & r_{32}                          \\
			      	0             & 0                            & r_{33}                          
			      	\end{pmatrix}
			      	\\
			      	=& \begin{pmatrix}
			      	r_{11}^2      &                              & \text{symmetric}                \\
			      	r_{21}r_{11} & r_{21}^2 + r_{22}^2 \\
			      	r_{31} r_{11} & r_{31}r_{21} + r_{32} r_{22} & r_{31}^2 + r_{32} ^2 + r_{33}^2 
			      	\end{pmatrix}.
			      \end{align*}
			\item Note that the lower rows of the product depend on the higher rows of the Cholesky factor. We will
			      exploit this fact.
			\item We parameterise $\mLambda$ as $\mLambda = \mR \mR^\top$ so that it's symmetric
			      and	we have half as many parameters to deal with.
			\item We ensure $\mLambda$ is positive semi--definite by parameterising the diagonals
			      $\mLambda_{ii} = \exp(\mR_{ii})^2$.
		\end{itemize}	
	\end{frame}
			
	\begin{frame}
		\frametitle{Structure of $\mLambda$}
		\begin{itemize}
			\item By re-ordering the fixed and random effects in our design matrices and hence
				  in $\mLambda$, we end up with a covariance structure which is sparse in the 
				  upper left block.
			      \begin{figure}
			      	\begin{subfigure}{0.4\textwidth}
			      		\includegraphics[scale=.15]{mX_mZ_mLambda.pdf}
			      		\caption{\tiny Covariance matrix -- Fixed effects before random effects}
			      	\end{subfigure}
			      	%
			      	\begin{subfigure}{0.4\textwidth}
			      		\includegraphics[scale=.15]{mZ_mX_mLambda.pdf}
			      		\caption{\tiny Covariance matrix -- Random effects before fixed effects}
			      	\end{subfigure}
			      				      				      			      			      			      	
			      	\begin{subfigure}{0.4\textwidth}
			      		\includegraphics[scale=.15]{mX_mZ_cholesky.pdf}
			      		\caption{\tiny Cholesky factor -- Fixed effects before random effects}
			      	\end{subfigure}
			      	%
			      	\begin{subfigure}{0.4\textwidth}
			      		\includegraphics[scale=.15]{mZ_mX_cholesky.pdf}
			      		\caption{\tiny Cholesky factor -- Random effects before fixed effects}
			      	\end{subfigure}
			      \end{figure}
		\end{itemize}
	\end{frame}
			
	% \begin{frame}
	% 	% Motivations behind the algorithms
	% 	\frametitle{Motivations behind the algorithms}
	% 	% ii) The Cholesky factor of a block matrix of the form
	% 	% diag for random effects, block for cross effects
	% 	% block for cross effects, diag for fixed effects
	% 	% is mostly diagonal
	% 	% Less parameters to optimise and store
	% 	If we re-arrange the covariance matrix to have random effects before
	% 	fixed effects, our posterior covariance matrix is of the form
	% 	\begin{align*} \mLambda =
	% 		\begin{pmatrix}
	% 		\mLambda_{\vu_1 \vu_1}    & 0                      & \ldots & 0                          & \mLambda_{\vbeta_0 \vu_1}    & \mLambda_{\vbeta_1 \vu_1}    \\
	% 		0                         & \mLambda_{\vu_2 \vu_2} & 0      & 0                          & \vdots                       & \vdots                       \\
	% 		\vdots                    & 0                      & \ddots & 0                          & \vdots                       & \vdots                       \\
	% 		0                         & 0                      & \ldots & \mLambda_{\vu_{m} \vu_{m}} & \mLambda_{\vbeta_0 \vu_{m}}  & \mLambda_{\vbeta_1 \vu_{m}}  \\
	% 		\mLambda_{\vbeta_0 \vu_1} & \ldots                 & \ldots & \mLambda_{\vbeta_0 \vu_m}  & \mLambda_{\vbeta_0 \vbeta_0} & \mLambda_{\vbeta_0 \vbeta_1} \\
	% 		\mLambda_{\vbeta_1 \vu_1} & \ldots                 & \ldots & \mLambda_{\vbeta_1 \vu_m}  & \mLambda_{\vbeta_1 \vbeta_0} & \mLambda_{\vbeta_1 \vbeta_1} 
	% 		\end{pmatrix}
	% 	\end{align*}
					
	% 	As the $\sigma_{\vu^2}$ block is diagonal, the Cholesky factor is 
	% 	sparse, with $\half m(m-1)$ zero elements. If $p$ is small
	% 	and $m$ is large, as will typically be the case, then dimension of the parameter space of our optimisation
	% 	problem will be much less.
	% \end{frame}
			
	% \begin{frame}
	% 	\frametitle{L-BFGS-B}
	% 	\begin{itemize}
	% 		\item Approximate a smooth convex function locally with a quadratic form i.e. Taylor expand around the current point twice.
	% 		      \[
	% 		      	m_k(\vp) = f(\vx_k) + \nabla f(\vx_k)^\top \vp + \half \vp^\top \mB \vp
	% 		      \]
	% 		\item \begin{enumerate}
	% 		\item Gradient and Hessian establish the direction of line search.
	% 		\item Line search to find the best point in that direction.
	% 		\end{enumerate}
	% 		\item We used \texttt{optim} in base R, which tries lots of points alone the line, including some extreme 
	% 		      ones.
	% 		\item The parameterisation of the variational lower bound that we chose proved to be unstable in some 
	% 		      regions. Much effort went into mitigating this.
	% 	\end{itemize}
	% \end{frame}
			
	\begin{frame}
		\frametitle{Application}
		\begin{itemize}
			\item The application data set is a study of cockroach infestation in city apartments, taken from
			      Data Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer
			      Hill, \S 6.7.
			      % TODO: Might need more explanation.
			\item The model is $$y_i = \beta_0 + \beta_1 t_i + \beta_2 t_i \times \text{treatment}_i + u_i, \quad 1 \leq i \leq n,$$ 
			      where\\
			      $y_i$ = cockroaches caught in apartment i, \\
			      $\beta_0$ = fixed intercept, \\
			      $\beta_1$ = time, \\
			      $\beta_2$ = time $\times$ treatment, \\
			      $u_1, \ldots, u_{12}$ random intercepts for the 12 buildings in the study, and \\
			      $t_i$ is the time in days.\\
		\end{itemize}
	\end{frame}
		
	
	\begin{frame}
		\frametitle{Accuracy results using MCMC}
		\begin{itemize}
			\item	We measure the accuracy using
			      $$\text{Accuracy} = 1 - \half \int \left |p(\vtheta|\vy) - q(\vtheta) \right | d \vtheta.$$
			\item	$p(\vtheta|\vy)$ was estimated using the kernel density estimate of the posterior
			      distribution from MCMC.
			\item MCMC was run for 1 million iterations from Stan.
		\end{itemize}
	\end{frame}
		
			
	% \begin{frame}
	% 	\frametitle{Accuracy results}
	% 	\begin{tabular}{|c|cccc|}
	% 		\hline
	% 		               & Laplace & GVA $(\mLambda=\mR \mR^\top)$ & GVA $[\mLambda=(\mR \mR)^{-1}]$ & GVA NR \\
	% 		\hline
	% 		$\vbeta_0$     & 0.84      & 0.94                          & 0.89                            & 0.94   \\
	% 		$\vbeta_1$     & 0.85      & 0.99                          & 0.97                            & 0.99   \\
	% 		$\vbeta_2$     & 0.85      & 0.97                          & 0.94                            & 0.97   \\
	% 		$\vu_1$        & 0.86      & 0.95                          & 0.90                            & 0.95   \\
	% 		$\vu_2$        & 0.88      & 0.95                          & 0.90                            & 0.95   \\
	% 		$\vu_3$        & 0.83      & 0.94                          & 0.90                            & 0.94   \\
	% 		$\vu_4$        & 0.58      & 0.93                          & 0.90                            & 0.93   \\
	% 		$\vu_5$        & 0.75      & 0.88                          & 0.82                            & 0.88   \\
	% 		$\vu_6$        & 0.62      & 0.83                          & 0.75                            & 0.83   \\
	% 		$\vu_7$        & 0.87      & 0.94                          & 0.91                            & 0.94   \\
	% 		$\vu_8$        & 0.83      & 0.97                          & 0.94                            & 0.97   \\
	% 		$\vu_9$        & 0.65      & 0.84                          & 0.79                            & 0.84   \\
	% 		$\vu_{10}$     & 0.84      & 0.96                          & 0.91                            & 0.96   \\
	% 		$\vu_{11}$     & 0.84      & 0.95                          & 0.92                            & 0.95   \\
	% 		$\vu_{12}$     & 0.79      & 0.96                          & 0.91                            & 0.96   \\
	% 		$\sigma_\vu^2$ & 0.58      & 0.57                          & 0.62                            & 0.57   \\
	% 		$\rho$         & 0.87      & 0.88                          & 0.87                            & 0.88   \\
	% 		\hline
	% 	\end{tabular}
	% \end{frame}
	
	\begin{frame}
		\frametitle{Accuracy of parameter estimation}
		\begin{figure}
			% \caption{Accuracy of parameter estimation}
			% \includegraphics[scale=0.23]{code/results/accuracy_plots_application_laplace-montage.pdf}
			\includegraphics[scale=0.45]{code/results/accuracy_of_parameter_estimation.pdf}
		\end{figure}
	\end{frame}
			
	% Pretty pictures
	\begin{frame}
		\frametitle{Plots of the MCMC-estimated and approximating densities}
		\begin{figure}
			\caption{Laplace's method}
			\includegraphics[scale=0.23]{code/results/accuracy_plots_application_laplace-montage.pdf}
		\end{figure}
	\end{frame}
			
	\begin{frame}
		\frametitle{Plots of the MCMC-estimated and approximating densities}
		\begin{figure}
			\caption{GVA (precision parameterisation)}
			\includegraphics[scale=0.23]{code/results/accuracy_plots_application_GVA-montage.pdf}
		\end{figure}
	\end{frame}
			
	% \begin{frame}
	% 	\frametitle{Plots of the MCMC-estimated and approximating densities}
	% 	\begin{figure}
	% 		\caption{GVA ($\mLambda^{-1}$ parameterisation)}
	% 		\includegraphics[scale=0.15]{code/results/output_montage_application_gva2.png}
	% 	\end{figure}
	% \end{frame}
			
	% \begin{frame}
	% 	\frametitle{Plots of the MCMC-estimated and approximating densities}
	% 	\begin{figure}
	% 		\caption{GVA Newton-Raphson}
	% 		\includegraphics[scale=0.15]{code/results/output_montage_application_gva_nr.png}
	% 	\end{figure}
	% \end{frame}
			
	\begin{frame}
		\frametitle{Stability results}
				
		% A data set of 100 individuals in ten groups (m=10) was generated from a model with a fixed intercept
		% and slope, and a random intercept. $\vmu$ was initialised from a grid of points on the interval
		% $[-4.5, 5]$ for intercept and slope. The error counts are presented below.
				
		\begin{itemize}
			\item The numerical stability of each fitting algorithm was assessed on simulated data 
					by initialising each algorithm from a range of different starting points.
			\item A count of errors due to numerical instability and the fitted
				  $\vmu$ were recorded for each starting point.
		\end{itemize}				
		\begin{table}
			\caption{Count of numerical errors for each algorithm during stability tests}
			\label{tab:stability_results}
			\begin{tabular}{|l|r|}
				\hline
				Algorithm                       & Error count \\
				\hline
				Laplace's algorithm             & 12          \\
				GVA covariance parameterisation & 1,771       \\
				GVA precision parameterisation  & 537         \\
				% GVA Newton-Raphson fixed point  & 992         \\
				\hline
			\end{tabular}
		\end{table}
	\end{frame}
		
	\begin{frame}{Conclusion and future work}
		\begin{itemize}
			\item I'm intending to release the \texttt{zipvb} on GitHub when it's ready.
			\item 1,463 lines of R, 117 lines of C++ using \texttt{Rcpp} and \texttt{RcppEigen}.
			\item Interface is currently pretty rough. I'd prefer an interface modelled on \texttt{lme4}.
			\item Random intercept and random slope models are supported.
			\item Splines are also supported.
			\item Other covariance structures could easily be added.
			\item The current parameterisation is numerically unstable. We will modify the Gaussian
			      Variational Approximation part of the algorithm to solve this problem.
		\end{itemize}	
	\end{frame}
			
	\begin{frame}
		\frametitle{References}
		\bibliographystyle{elsarticle-harv}
		\bibliography{references_mendeley}
	\end{frame}

			
\end{document}
