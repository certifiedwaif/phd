\documentclass{amsart}

\input{include.tex}
\input{Definitions.tex}

\begin{document}

\section{Latent variable formulation}

Suppose we observe counts data, which is a mixture of zero and non-zero Poisson parts. Let $y_i$ be the vector
of observations, $r_i$ be a vector latent variables indicating whether each observation comes from the zero
part of the mixture or the Poisson part and $x_i$ be the Poisson count if an observation is non-zero. $\rho$
is the proportion of observations coming from the zero part of the mixture. We put a gamma prior on $\lambda$,
the parameter of the Poisson distribution, and a beta prior on $\rho$.

\section{Extending the univariate zero--inflated model}

This gives us some idea of how to construct a zero--inflated model. But we want to fit regression models, and
we'd like these models to be flexible enough to deal with complications like repeated observations for
longitudinal studies or multi-level models. We'd also like to have enough flexibility to fit curves to data. So
we extend this formulation to a mixed model.

We replace the non--zero Poisson part of our mixture with a multivariate normal distribution for ease of
interpretation. Altering the covariance structure of the normal allows us to incorporate random effects or
splines.

\section{Multivariate model formulation}

To construct a zero--inflated Poisson mixed model, we make a slight alteration to a Poisson regression model
in exponential family form, multiplying the vector of responses $\vy$ by the vector of non-zero indicators
$\vr$.

Note that we've used a normal prior for this model for ease of interpretation, which is not conjugate with the
Poisson model. So this is a non-conjugate model.

\section{Fitting techniques}

There are a number of fitting techniques that we could attempt to use. We could use EM, but it is not flexible
to complications like mixed models.

A popular choice for Bayesian random effects models is Monte Carlo Markov Chains, which is very accurate but inherently computationally intensive.

An alternative which has become more popular in the statistical community in recent years is Variational
Bayes.

\section{Variational Bayes}

Variational Bayes is a technique where the full posterior distribution $p(\vtheta | \vy)$ is approximated
by a distribution $q(\vtheta)$. The KL divergence between $p(\vtheta | \vy)$ and $q(\vtheta)$ is then
iteratively minimised.

\section{Form of the multivariate approximation}

We adopt a factored approximation, partitioning the fixed effects $\vbeta$ and the random effects $\vu$
together, and choose a multivariate normal distribution for ease of interpretation.

For the other parameters, we choose distributions matching the conjugate priors for those parameters.

\section{Gaussian Variational Approximations}

Because the multivariate normal is not conjugate, we cannot derive a closed form mean field update for
$\vbeta$ and $\vu$. Instead, we use a Gaussian Variational Approximation to optimise $\vbeta$ and $\vu$
while keeping the other parameters fixed, and then perform mean field updates on the other parameters in turn.
Thus we arrive at the following algorithm.

\section{Algorithm}

\section{Optimising the Gaussian Variational Approximation}

The true log likelihood is hard to optimise, because we need to integrate over the random effects. But the
Gaussian Variational lower bound can be optimised using quasi-Newton Raphson style algorithms.

\section{Parameterisations}

There are several alternative parameterisaitons we've implemented to fit this model. The main difference between
these parameterisations is in how the covariance matrix is parameterised.

\section{Cholesky factors of $\mLambda$}

Our parameterisations are in terms of the Cholesky factors of $\mLambda$. Any symmetric matrix $\mSigma$
can be factorised into a lower triangular matrix multiplied by its' own transpose.

Note that the lower rows of the product depend on the higher rows of the Cholesky factor. We will exploit this
fact.

We parameterise $\mLambda$ as $\mLambda = \mR \mR^\top$ so that $\mLambda$ is symmetric and we have half as
many parameters to deal with.

We ensure $\mLambda$ is positive semi--definite by parameterising the diagonals
$\mLambda_{ii} = \exp(\mR_{ii})^2$.

\section{Structure of $\mLambda$}

By re--ordering the fixed and random effects in $\mLambda$, we can ensure sparsity in the Cholesky factors,
as we can see in the following figures.

\section{Application}

We applied our fitting algorithm to a data set on a study of cockroach infestation in city apartments. We fit
a random intercept model, allowing for variation in the number of cockroaches in each building the apartments
were in.

\section{Accuracy results}

We measure the accuracy using the $L_1$ norm of the difference between the true posterior and the approximating
distribution.

The true posterior was estimated using Stan.

\section{Accuracy of parameter estimation}

We can see that Laplace's method does okay, but the Gaussian Variational Approximation does better on all
parameters. We see that the covariance parameterisation is more accurate than the precision parameterisation.

\section{Plots}

and when we look at the plots of the distributions, we can see why. The Laplace approximation consistently
underestimates the variance, while the Gaussian Variational Approximation gets it right.

\section{Stability results}

We also tested the stability of the algorithms. And we see that Laplace's algorithm is the most stable,
followed by the precision parameterisation, and then the covariance parameterisation followed by the
Newton-Raphson fixed point algorithm.

\section{R package}

I'm intending to release a zipvb package on GitHub when it's ready.

...

The current parameterisation is numerically unstable, because the exponential function grows very quickly.
Over the summer, we plan on trying a function which grows less aggressively.

\end{document}