\documentclass{article}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\title{Variational approximations to zero-inflated Bayesian models}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\maketitle

Abstract:

Keywords: Approximate Bayesian inference . mixed model . Markov chain Monte Carlo

\section{Introduction}

% First, simplest zero-inflated count model to consider.
\noindent Count data with a large number of zero counts arises in many areas of
application, such as data arising from physical activity studies, 
insurance claims, hospital visits or defects in manufacturing processes.

\noindent While simple forms of these models are easy to fit with maximum likelihood techniques,
more general models incorporating random effects, splines and missing data typically
have no closed form solutions. Fitting these models is typically done with Monte Carlo
Markov Chain techniques, but these can be slow and prone to convergence problems. We
propose using Variational Bayes to fit close approximations to these models
using a deterministic algorithm which converges much more quickly.

% Cite prior publications in this area

\noindent In this paper, we build upon the earlier work of \cite{lambert1992} and
\cite{Ghosh20061360}.

\noindent In Section \ref{sec:methodology} we provide the framework for our approach. In
Section \ref{sec:methodology} we extend our approach to incorporate regression modelling
and random effects. In Section \ref{sec:results} we show how our approach offers 
computational advantages over existing approaches. In Section \ref{sec:application} we 
show an application of our method to physical activity data. Appendices contain details 
of our MCMC samplers.

\subsection{Notation}

The notation $\vx \sim N(\vmu, \mLambda)$ means that $\vx$ has a multivariate normal
density with mean $\vmu$ and covariance $\mLambda$. If $x$ has an inverse gamma
distribution, denoted $x \sim \mbox{IG}(\alpha, \beta)$, then it has density
$p(x) = \beta^\alpha \Gamma(\alpha)^{-1}x^{-\alpha-1} \exp{(-\beta/x)}, x, \alpha,
\beta > 0$.
If $\vx$ is a vector of length $d$ then $\mbox{diag}(\vx)$ is the $d \times d$
diagonal matrix whose diagonal elements are $\vx$.

\section{Methodology}\label{sec:methodology}

In this section we present a VB approach to a Bayesian zero-inflated Poisson model
for count data with extra zeroes. After introducing Bayesian zero-inflated models
and VB methodology we derive the VB factorised approximation to the full Bayesian
model. 

\subsection{Variational Bayesian inference}


\section{Extending the zero-inflated Poisson model to a regression model}

\noindent The above univariate model demonstrates that variational approximations are well-suited
to accelerating the fit of Bayesian zero-inflated models to data. Typically zero-inflated
models arise in applications where we wish to build multivariate regression models. To be able to
construct multivariate models with as much generality as possible, we specify the full
model as a General Design Bayesian Generalized Linear Mixed Model, as in \citep{zhao06}.
This allows us to incorporate within-subject correlation, measurement error, missing data
and smoothing splines in our models.

% TODO: Lower bound graph
% TODO: Accuracy of approximations
% TODO: Application, physical activity data
% Random intercept, longitudinal data
% Graph demonstrating additional zeroes

% Idea: We can use an approximation of the from q(\beta, \u, \Sigma) q(\rho) \Product q(r_i)
% and use GVA on q(\beta, \u, \Sigma) and mean field updates on \rho and r_i

\subsection{Model}
Let $\mR = \diag{(\vr)}$. Let $\mC = [\mX \mZ], \vnu = [\vbeta^\top \vu^\top]^\top$. Consider the
model

$$
\begin{array}{rl}
\log{p(\vy|\vr, \vbeta, \vu)} &= \vy^\top \mR (\mC\vnu) - \vr^\top \exp{(\mC\vnu)} - \vone^\top \log{\Gamma{(\vy + \vone)}}, \\
\mbox{ and }
r_i &\sim \text{Bernoulli}(\rho) \\
\end{array}
$$

\noindent with priors

$$ 
\begin{array}{rl}
\log{p(\mSigma_{\vu \vu})} &= \text{Inv. Wishart}(\mPsi, v),\\
p(\rho) &\propto 1 \\
\mbox{ and } \vnu|\sigma_\vu^2 &\sim \mbox{N}(\vzero, \sigma_\vu^2). \\
\end{array}
$$

\subsection{Approximation}
Let $\vr_0 = \{ r_i : y_i = 0 \}$.
We assume an approximation of the form
$$
q(\vr_0, \vnu, \sigma_{\vu}^2, \rho) = q(\vnu) q(\mSigma_{\vu \vu}) q(\rho) q(\vr_0) \\
$$

\noindent where $q(\vnu) = \mbox{N}(\vmu, \mLambda)$,
$q(\sigma_{\vu}^2) = \mbox{Inv. Wishart}\left(\mPsi + \sum_{i=1}^m \vmu_i \vmu_i^\top + \mLambda_{\vu_i \vu_i}, v + m + d\right)$ \mbox{and } $q(r_i) = \Bernoulli{(p_i)}$

\noindent with
$$
p_i = \expit\left[ \psi{(\alpha_{q(\rho)})} - \psi{(\beta_{q(\rho)})} - \exp{(c_i^\top\vmu + \half c_i^\top \mLambda c_i)} \right]
$$

\noindent \text{when} $\vy_i = 0$.

% What on Earth is this section doing here? This is very random.
%$\propto \exp{\left \{-r_i \bE_{-r_i} [\exp{(c_i^\top\vnu)}] + r_i [\psi(\alpha_\rho) - \psi(\beta_\rho)] \right \} }.\\$

% Include derivations for mean field updates at the end?

\noindent The optimal approximation for $\vr$ is
$$
\begin{array}{rl}
q(\vr) &\propto \exp \left [ \bE_{-q(\vr)}\vy^\top\mR(\mC\vmu) - \vr^\top\exp{(\mC\vnu)}-\half \vnu^\top \mSigma_{\vu \vu} \vnu \right ] \\
	&= \exp{ \left\{ \vy^\top\mR\mC \vmu - \vr^\top \exp{[\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)]} - \half \vmu^\top \mD \vmu - \half \text{tr}(\mLambda \mD ) \right\} }
\end{array}
$$

\noindent where $\mD =  \left( (\mPsi + \sum_{i=1}^m \vmu_i \vmu_i^\top + \mLambda_{\vu_i\vu_i}) / (v - d - 1) \right)^{-1}$. 

\noindent This is close in form to a Poisson regression model. Poisson regression models
with normal priors have no closed form for their mean field updates due to
non-conjugacy, but can be fit using Gaussian variational approximation
\citep{ormerod09}. The model can be fit using Algorithm 2 below.

\begin{algorithm}\label{alg:algorithm_two}
\label{algorithm2}
\caption[Algorithm 2]{Iterative scheme for obtaining the parameters in the
optimal densities $q^*(\vmu, \mLambda)$, $q^*(\mSigma_{\vu \vu})$ and $q^*(\rho)$}
\begin{algorithmic}
% Fit \vmu, \mLambda using Laplace approximation
\REQUIRE{$\alpha_{q(\rho)} \leftarrow \alpha_\rho + \vone^\top\vp, p_{q(\mSigma_{\vu \vu})} \leftarrow p + 1$}
\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
% \vmu, \mLambda
\STATE Optimise $\vmu$ and $\mLambda$ using $\vy, \mC, \vp$ and
$\mSigma_{\vu \vu}$
% \vp
% \rho is a prior? Not directly observed, except through \vr_i
\STATE $\beta_{q(\rho)} \leftarrow \beta_\rho + n - \vone^\top\vp$
\STATE $\eta \leftarrow -\exp \left [ \mC \vmu + \half \diag{(\mC\mLambda\mC^\top)} \right ] + \psi{(\alpha_{q{(\rho)}})} - \psi{(\beta_{q{(\rho)}})}$
\STATE $\vp_{q(\vr_0)} \leftarrow \expit{(\eta)}$
% \mSigma_{\vu \vu}
\STATE $\mPsi_{q(\mSigma_{\vu \vu})} \leftarrow \Psi + \sum_{i=1}^m \vmu_i \vmu_i^\top + \mLambda_{{\vu}_i}$
\STATE $\mSigma_{\vu\vu} \leftarrow [\mPsi_{q(\mSigma_{\vu \vu})}/(v - d - 1)]^{-1}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Lower bound}
% Where are the priors for \vbeta and \vu
\noindent The lower bound is equal to
$\bE_q[\log{p(\vy, \vtheta)} - \log{q(\vtheta)}] = T_1 + T_2 + T_3$,
where

% This is the new T_1
$$
\begin{array}{rl}
T_1 &= \bE_q[\log{p(\vy, \vnu)} - \log{q(\vnu)}] \\
&= \vy \mP \mC \vmu - \vp^\top \exp{\left[ \mC \vmu + \half \text{diag} (\mC \mLambda \mC^\top) \right]} - \vone^\top\log \Gamma{(\vy + \vone)}\\
& \quad + \frac{p + m}{2} (1 + \log{2 \pi}) + \half \log{|\mLambda|}, \\
T_2 &= \bE_q \left[ \log p(\mSigma_{\vu \vu}) - \log q(\mSigma_{\vu \vu}) \right] \\
&= \bE_q \big[ v/2(\log |\Psi| - \log |\Psi + \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu}|) + \half \log 2 + \half \log|\mSigma_{\vu \vu}| \log \Gamma_{p+1}(v/2) - \log \Gamma_{p}(v/2)\\
&\quad + \half \tr((\vmu_{\vu} \vmu_{\vu}^\top + \mLambda_{\vu \vu}) \mSigma_{\vu \vu}^{-1}) \big] \\
&= v/2\big(\log |\Psi| - \log |\Psi + \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu}|\big) + \half \log 2 + \half \bE_q \log |\mSigma_{\vu \vu}| + \log \Gamma_{p+1}(v/2) - \log \Gamma_{p}(v/2) \\
&\quad + \half \tr\big(\mI_m + \Psi(\Psi+ \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu})^{-1}/(v + p + 2)\big) \\
T_3 &= - \vp^\top \log \vp - (\vone - \vp)^\top \log (\vone - \vp) - \log \Beta (\alpha_\rho, \beta_\rho) + \log \Beta (\alpha_q, \beta_q)
\end{array}
$$

where

$$
\bE_q \big[ \log |\mSigma_{\vu \vu}| \big] = m \log 2 + \log \left | \Psi + \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu} \right | + \sum_{i=1}^m \Psi \left ( \frac{v - i + 1}{2} \right )
$$

$$$$

\section{Algorithms for fitting the variational approximation}
\subsection{Laplace-Variational Approximation}
Laplace's method of approximation relies on a second order Taylor expansion of the
log likelihood around the mode. This can then be optimised with Newton-Raphson
iterations. The algorithm is very quick to execute, but the resulting approximate
posterior distributions are not as accurate as those produced by the other algorithms
considered in this article.

% NR
% Detail the function and its derivatives
\noindent Taylor expanding the variational lower bound once around the mode yields the following function
\begin{align*}
\log \underline{p}(\vmu, \mLambda; \vy) = \vp^\top\vy^\top\mC\vmu - \vp^\top\exp \left (\mC \vmu + \half \mC \mLambda \mC^\top \right ) - \half \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mLambda \mSigma^{-1})}
\end{align*}

\noindent This can be optimised using a Newton-Raphson style algorithm where

\begin{align*}
\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \vmu} &\approx \vp^\top\mC(\vy - \exp{(\mC \vmu)}) - \mSigma^{-1} \vmu \\
\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \mLambda} &\approx -\vp^\top \mC^\top \text{diag}(\exp{(\mC \vmu)}) \mC - \mSigma^{-1}
\end{align*}

\begin{algorithm}\label{alg:algorithm_three}
\caption[Algorithm 3]{Newton-Raphson scheme for optimising $\log \underline{p}(\vmu, \mLambda; \vy)$}
\begin{algorithmic}
% Fit \vmu, \mLambda using Laplace approximation
\WHILE{the increase in $\log \underline{p}(\vmu, \mLambda; \vy)$ is significant}
% \vmu, \mLambda
\STATE $\mLambda \leftarrow \left [\vp^\top \mC^\top \text{diag}(\exp{(\mC \vmu)}) \mC + \mSigma^{-1} \right ]^{-1}$
\STATE $\vmu \leftarrow \vmu + \mLambda \frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \vmu}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Optimising the GVA lower bound}
% Detail techniques used for fitting models.
% TODO: Talk more about numerical scheme. This explanation is terrible!
The Gaussian variational lower bound in Algorithm 2 can be optimised
using a  variety of algorithms. Each of these algorithms is a trade-off between
accuracy,  stability and speed. The variational lower bound is not necessarily unimodal,
leading to potential difficulty in optimising to the global maximum. However, for fixed
$\vp$ and $\mSigma$, the variational lower bound is log-concave, and so standard
optimisation methods such as L-BFGS-B work well. This led to an extremely accurate approximation
of the true posterior at the expense of some additional computational effort.

\subsubsection{GVA, $\mLambda = \mR \mR^\top$}
% TODO: Should talk about the reasons for choosing the various parameterisations
% Ensure positive definiteness of \mLambda

The first variant of the Gaussian Variational Approximation algorithm
optimises the Gaussian variational lower bound of the log likelihood with
respect to $\vmu$ and the Cholesky decomposition $\mR$ of $\mLambda$, that is,
$\mLambda = \mR \mR^\top$. This algorithm trades the computational complexity of
numerically evaluating an integral for greatly increased accuracy in the
approximating posterior distribution. The resulting function is below and  can
be optimised with L-BFGS-B:

% Detail the function and its derivatives
\begin{align*}
\log \underline{p}(\vmu, \mLambda; \vy) &= \vy^\top\mP \mC \vmu - \vp^\top \B(\mC \vmu, \text{diag}(\mC \mLambda \mC^\top)) - \half \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} + \log{|\mR|} \\
&\quad - \frac{d}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \frac{d}{2} \log{(2 \pi)} + \frac{d}{2} \\
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu} &= \mC^\top \vp^\top(\vy - \B^{(1)}(\mC \vmu, \mC \mLambda \mC^\top)) - \mSigma^{-1} \vmu \\
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} &= \left [\mLambda^{-1} - \vp^\top \mC^\top \text{diag}(\B^{(2)}(\mC \vmu, \mC \mLambda \mC^\top)) \mC \vp) - \mSigma^{-1} \right ] \mR
\end{align*}

\subsubsection{GVA, $\mLambda = \left (\mR \mR^\top \right )^{-1}$}
The second variant of the Gaussian Variational Approximation algorithm is similiar 
to the first, but instead of optimising $\vmu$ and $\mR$ where $\mLambda=\mR \mR^\top$,
instead we optimise $\mR$ where $\mLambda = (\mR \mR^\top)^{-1}$.
% The function and derivatives change

\noindent This new parameterisation allows us to calculate $\B(\mC \vmu,
\text{diag}(\mC \mLambda \mC^\top))$ and its derivatives by solving $\mR \va = \mC_{i},
 i=1, \ldots, n$ for $\va$ and then calculating $\va^\top\va$ rather than calculating
$\text{diag}(\mC \mLambda \mC^\top)$ directly.

% Re-order the covariance matrix so that Chevron form is ``reversed''. This makes
% the Cholesky factorisation very simple, and reduces the number of parameters that
% you need to optimise and store.

% More complicated functions and derivatives, but more accurate
% Can be quite fast because of the special form of $\mR$.

% Detail the function and its derivatives under this parameterisation
% Difficulties re: multimodal likelihood, tricks to get that to work
% I think it's log concave for \vp and \mSigma fixed

\noindent The Gaussian variational lower bound is

\begin{align*}
\log \underline{p}(\vmu, \mLambda; \vy) &= \vy\mP\mC \vmu - \vp^\top \B(\mC \vmu, \text{diag}(\mC \mLambda \mC^\top)) - \half \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} \\
&\quad- \frac{d}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \frac{d}{2} \log{(2 \pi)} + \frac{d}{2} - \log{|\mR|}
\end{align*}

\noindent The derivative with respect to $\vmu$ is the same as that in the GVA 
algorithm, but as the parameterisation of $\mLambda$ has changed, the  
derivative with respect to $\mLambda$ is now

\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda}
&= \hphantom{-}(\mLambda^{-1} + \mH)(-\mLambda \mR \mLambda) \\
&= -(\mI + \mH\mLambda)\mR\mLambda \\
&= - (\mR\mLambda + \mH\mLambda\mR\mLambda)
\end{align*} 

\noindent where $\mH = \vp^\top \mC^\top \text{diag}(\B^{(2)}(\mC \vmu, \mC \mLambda \mC^\top)) \mC \vp - \mSigma^{-1}$.


\subsubsection{GVA NR}
% Fixed point update of \mLambda
This variant of the algorithm uses Newton-Raphson-like optimisation on the Gaussian
variational lower bound. This algorithm is fast, but potentially unstable.
% Essentially a very straightforward optimisation approach, but potentially unstable.
% Detail the function and its derivatives
\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu} &= \quad \mC^\top\vp \left [\vy - \B^{(1)}(\mC \vmu, \text{diag}(\mC \mLambda \mC^\top)) \right ] - \mSigma^{-1} \vmu \\
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} &= -\mC^\top \text{diag}(\vp \odot \B^{(2)}(\mC \vmu, \text{diag}(\mC \mLambda \mC^\top))) - \mSigma^{-1}
\end{align*}

\begin{algorithm}\label{alg:algorithm_nr}
\label{algorithm_nr}
\caption[Algorithm GVA NR]{Iterative scheme for obtaining optimal $\vmu$ and $\mLambda$
given $\vy$, $\mC$ and $\vp$}
\begin{algorithmic}
% Fit \vmu, \mLambda using Laplace approximation
\WHILE{the increase in $\log{\underline{p}}(\vmu, \mLambda; \vy)$ is significant}
% \vmu, \mLambda
\STATE $\mLambda \leftarrow \left [ \mC^\top\vp^\top B^{(2)}(\mC \vmu, \text{diag}(\mC \mLambda \mC^\top)) \mC \right ]^{-1}$
\STATE $\vmu \leftarrow \vmu + \mLambda \frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

% Splines

\subsection{Results/Numerical experiments}\label{sec:results}

% Comparison with MCMC
Our approximation algorithms were compared with the posterior distribution estimated
by kernel density estimate from a 1 million MCMC sample produced using Stan.
% It works
% Stability was confirmed over 100 randomly generated data sets.

\noindent The stability of the algorithms was confirmed by running them on 10,000 
different data sets that were randomly generated after having initialised the random 
number generator with different seeds. Median accuracy of the algorithms was assessed
by running them on 100 randomly generated data sets.

% Table of accuracy results - intercept model
\begin{table}
\label{tab:accuracy_int}
\caption{Table of accuracy - Random intercept model}
\begin{tabular}{l|llll}
\hline
& Laplace's Method & GVA $(\mLambda = \mR \mR^\top)$ & GVA2 $(\mLambda = (\mR \mR^\top)^{-1})$ & GVA NR\\
\hline
$\vbeta_1$ & $0.833$ & $0.908$ & $0.908$ & $0.908$ \\ 
$\vbeta_2$ & $0.770$ & $0.987$ & $0.987$ & $0.987$ \\ 
Mean of $\vu$ & $0.807$ & $0.947$ & $0.947$ & $0.947$ \\
$\sigma^2_\vu$ & $???$ & $???$ & $???$ & $???$ \\ 
$\rho$ & $0.976$ & $0.974$ & $0.974$ &  $0.974$ \\ 
\hline
\end{tabular}
\end{table}

\begin{table}
\label{tab:accuracy_slope}
\caption{Table of accuracy - Random slope model}
\begin{tabular}{l|lllll}
\hline
& Laplace's Method & GVA $(\mLambda = \mR \mR^\top)$ & GVA $(\mLambda = (\mR \mR^\top)^{-1})$ & GVA NR\\
\hline
$\vbeta_1$     &0.658&0.879&0.893&0.893&0.879\\
$\vbeta_2$     &0.692&0.883&0.899&0.899&0.883\\
Mean of $\vu$        &0.719&0.908&0.911&0.912&0.908\\
$\sigma^2_\vu$ &???&???&???&???&???\\
$\rho$ &0.911&0.896&0.896&0.896&0.896\\
\hline
\end{tabular}
\end{table}

% Graphs - exactly what sort of graphs do we need?
% Median accuracy
% Increase in lower bound
% MCMC posterior, with approximating posterior for at least one or two of the
% key parameters, such as, say, vbeta[2]

\section{Application}\label{sec:application}

\section{Appendix} 
% TODO: Mean field updates?

\bibliographystyle{elsarticle-harv}
\bibliography{Chapter_1_zero_inflated_models}

\end{document}
