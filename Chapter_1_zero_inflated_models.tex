\documentclass{amsart}
\input{include.tex}
\begin{document}
% First, simplest zero-inflated count model to consider.
Consider $X_1, X_2, \ldots, X_n$ where $X_i = R_i Y_i$, $Y_i \sim \Poisson{\lambda}$ independent of
$R_i \text{indep.} \sim \Bernoulli{\rho}$.

Let $\rho \sim \Unif{0, 1}$, $\lambda \sim \Gamma(a, b)$ and $r_i | \rho \sim \Bernoulli{\rho}$, $1 \leq i \leq n$.

Then the joint likelihood is:

$$
p(\vx, \vr, \lambda, \rho) = \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)}}{\Gamma{a}} \Pi_{i=1}^n \frac{\exp{(-\lambda r_i)} (\lambda r_i)^{x_i}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i}
$$

We seek the full conditionals for $\lambda$ and $\rho$.

First, we calculate the full conditional for $\lambda$.

\begin{align*}
p(\lambda|\vx, \vr, \rho) &= \frac{\Pi_{i=1}^n p(x_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)}{\int \Pi_{i=1}^n p(x_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)) d \lambda} \\
\end{align*}

Concentrating for now on the denominator in this expression, we re-arrange and collect
like terms to obtain
$$
\Pi_{i=1}^n \rho^r_i (1 - \rho)^{1 - r_i} \frac{r_i^{x_i}}{x_i !}
	\int \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)} \lambda^{x_i} \exp{(-\lambda r_i)}}{\Gamma{(a)}} d \lambda
$$

The integral in this expression is
\begin{align*}
& \int \frac{b^a \lambda^{(a + x_i) - 1} \exp{(-\lambda(b + r_i))}}{\Gamma{(a + x_i)}} d \lambda \frac{\Gamma{(a+ x_i)}}{\Gamma{(a)} b^{-x_i}} \\
=& \frac{\Gamma{(a+ x_i)}}{\Gamma{(a)} b^{-x_i}}
\end{align*}

Collecting the multiplicands in the integral over $\lambda$ together, we obtain
$$
\Pi_{i=1}^n \rho^{r_i} (1 - \rho)^{1 - r_i} \frac{r_i^{x_i}}{x_i!}
	\int \frac{b^{na + \sum_{i=1}^n x_i} \lambda^{(na + \sum_{i=1}^n x_i) - 1} \exp{(-\lambda(nb + \sum_{i=1}^n r_i))}}{\Gamma{(na + \sum_{i=1}^n x_i)}} d \lambda
	\frac{\Gamma{(na + \sum_{i=1}^n x_i)}}{\Gamma{(na)} b^{-\sum_{i=1}^n x_i}}
$$

By cancelling like terms in the numerator and denominator of the full likelihood we arrive 
at the functional form of the full conditional, which is proportional to

$$
b^{a+\sum_{i=1}^n x_i} \lambda^{(a + \sum_{i=1}^n x_i) - 1} \exp{(-(b + \sum_{i=1}^n r_i) \lambda)}
$$

which is clearly seen to be a Gamma distribution. Thus
$\lambda | \vx, \vr, \rho \sim \Gamma{(a + \sum_{i=1}^n x_i), b + \sum_{i=1}^n r_i)}$

We now turn our attention to the full conditional distribution for $\rho$. Following the same
technique as above, we first concentrate on the denominator in the full conditional:
\begin{align*}
p(\vx, \vr, \lambda) &= \int \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)}}{\Gamma{a}} \Pi_{i=1}^n \frac{\exp{(-\lambda r_i)} (\lambda r_i)^{x_i}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i} d \rho \\
&= \left(\frac{b^a \lambda^{a - 1} \exp{(-b \lambda)}}{\Gamma{a}} \Pi_{i=1}^n \frac{\exp{(-\lambda r_i)} (\lambda r_i)^{x_i}}{x_i !} \right) \int \rho^{\sum_{i=1}^n r_i} (1 - \rho)^{n - \sum_{i=1}^n r_i} d \rho
\end{align*}

This integral on the right hand side is a Beta integral with the parameters
$a = \sum_{i=1}^n r_i + 1$ and $b = (n - \sum_{i=1}^n r_i) + 1$. All of the terms in the numerator and 
denominator of the full conditional cancel except for
$$
\frac{\rho^{\sum_{i=1}^n r_i} (1 - \rho)^{n - \sum_{i=1}^n r_i}}{\Beta{(\sum_{i=1}^n r_i + 1, n - \sum_{i=1}^n r_i + 1)}}
$$

and thus we see that

$\rho | \vx, \vr, \lambda \sim \Beta{\sum_{i=1}^n r_i + 1, (n - \sum_{i=1}^n r_i) + 1}$.

% Step Two: Assume q(r_i) = Bernoulli(\rho_i), 1 \leq i \leq n for some known \rho_i. Find the
% variational Bayes updates of the q-densities q(\lambda) and q(\rho) corresponding to the
% factorisation
% q(\vr, \lambda, \rho) = q(\lambda) q(\rho) \sum_{i=1}^n q(r_i)

\subsection{Mean field update equations}
Assuming that $q^*(r_i) \sim \Bernoulli(p_i)$,

\begin{align*}
q^*(\lambda) &\propto \lambda^{a+\sum_{i=1}^n x_i - 1} \exp{\{ E_{-q(\lambda)} [-(b + \sum_{i=1}^n r_i) \lambda] \}} \\
&\propto \lambda^{a+\sum_{i=1}^n x_i - 1} \exp{-(b + \sum_{i=1}^n p_i)\lambda} \\
&= \myGamma{a+\sum_{i=1}^n x_i, b + \sum_{i=1}^n p_i}
\end{align*}

and

\begin{align*}
q^*(\rho) &\propto \exp{\{ E_{-q(\rho)} \sum_{i=1}^n r_i \log{(\rho)} + \sum_{i=1}^n (1- r_i) \log{(1 - \rho)} \}} \\
&\propto \exp{\{ \sum_{i=1}^n p_i \log{\rho} + \sum_{i=1}^n (1 - p_i) \log{(1 - \rho)} \}} \\
&\propto \Beta{1+ \sum_{i=1} p_i, n - \sum_{i=1}^n p_i + 1}
\end{align*}

Turning our attention now to $r_i$, we first note that the relevant terms of the
log-likelihood are

$$
-\lambda r_i + x_i \log{r_i} + r_i \log{\frac{\rho}{1 - \rho}}
$$

Taking expectations with respect to $\lambda$ and $\rho$ we have

$$
-r_i \frac{\alpha_\lambda^*}{\beta_\lambda^*} + x_i \log{r_i} + r_i (\Psi(\alpha_\rho^*) - \Psi(\beta_\rho^*))
$$

This log likelihood will be $-\infty$ in the case where $x_i = 0$. Dr John Ormerod hit
upon the idea of side-stepping this problem by writing the q-likelihood as

$$
q_{r_i}^*(r_i) \propto r_i^{x_i} \exp{(r_i \eta_i)}
$$

where $\eta_i = - \frac{\alpha_\lambda^*}{\beta_\lambda^*} + \Psi(\alpha_\rho^*) - \Psi(\beta_\rho^*)$.

Now, either $x_i = 0$ or $x_i \ne 0$.

\begin{align*}
p_i &= \frac{exp{\eta_i}}{\exp{\eta_i} + I(x_i = 0)} \\
&= \text{expit}(\eta_i)
\end{align*}

\subsection{Full conditional for $r_i$}
\begin{align*}
p(r_i | \text{rest}) \propto \frac{(\lambda r_i)^{x_i} \exp{(-\lambda r_i)}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i} \\
\propto r_i^{x_i} (e^{-\lambda})^{r_i} \rho^{r_i} (1 - \rho)^{1 - r_i}
\end{align*}

Once again, we rely upon the fact that if $x_i = 0$, $r_i = 0$, and if $x_i \ne 0$,
$r_i = 1$. So $x_i^{r_i} = I(x \ne 0)$ and hence the likelihood can be re-written as

\begin{align*}
p(r_i | \text{rest}) &= \frac{(e^{-\lambda} \rho)^{r_i}}{I(x_i = 0) + (e^{-\lambda} \rho)^{r_i}} \\
&= \Bernoulli{(\text{expit}(\eta_i))}
\end{align*}
where $\eta_i = e^{-\lambda} \rho$.

\end{document}