\documentclass{article}[12pt]

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\title{Variational approximations to zero-inflated Bayesian models}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\maketitle

Abstract:

Keywords: Approximate Bayesian inference . mixed model . Markov chain Monte Carlo

\section{Introduction}

% First, simplest zero-inflated count model to consider.
\noindent Count data with a large number of zero counts arises in many areas of
application, such as data arising from physical activity studies, 
insurance claims, hospital visits or defects in manufacturing processes.

\noindent While simple forms of these models are easy to fit with maximum likelihood techniques,
more general models incorporating random effects, splines and missing data typically
have no closed form solutions. Fitting these models is typically done with Monte Carlo
Markov Chain techniques, but these can be slow and prone to convergence problems. We
propose to use Variational Bayes to fit close approximations to these models
using a deterministic algorithm which converges much more quickly.

% Cite prior publications in this area

\noindent In this paper, we build upon the earlier work of \cite{lambert1992} and \cite{Ghosh20061360}.

\noindent In Section \ref{sec:methodology} we provide the framework for our approach. In
Section \ref{sec:methodology} we extend our approach to incorporate regression modelling
and random effects. In Section \ref{sec:results} we show how our approach offers 
computational advantages over existing approaches. In Section \ref{sec:application} we 
show an application of our method to physical activity data. Appendices contain details 
of our MCMC samplers.

\subsection{Notation}

The notation $\vx \sim N(\vmu, \mSigma)$ means that $\vx$ has a multivariate normal
density with mean $\vmu$ and covariance $\mSigma$. If $x$ has an inverse gamma
distribution, denoted $x \sim \mbox{IG}(\alpha, \beta)$, then it has density
$p(x) = \beta^\alpha \Gamma(\alpha)^{-1}x^{-\alpha-1} \exp{(-\beta/x)}, x, \alpha,
\beta > 0$.
If $\vx$ is a vector of length $d$ then $\mbox{diag}(\vx)$ is the $d \times d$
diagonal matrix whose diagonal elements are $\vx$.

\section{Methodology}\label{sec:methodology}

In this section we present a VB approach to a Bayesian zero-inflated Poisson model
for count data with extra zeroes. After introducing Bayesian zero-inflated models
and VB methodology we derive the VB factorised approximation to the full Bayesian
model. 

\subsection{Variational Bayesian inference}

\subsection{Variational Bayes for zero-inflated count models}

\noindent Consider the model

$$
y_i = r_i x_i, 1 \leq i \leq n,
$$

\noindent where $x_i \sim \Poisson{(\lambda)}$ independent of $r_i \stackrel{\text{ind.}}{\sim} \Bernoulli{(\rho)}, 1 \leq i \leq n$.

We employ priors $\rho \sim \Unif{(0, 1)}$ and
$\lambda \sim \myGamma{(0.01, 0.01)}$. 
 
% TODO: Add graphical model

  We use a factorised approximation to the full likelihood, as detailed in \citep{ormerod10}.
The use of conjugate priors in the full model yields easier mean field updates in the
variational approximation.

  It can be shown via standard algebraic manipulations that the
full conditionals for $\lambda, \rho$ and $\vr$ are:

$$
\begin{array}{rl}
\lambda | \textbf{rest} &\sim \myGamma{(\alpha_\lambda + \vone^T\vx, \beta_\lambda + \vone^T\vr)}, \\ [0.5ex]
\rho | \textbf{rest} &\sim \Beta{(\alpha_\rho + \vone^T \vr, \beta_\rho + n - \vone^T\vr)} \\ [0.5ex]
\mbox{ and } \quad r_i | \textbf{rest} &\sim \Bernoulli{(\text{expit}(\eta_i))}, \quad 1 \leq i \leq n.
\end{array}
$$

% Step Two: Assume q(r_i) = Bernoulli(\rho_i), 1 \leq i \leq n for some known \rho_i. Find the
% variational Bayes updates of the q-densities q(\lambda) and q(\rho) corresponding to the
% factorisation
% q(\vr, \lambda, \rho) = q(\lambda) q(\rho) \sum_{i=1}^n q(r_i)

\noindent We assume a factorised approximation of the form

$$
q(\lambda, \rho, \vp) = q(\lambda) q(\rho) \left [ \prod_{i=1}^n q(r_i) \right ]
$$

\noindent where $q(\lambda)$ is a Gamma distribution, $q(\rho)$ is a Beta distribution 
and $q(r_i)$ are Bernoulli distributions.

\noindent This leads to the following functional forms of the optimal q-densities

$$
\begin{array}{l}
\mbox{$q^*(\lambda)$ is the $\myGamma{\alpha_{q(\lambda)}, \beta_{q(\lambda)}}$ density function,} \\ [0.5ex]
\mbox{$q^*(\rho)$ is the $\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})$ density function, and} \\ [0.5ex]
\mbox{$q^*(r_i)$  is the $\text{Bernoulli}(p_{q(r_i)})$ density function, $1 \leq i \leq n$,}
\end{array}
$$

%$$
%\begin{array}{c}
%q^*(\lambda) \sim \myGamma(\alpha_{q(\lambda)}, %\beta_{q(\lambda)}),
%q^*(\rho) \sim \text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)}),
%\quad \mbox{ and } \quad  
%q^*(r_i) \sim \text{Bernoulli}(p_{q(r_i)}), \ \ 1 \leq i \leq n,
%\end{array}
%$$

\noindent where the parameters are updated according to Algorithm \ref{algorithm1}. 

\subsection{Lower bound}
The lower bound of the univariate model can be calculated directly to be
$$
\begin{array}{rl}
\bE_q \left\{ \log{p(\vx, \vr, \lambda, \rho)} - \log{q(\vr, \lambda, \rho)} \right\} &= T_1 + T_2 \\
\end{array}
$$

\noindent where
$$
\begin{array}{rl}
T_1 & \ds =
\alpha_\lambda \log{(\beta_\lambda)} + (\alpha_\lambda - 1) [\psi(\alpha_{q(\lambda)}) - \log{(\beta_{q(\lambda)})}] - \beta_\lambda \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} - \log\Gamma(\alpha_\lambda) \\
& \ds \quad -\vp^T\frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + \bE_q[\vx^T \log{(\lambda \vr)}] - \log\Gamma(\vx+1)) \quad \mbox{and} 
\\ [1ex]
T_2 &= - \vp^T \log \vp - (\vone - \vp)^T \log (\vone - \vp) - \log \Beta (\alpha_\rho, \beta_\rho) + \log \Beta (\alpha_q, \beta_q)
\end{array}
$$

\noindent with 
$$
\bE [x_i \log{(\lambda r_i)}]  =
	\begin{cases}
	0 & \textbf{if } x_i = 0 \\
	\bE_q [\log \lambda] = \psi(\alpha_{q(\lambda)}) - \log{(\beta_{q(\lambda)})} & \textbf{if } x_i \ne 0. \\
	\end{cases}
$$

\begin{algorithm} 
\caption[Algorithm 1]{Iterative scheme for obtaining the parameters in the
optimal densities $q^*(\lambda)$ and $q^*(\rho)$}
\begin{algorithmic}
\REQUIRE{$\alpha_{q(\rho)} \leftarrow \alpha_\rho + \vone^T\vp, 
\alpha_{q(\lambda)} \leftarrow \alpha_\lambda + \vone^T\vx$}
\WHILE{the increase in $\log{\underline{p}}(\vx;q)$ is significant}
\STATE $\beta_{q(\rho)} \leftarrow \beta_\rho + n - \vone^T\vp$
\STATE $\eta \leftarrow -\alpha_{q(\lambda)}/\beta_{q(\lambda)} + \psi{(a_{q{(\rho)}})} - \psi{(b_{q{(\rho)}})}$
\STATE $\vp_{q(\vr_0)} \leftarrow \expit{(\eta)}$
\STATE $\beta_{q(\lambda)} \leftarrow \beta_\lambda + \vone^T\vp$
\ENDWHILE
\end{algorithmic}
\label{algorithm1}
\end{algorithm}

%By taking the expectation of each full conditional with respect to 

% This should be made the numerical experiment section.
\subsection{Results}
A data set of 10,000 points were simulated from the univariate ZIP model with
$\lambda = 1$ and $\rho = 0.5$. The vague gamma prior $\alpha_\lambda = 0.01$ and $\beta_\lambda = 0.01$ was chosen for $\lambda$.

\section{Extending the zero-inflated Poisson model to a regression model}
\noindent The above univariate model demonstrates that variational approximations are well-suited
to accelerating the fit of Bayesian zero-inflated models to data. Typically zero-inflated
models arise in applications where we wish to build multivariate regression models. To be able to
construct multivariate models with as much generality as possible, we specify the full
model as a General Design Bayesian Generalized Linear Mixed Model, as in \citep{zhao06}.
This allows us to incorporate within-subject correlation, measurement error, missing data
and smoothing splines in our models.

% TODO: Lower bound graph
% TODO: Accuracy of approximations
% TODO: Application, physical activity data
% Random intercept, longitudinal data
% Graph demonstrating additional zeroes

% Idea: We can use an approximation of the from q(\beta, \u, \Sigma) q(\rho) \Product q(r_i)
% and use GVA on q(\beta, \u, \Sigma) and mean field updates on \rho and r_i

\subsection{Model}
Let $\mR = \diag{(\vr)}$. Let $\mC = [\mX \mZ], \vnu = [\vbeta^T \vu^T]^T$. Consider the
model

$$
\begin{array}{rl}
\log{p(\vy|\vr, \vbeta, \vu)} &= \vy^T \mR (\mC\vnu) - \vr^T \exp{(\mC\vnu)} - \vone^T \log{\Gamma{(\vy + \vone)}}, \\
\mbox{ and }
r_i &\sim \text{Bernoulli}(\rho) \\
\end{array}
$$

\noindent with priors

$$ 
\begin{array}{rl}
\log{p(\mSigma_\vu^2)} &= v/2 \log |\Psi| - \log \Gamma_p (v/2) - \frac{v+p+1}{2} \log | \mSigma_{\vu \vu}| - \half \tr (\Psi \mSigma^{-1}_{\vu \vu}),\\
p(\rho) &\propto 1 \\
\mbox{ and } \vnu|\sigma_\vu^2 &\sim \mbox{N}(0, \sigma_\vu^2). \\
\end{array}
$$

\subsection{Approximation}
Let $\vr_0 = \{ r_i : y_i = 0 \}$.
We assume an approximation of the form
$$
q(\vr_0, \vnu, \sigma_{\vu}^2, \rho) = q(\vnu) q(\mSigma_{\vu \vu}) q(\rho) q(\vr_0) \\
$$

\noindent where $q(\vnu) = \mbox{N}(\vmu, \mLambda)$, $q(\sigma_{\vu}^2) = \mbox{Inv. Wishart}\left(v, p+1, \Psi + \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu}\right)$ \mbox{and } $q(r_i) = \Bernoulli{(p_i)}$

\noindent with
$$
p_i = \expit\left[ \psi{(\alpha_{q(\rho)})} - \psi{(\beta_{q(\rho)})} - \exp{(c_i^T\vmu + \half c_i^T \mLambda c_i)} \right]
$$

\noindent \text{when} $\vy_i = 0$.

% What on Earth is this section doing here? This is very random.
%$\propto \exp{\left \{-r_i \bE_{-r_i} [\exp{(c_i^T\vnu)}] + r_i [\psi(\alpha_\rho) - \psi(\beta_\rho)] \right \} }.\\$

% Include derivations for mean field updates at the end?

\noindent The optimal approximation for $\vr$ is
$$
\begin{array}{rl}
q(\vr) &\propto \exp \left [ \bE_{-q(\vr)}y^T\mR(\mC\vmu) - \vr^T\exp{(\mC\vnu)}-\half \vnu^T \mSigma_{\vu \vu} \vnu \right ] \\
&= \exp{\left [\vy^T\mR\mC \vmu - \vp^T \exp{\{\mC \vmu + \half \text{diag}(\mC \mLambda \mC^T)\}} - \half \vmu^T \mD \vmu - \half \text{tr}(\mLambda \mD ) \right ]}
\end{array}
$$

\noindent where $\mD = \left( \Psi + \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu} \right)/(v - p)$. 

\noindent This is close in form to a Poisson regression model. Poisson regression models
with normal priors have no closed form for their mean field updates due to
non-conjugacy, but can be fit using Gaussian variational approximation
\citep{ormerod09}. The model can be fit using Algorithm 2 below.

\begin{algorithm}\label{alg:algorithm_two}
\label{algorithm2}
\caption[Algorithm 2]{Iterative scheme for obtaining the parameters in the
optimal densities $q^*(\vmu, \mLambda)$, $q^*(\mSigma_{\vu \vu})$ and $q^*(\rho)$}
\begin{algorithmic}
% Fit \vmu, \mLambda using Laplace approximation
\REQUIRE{$\alpha_{q(\rho)} \leftarrow \alpha_\rho + \vone^T\vp, p_{q(\mSigma_{\vu \vu})} \leftarrow p + 1$}
\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
% \vmu, \mLambda
\STATE Optimise $\vmu$ and $\mLambda$ using $\vy, \mX, \mZ, \rho, \vp$ and
$\mSigma_{\vu \vu}$
% \vp
% \rho is a prior? Not directly observed, except through \vr_i
\STATE $\beta_{q(\rho)} \leftarrow \beta_\rho + n - \vone^T\vp$
\STATE $\eta \leftarrow -\exp \left [ \mC \vmu + \half \diag{(\mC\mLambda\mC^T)} \right ] + \psi{(a_{q{(\rho)}})} - \psi{(b_{q{(\rho)}})}$
\STATE $\vp_{q(\vr_0)} \leftarrow \expit{(\eta)}$
% \mSigma_{\vu \vu}
\STATE $\Psi_{q(\mSigma_{\vu \vu})} \leftarrow \Psi + \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Lower bound}
% Where are the priors for \vbeta and \vu
\noindent The lower bound is equal to
$\bE_q[\log{p(\vy, \vtheta)} - \log{q(\vtheta)}] = T_1 + T_2 + T_3$,
where

% This is the new T_1
$$
\begin{array}{rl}
T_1 &= \bE_q[\log{p(\vy, \vnu)} - \log{q(\vnu)}] \\
&= \quad \vy^T \mP \mC \vmu - \vp^T \exp{\left [\mC \vmu + \half \text{diag} (\mC \mLambda \mC^T)\right ]} - \vone^T\log \Gamma{(\vy + \vone)}\\
& \quad + \frac{p + m}{2} (1 + \log{2 \pi}) + \half \log{|\mLambda|},
\end{array}
$$

%\noindent where $A_q = A + m/2$ and $B_q = B + \|\vmu_\vu\|^2/2 +  \tr{(\mLambda_{\vu \vu})}/2$.

$$
\begin{array}{rl}
T_2 &= \bE_q [\log p(\mSigma_{\vu \vu}) - \log q(\mSigma_{\vu \vu})] \\
&= \bE_q [v/2(\log |\Psi| - \log |\Psi + \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu}|) + \half \log 2 + \half \log|\mSigma_{\vu \vu}| \log \Gamma_{p+1}(v/2) - \log \Gamma_{p}(v/2)\\
&\quad + \half \tr((\vmu_{\vu} \vmu_{\vu}^T + \mLambda_{\vu \vu}) \mSigma_{\vu \vu}^{-1}) ] \\
&= \quad v/2(\log |\Psi| - \log |\Psi + \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu}|) + \half \log 2 + \half \bE_q \log |\mSigma_{\vu \vu}| + \log \Gamma_{p+1}(v/2) - \log \Gamma_{p}(v/2) \\
&\quad + \half \tr(\mI_m + \Psi(\Psi+ \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu})^{-1}/(v + p + 2))
\end{array}
$$

with

$$
\bE_q [\log |\mSigma_{\vu \vu}|] = m \log 2 + \log \left | \Psi + \vmu_\vu \vmu_\vu^T + \mLambda_{\vu \vu} \right | + \sum_{i=1}^m \Psi \left ( \frac{v - i + 1}{2} \right )
$$

$$T_3 = - \vp^T \log \vp - (\vone - \vp)^T \log (\vone - \vp) - \log \Beta (\alpha_\rho, \beta_\rho) + \log \Beta (\alpha_q, \beta_q)$$

\subsection{Laplace-Variational Approximation}
Laplace's method of approximation relies on a second order Taylor expansion of the
log likelihood around the mode. This can then be optimised with Newton-Raphson
iterations. The algorithm is very quick to execute, but the resulting approximate
posterior distributions are not as accurate as those produced by the other algorithms
considered in this article.

% NR
% Detail the function and its derivatives
\noindent Taylor expanding the variational lower bound once around the mode yields the following function
\begin{align*}
\log \underline{p}(\vmu, \mLambda; \vy) = \vp^T\vy^T\mC\vmu - \vp^T\exp \left (\mC \vmu + \half \mC \mLambda \mC^T \right ) - \half \vmu^T \mSigma^{-1} \vmu - \half \tr{(\mLambda \mSigma^{-1})}
\end{align*}

\noindent This can be optimised using a Newton-Raphson style algorithm where

\begin{align*}
\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \vmu} &\approx \vp^T\mC(\vy - \exp{(\mC \vmu)}) - \mSigma^{-1} \vmu \\
\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \mLambda} &\approx -\vp^T \mC^T \text{diag}(\exp{(\mC \vmu)}) \mC - \mSigma^{-1}
\end{align*}

\begin{algorithm}\label{alg:algorithm_three}
\caption[Algorithm 3]{Newton-Raphson scheme for optimising $\log \underline{p}(\vmu, \mLambda; \vy)$}
\begin{algorithmic}
% Fit \vmu, \mLambda using Laplace approximation
\WHILE{the increase in $\log \underline{p}(\vmu, \mLambda; \vy)$ is significant}
% \vmu, \mLambda
\STATE $\mLambda \leftarrow \left [\vp^T \mC^T \text{diag}(\exp{(\mC \vmu)}) \mC + \mSigma^{-1} \right ]^{-1}$
\STATE $\vmu \leftarrow \vmu + \mLambda \frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \vmu}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Optimising the GVA lower bound}
% Detail techniques used for fitting models.
The Gaussian variational lower bound in Algorithm 2 can be optimised using a 
variety of algorithms. Each of these algorithms is a trade-off between accuracy, 
stability and speed.

\subsubsection{GVA}
% More complicated functions and derivatives, but more accurate
The first variant of the Gaussian Variational Approximation algorithm
optimises the Gaussian variational lower bound of the log likelihood with respect
to $\vmu$ and the Cholesky decomposition $\mR$ of $\mLambda$, that is,
$\mLambda = \mR \mR^T$. This algorithm trades the computational complexity of 
numerically evaluating the integral in Equation \ref{eq:gva} for greatly increased 
accuracy in the approximating posterior distribution. The resulting function is below and 
can be optimised with L-BFGS:

% Detail the function and its derivatives
\begin{align}\label{eq:gva}
\log \underline{p}(\vmu, \mLambda; \vy) &= \vp^T\vy^T\mC \vmu - \vp^T \B(\mC \vmu, \text{diag}(\mC \mLambda \mC^T)) - \half \vmu \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} \\
&\quad - \frac{d}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \frac{d}{2} \log{(2 \pi)} + \frac{d}{2} + \log{|\mR|}
\end{align}

\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu} &= \mC^T \vp^T(\vy - \B(\mC \vmu, \mC \mLambda \mC^T)) - \mSigma^{-1}
\end{align*}

\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} &= \left [\mLambda^{-1} - \vp^T \mC^T \text{diag}(\B(\mC \vmu, \mC \mLambda \mC^T)) \mC \vp) - \mSigma^{-1} \right ] \mR
\end{align*} 
 
\subsubsection{GVA2}
The second variant of the Gaussian Variational Approximation algorithm is similiar 
to the first, but instead of optimising $\vmu$ and $\mR$ where $\mLambda=\mR \mR^T$,
instead we optimise $\mR$ where $\mLambda = (\mR \mR^T)^{-1}$.
% The function and derivatives change

\noindent By re-ordering the columns and rows of $\mR$ so that random effects
precede fixed effects, the assumption of independence between the components of
$\vu$ ensures sparsity of $\mR$. This allows optimisation of the Gaussian 
variational lower bound to be done over fewer parameters, and for inverses of $\mR$ 
to be calculated more quickly - a reduction from $O((m + p)^3)$ to
$O(m + (pm)^2 + p^3)$, which is a substantial reduction in computation if $m$ is 
substantially greater than $p$, as is typical in applications.

% More complicated functions and derivatives, but more accurate
% Can be quite fast because of the special form of $\mR$.

% Detail the function and its derivatives under this parameterisation
% Difficulties re: multimodal likelihood, tricks to get that to work
\noindent The variational lower bound is not necessarily unimodal, leading to 
potential difficulty in optimising to the global maximum. This problem was 
sidestepped by optimising the parameter estimates using Laplace's method first to 
get a rough estimate, then using the GVA2 algorithm to further refine that estimate. 
This led to an extremely accurate approximation of the true posterior at the expense 
of some additional computational effort.
% Re-order the covariance matrix so that Chevron form is ``reversed''. This makes
% the Cholesky factorisation very simple, and reduces the number of parameters that
% you need to optimise and store.

\noindent The Gaussian variational lower bound is

\begin{align*}
\log \underline{p}(\vmu, \mLambda; \vy) &= \vp^T\vy^T\mC \vmu - \vp^T \B(\mC \vmu, \text{diag}(\mC \mLambda \mC^T)) - \half \vmu \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} \\
&\quad- \frac{d}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \frac{d}{2} \log{(2 \pi)} + \frac{d}{2} - \log{|\mR|}
\end{align*}

\noindent The derivative with respect to $\vmu$ is the same as that in the GVA 
algorithm, but as the parameterisation of $\mLambda$ has changed, the  
derivative with respect to $\mLambda$ is now

\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} &= -\mR^{-1}\left [\mLambda^{-1} - \mH \right ]\mLambda \\
&= -\mR^{-1} (\mI + \mH \mLambda)
\end{align*} 

\noindent where $\mH = \vp^T \mC^T \text{diag}(\B^{(2)}(\mC \vmu, \mC \mLambda \mC^T)) \mC \vp - \mSigma^{-1}$.


\subsubsection{GVA NR}
% Fixed point update of \mLambda
This variant of the algorithm uses Newton-Raphson-like optimisation on the Gaussian
variational lower bound. This algorithm is fast, but potentially unstable.
% Essentially a very straightforward optimisation approach, but potentially unstable.
% Detail the function and its derivatives
\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu} &= \mC^T\vp \left [\vy - \B^{1}(\mC \vmu, \text{diag}(\mC \mLambda \mC^T)) \right ] - \mSigma^{-1} \vmu
\end{align*}

\begin{align*}
\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} &= -\mC^T \text{diag}(\vp \odot \B^{(2)}(\mC \vmu, \text{diag}(\mC \mLambda \mC^T))) - \mSigma^{-1}
\end{align*}

\begin{algorithm}\label{alg:algorithm_nr}
\label{algorithm_nr}
\caption[Algorithm GVA NR]{Iterative scheme for obtaining optimal $\vmu$ and $\mLambda$
given $\vy$, $\mC$ and $\vp$}
\begin{algorithmic}
% Fit \vmu, \mLambda using Laplace approximation
\WHILE{the increase in $\log{\underline{p}}(\vmu, \mLambda; \vy)$ is significant}
% \vmu, \mLambda
\STATE $\mLambda \leftarrow \left [ \mC^T\vp^T B^{(2)}(\mC \vmu, \text{diag}(\mC \mLambda \mC^T)) \mC \right ]^{-1}$
\STATE $\vmu \leftarrow \vmu + \mLambda \frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

% Splines

\subsection{Results/Numerical experiments}\label{sec:results}

% Comparison with MCMC
Our approximation algorithms were compared with the posterior distribution estimated
by kernel density estimate from a 1 million MCMC sample produced using Stan.
% It works
% Stability was confirmed over 100 randomly generated data sets.

\noindent The stability of the algorithms was confirmed by running them on 10,000 
different data sets that were randomly generated after having initialised the random 
number generator with different seeds.

% Table of accuracy results - intercept model
\begin{table}
\label{tab:accuracy_int}
\caption{Table of accuracy - Random intercept model}
\begin{tabular}{l|lllll}
\hline
& Laplace's Method & GVA $(\mLambda = \mR \mR^T)$ & GVA2 $(\mLambda = (\mR \mR^T)^{-1})$ & GVA2 new & GVA NR\\
\hline
$\vbeta_1$ & $0.955$ & $0.961$ & $0.965$ & $0.956$ & $0.961$ \\ 
$\vbeta_2$ & $0.790$ & $0.878$ & $0.857$ & $0.877$ & $0.878$ \\ 
$\vu_1$ & $0.954$ & $0.966$ & $0.966$ & $0.955$ & $0.966$ \\ 
$\vu_2$ & $0.946$ & $0.967$ & $0.948$ & $0.959$ & $0.967$ \\ 
$\vu_3$ & $0.955$ & $0.964$ & $0.950$ & $0.953$ & $0.964$ \\ 
$\vu_4$ & $0.954$ & $0.966$ & $0.970$ & $0.954$ & $0.966$ \\ 
$\vu_5$ & $0.952$ & $0.966$ & $0.975$ & $0.956$ & $0.966$ \\ 
$\vu_6$ & $0.953$ & $0.966$ & $0.976$ & $0.956$ & $0.966$ \\ 
$\vu_7$ & $0.950$ & $0.966$ & $0.977$ & $0.958$ & $0.966$ \\ 
$\vu_8$ & $0.951$ & $0.965$ & $0.976$ & $0.955$ & $0.965$ \\ 
$\vu_9$ & $0.938$ & $0.960$ & $0.894$ & $0.952$ & $0.960$ \\ 
$\vu_10$ & $0.952$ & $0.967$ & $0.950$ & $0.958$ & $0.967$ \\ 
$\sigma^2_\vu$ & $0.961$ & $0.963$ & $0.962$ & $0.962$ & $0.963$ \\ 
$\rho$ & $0.893$ & $0.893$ & $0.891$ & $0.893$ & $0.893$ \\ 
\hline
\end{tabular}
\end{table}

\begin{table}
\label{tab:accuracy_slope}
\caption{Table of accuracy - Random slope model}
\begin{tabular}{l|llllll}
\hline
& Laplace's Method & GVA $(\mLambda = \mR \mR^T)$ & GVA2 $(\mLambda = (\mR \mR^T)^{-1})$ & GVA2 fast alg. & GVA NR\\
\hline
$\vbeta_1$     &0.851&0.976&0.758&0.950&0.976\\
$\vbeta_2$     &0.748&0.959&0.951&0.957&0.959\\
$\vu_1$        &0.869&0.991&0.952&0.966&0.991\\
$\vu_2$        &0.822&0.993&0.957&0.960&0.993\\
$\vu_3$        &0.691&0.985&0.971&0.751&0.985\\
$\vu_4$        &0.699&0.984&0.947&0.722&0.984\\
$\vu_5$        &0.782&0.951&0.954&0.958&0.951\\
$\vu_6$        &0.836&0.983&0.974&0.951&0.983\\
$\vu_7$        &0.909&0.994&0.933&0.984&0.994\\
$\vu_8$        &0.861&0.993&0.987&0.962&0.993\\
$\vu_9$        &0.754&0.991&0.948&0.936&0.991\\
$\vu_{10}$     &0.627&0.992&0.954&0.905&0.992\\
$\vu_{11}$     &0.745&0.981&0.964&0.865&0.981\\
$\vu_{12}$     &0.802&0.984&0.988&0.834&0.984\\
$\vu_{13}$     &0.863&0.991&0.932&0.954&0.991\\
$\vu_{14}$     &0.892&0.993&0.991&0.957&0.993\\
$\vu_{15}$     &0.877&0.98&0.957&0.943&0.98\\
$\vu_{16}$     &0.903&0.987&0.972&0.935&0.987\\
$\vu_{17}$     &0.819&0.979&0.955&0.931&0.979\\
$\vu_{18}$     &0.896&0.989&0.975&0.937&0.989\\
$\vu_{19}$     &0.741&0.985&0.951&0.929&0.985\\
$\vu_{20}$     &0.704&0.99&0.979&0.901&0.99\\
$\vu_{21}$     &0.823&0.978&0.958&0.980&0.978\\
$\vu_{22}$     &0.871&0.988&0.989&0.967&0.988\\
$\vu_{23}$     &0.822&0.979&0.95&0.971&0.979\\
$\vu_{24}$     &0.82&0.989&0.981&0.961&0.989\\
$\vu_{25}$     &0.812&0.981&0.935&0.969&0.981\\
$\vu_{26}$     &0.789&0.989&0.963&0.967&0.989\\
$\vu_{27}$     &0.782&0.991&0.925&0.971&0.991\\
$\vu_{28}$     &0.833&0.992&0.988&0.982&0.992\\
$\vu_{29}$     &0.748&0.985&0.93&0.972&0.985\\
$\vu_{30}$     &0.723&0.991&0.956&0.958&0.991\\
$\vu_{31}$     &0.749&0.989&0.934&0.970&0.989\\
$\vu_{32}$     &0.802&0.994&0.971&0.974&0.994\\
$\vu_{33}$     &0.897&0.993&0.925&0.974&0.993\\
$\vu_{34}$     &0.922&0.996&0.985&0.981&0.996\\
$\vu_{35}$     &0.834&0.964&0.917&0.950&0.964\\
$\vu_{36}$     &0.873&0.982&0.982&0.958&0.982\\
$\vu_{37}$     &0.813&0.993&0.932&0.977&0.993\\
$\vu_{38}$     &0.777&0.994&0.983&0.977&0.994\\
$\vu_{39}$     &0.867&0.988&0.888&0.948&0.988\\
$\vu_{40}$     &0.837&0.981&0.977&0.948&0.981\\
$\sigma^2_\vu$ &0.518&0.831&0.822&0.784&0.831\\
$\rho$ &0.988&0.981&0.982&0.983&0.981\\
\hline
\end{tabular}
\end{table}

% Graphs - exactly what sort of graphs do we need?
% Increase in lower bound
% MCMC posterior, with approximating posterior for at least one or two of the
% key parameters, such as, say, vbeta[2]


\section{Application}\label{sec:application}

% This section can probably be dropped.
\section{Theory of variational approximation of zero-inflated Poisson models}
Let $Z_i = R_i Y_i$.

\noindent Then the probability that $Z_i = 0$ is
$$
\begin{array}{rl}
P(Z_i = 0) &= P(R_i = 0)P(Y_i = y) + P(R_i = 1) P(Y_i = 0) - P(R_i = 0) P(Y_i = 0) \\
&= (1 - \rho) + \rho e^{-\lambda} - (1 - \rho) e^{-\lambda} \\
&= (1 - \rho) + e^{-\lambda}(2 \rho - 1)
\end{array}
$$

\section{Appendix} 

\subsection{Mean field update equations}
We are now in a position to calculate the mean field update equations for the factorised
variational approximation. Assuming that $q(r_i) \sim \Bernoulli{(p_i)}$ then

% Mean field update for q(\lambda)
$$
\begin{array}{rl}
q^*(\lambda)
    & \propto 
    \lambda^{\alpha_\lambda+\vone^T\vx - 1} 
    \exp\left\{ 
    \bE_{-q(\lambda)} \left[
    -(\beta_\lambda + \vone^T\vr) \lambda 
    \right] 
    \right\} 
    \\ [0.5ex]
    &
    \propto \lambda^{\alpha_\lambda+\vone^T\vx - 1} \exp{\left \{-(\beta_\lambda + \vone^T\vp)\lambda \right \} } 
\\
    & = \myGamma{(\alpha_\lambda+\vone^T\vx, \beta_\lambda+\vone^T\vp)},
\end{array}
$$

% Mean field update for q(\rho)
$$
\begin{array}{rl}
\log{q^*(\rho)} 
    &
    \propto \left\{ 
    \bE_{-q(\rho)}\left[ 
    \vone^T\vr \log{(\rho)} 
    + (n - \vone^T\vr) \log{(1 - \rho)} 
    \right] 
    + \alpha_\rho \log{(\rho)} 
    + \beta_\rho \log{(1 - \rho)} 
    \right\} 
    \\ [0.5ex]
    &
    \propto \exp \left( 
    (\vone^T\vp + \alpha_\rho) \log{(\rho)} 
    + (n - \vone^T\vp + \beta_\rho) \log{(1 - \rho)} 
    \right) 
    \\ [0.5ex]
    &= \Beta(\alpha_\rho + \vone^T\vp, \beta_\rho + n - \vone^T\vp),
\end{array}
$$

\noindent and

$$
\begin{array}{rl}
\ds \log{q^*(r_i)} &\propto -\bE_{q(\lambda)} [\lambda ] r_i + x_i \log{(r_i)} + r_i \bE_{q(\rho)} \left[\log{\left(\frac{\rho}{1 - \rho}\right)}\right]\\[0.5ex]
& \ds = -r_i \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + x_i \log{(r_i)} + r_i \left(\psi(\alpha_{q(\rho)}) - \psi(\beta_{q(\rho)})\right)\\ [0.5ex]%
& \ds = \text{Bernoulli}(p_i)
\end{array}
$$

\noindent where

$$
\begin{array}{rl}
\ds p_i 
     = \frac{\exp{(\eta_i)}}{I(x_i = 0) + \exp{(\eta_i)}}  
     = \text{expit}(\eta_i), \quad \mbox{(when $x_i = 0$)} 
\end{array}
$$

\noindent and $\eta_i = - \alpha_{q(\lambda)}/\beta_{q(\lambda)} + \psi(\alpha_{q(\rho)}) - \psi(\beta_{q(\rho)})$.

\noindent The optimal approximation for $\vr$ is
$$
\begin{array}{rl}
q(\vr) &\propto \exp{\{\bE_{-q(\vr)}y^T\mR(\mC\vmu) - \vr^T\exp{(\mC\vnu)}-\half \vnu^T \text{diag}(\sigma_{\vu}^2)^{-1} \vnu\}}
\end{array}
$$

$$
\begin{array}{rl}
&\bE_{-q(\vr)} [\vy^T\mR(C\vnu) - \vr^T\exp{(\mC\vnu)}-\half \vnu^T \text{diag}(\sigma_{\vu}^2)^{-1} \vnu]\\
=&\vy^T\mR\mC \vmu - \vp^T \exp{\{\mC \vmu + \half \text{diag}(\mC \mLambda \mC^T)\}} - \half \vmu^T \hat{\mD} \vmu - \half \text{tr}(\mLambda \hat{\mD} ))
\end{array}
$$

\subsection{Lower bound}
The lower bound $\log{\underline{p}(\vtheta)}$ is
$$
	\bE_q[\log{p(\vx, \vtheta)} - \log{q(\vtheta)}]
$$

\noindent where $q(\vtheta) = q(\lambda) q(\rho) \prod_{i=1}^n q(r_i)$,
$q(\lambda) \sim \text{Gamma}{(\alpha_{q(\lambda)}, \beta_{q(\lambda)})}$,
$q(\rho) \sim \text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})$ and
$q(r_i) = 1$ if $x_i \ne 0$, and $p_i$ if $x_i = 0$, where $p_i$ is
calculated for each iteration as specified in Algorithm \ref{algorithm1}.

The lower bound can be calculated directly to be
$$
\begin{array}{rl}
\bE_q \left\{ \log{p(\vx, \vr, \lambda, \rho)} - \log{q(\vr, \lambda, \rho)} \right\} &= T_1 + T_2 \\
\end{array}
$$

\noindent where
$$
\begin{array}{rl}
T_1 & \ds =
\alpha_\lambda \log{(\beta_\lambda)} + (\alpha_\lambda - 1) \bE_q[\log{(\lambda)}] - \beta_\lambda \bE_q [\lambda] - \log\Gamma(\alpha_\lambda) \\
& \ds \quad + \sum_{i=1}^n ( -\bE_q [\lambda] \bE_q [r_i] + \bE_q[x_i \log{(\lambda r_i)}] - \log\Gamma(x_i+1)) \quad \mbox{and}
\\ [1ex]
T_2 &=\bE_q[r_i] \bE_q[\log{(\rho)}] + \bE_q[(1 - r_i)] \bE_q[\log{(1 - \rho)}] 
- \bE_q[\log{q(r)}] 
- \bE_q[\log{q(\lambda)}] 
- \bE_q[ \log{q(\rho)}]
\end{array}
$$

\noindent with 
$\bE[\lambda] = \alpha_{q(\lambda)}/\beta_{q(\lambda)}$,
$\bE [\lambda] = \alpha_{q(\lambda)}/\beta_{q(\lambda)}$,
% T_1 terms
$$
\begin{array}{rl}
\bE [\log{\lambda}] & \ds = -\{ \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma(\alpha_{q(\lambda)})} + (1 - \alpha_{q(\lambda)}) \psi{(\alpha_{q(\lambda)})} \}, \\
 \\
\bE[\log{\rho}] & \ds = - \{ \log{\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\psi{(\beta_{q(\rho)})} \\
    & \ds \qquad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \},
\\
\bE[r_i] & \ds = 
	\begin{cases}
	1 & \text{if } x_i \ne 0 \\
	p_i & \text{if } x_i = 0, \\
	\end{cases}
\\
-\bE_q[\log{q(r)}] & \ds = \sum_{i=1}^n I(x_i = 0) \log{(p_i)} \\ [0.5ex]
-\bE_q[\log{q(\lambda)}] 
    & \ds = \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma{(\alpha_{q(\lambda)})}} + (1 - \alpha_{q(\lambda)}) \psi{(\alpha_{q(\lambda)})}, \\ [0.5ex]
 \mbox{and}\quad \bE_q[\log{q(\rho)}] 
    & \ds = - \{ \log{(\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\psi{(\beta_{q(\rho)})}  \\ [0.5ex]
& \ds \quad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \}. \\ [0.5ex]
\end{array}
$$

\section{Appendix - Algebraic derivations of conditional likelihoods}

The joint likelihood is:

$$
p(\vx, \vr, \lambda, \rho) = \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)}}{\Gamma{(a)}} \prod_{i=1}^n \frac{\exp{(-\lambda r_i)} (\lambda r_i)^{x_i}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i}.
$$

%\begin{align*}
%p(\lambda|\vx, \vr, \rho) &= \frac{\prod_{i=1}^n p(x_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)}{\int \prod_{i=1}^n p(x_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)) d \lambda}.
%\end{align*}
%
%Concentrating for now on the denominator in this expression, we re-arrange and collect
%like terms to obtain
%$$
%\prod_{i=1}^n \rho^r_i (1 - \rho)^{1 - r_i} \frac{r_i^{x_i}}{x_i !}
%	\int \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)} \lambda^{x_i} \exp{(-\lambda r_i)}}{\Gamma{(a)}} d \lambda
%$$
%
%The integral in this expression is
%\begin{align*}
%& \int \frac{b^a \lambda^{(a + x_i) - 1} \exp{(-\lambda(b + r_i))}}{\Gamma{(a + x_i)}} d \lambda \frac{\Gamma{(a+ x_i)}}{\Gamma{(a)} b^{-x_i}} \\
%=& \frac{\Gamma{(a+ x_i)}}{\Gamma{(a)} b^{-x_i}}.
%\end{align*}
%
%Collecting the multiplicands in the integral over $\lambda$ together, we obtain
%$$
%\prod_{i=1}^n \rho^{r_i} (1 - \rho)^{1 - r_i} \frac{r_i^{x_i}}{x_i!}
%	\int \frac{b^{na + \sum_{i=1}^n x_i} \lambda^{(na + \sum_{i=1}^n x_i) - 1} \exp{(-\lambda(nb + \sum_{i=1}^n r_i))}}{\Gamma{(na + \sum_{i=1}^n x_i)}} d \lambda
%	\frac{\Gamma{(na + \sum_{i=1}^n x_i)}}{\Gamma{(na)} b^{-\sum_{i=1}^n x_i}}.
%$$

%By cancelling like terms in the numerator and denominator of the full likelihood we arrive 
%at

$$
\beta_{\lambda}^{\alpha_\lambda+\vone^T\vx} \lambda^{(\alpha_\lambda + \vone^T\vx) - 1} \exp{\left(-(\beta_\lambda + \vone^T\vr) \lambda \right)}
$$

$$
\frac{\rho^{\vone^T\vr} (1 - \rho)^{\vone^T(\vone - \vr)}}{\Beta{\alpha_\rho + \vone^T \vr, \beta_\rho + \vone^T(\vone - \vr)}}
$$

$$
\begin{array}{ll}
p(r_i | \text{rest}) & \ds \propto \frac{(\lambda r_i)^{x_i} \exp{(-\lambda r_i)}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i} \\
& \ds \propto r_i^{x_i} (e^{-\lambda})^{r_i} \rho^{r_i} (1 - \rho)^{1 - r_i}
\end{array}
$$

We make use of the fact that if $x_i = 0$, $r_i = 0$, and if $x_i \ne 0$,
$r_i = 1$. So $x_i^{r_i} = I(x \ne 0)$ and hence the likelihood can be re-written as

$$
\begin{array}{ll}
p(r_i | \text{rest}) & \ds = \frac{(e^{-\lambda + \logit{(\rho)}})^{r_i}}{I(x_i = 0) + (e^{-\lambda + \logit{(\rho)}})^{r_i}}
\end{array}
$$

\bibliographystyle{elsarticle-harv}
\bibliography{Chapter_1_zero_inflated_models}

\end{document}
