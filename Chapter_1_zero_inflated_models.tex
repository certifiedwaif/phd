\documentclass{amsart}[12pt]

\usepackage{graphicx}

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
%\setlength\parindent{0pt}
%\setlength{\bibsep}{0pt plus 0.3ex}



\title{Variational approximations to zero-inflated Bayesian models}
\author{Mark Greenaway}
\input{include.tex}
\input{Definitions.tex}
\begin{document}
\maketitle
% First, simplest zero-inflated count model to consider.
\noindent Count data with a large number of zero counts arises in many areas of
application, such as data arising from physical activity studies, 
insurance claims, hospital visits or defects in manufacturing processes.

\section{A factorised variational approximation to the univariate zero-inflated Poisson model}

% TODO: Add graphical model

\noindent We use a factorised approximation to the full likelihood, as detailed in \cite{ormerod10}.
The use of conjugate priors in the full model yields easier mean field updates in the
variational approximation.

\noindent We start with the simplest example of a zero-inflated Poisson model. Let

$$
y_i = r_i x_i, 1 \leq i \leq n,
$$

\noindent where $x_i \sim \Poisson{\lambda}$ independent of $r_i \stackrel{\text{ind.}}{\sim} \Bernoulli{\rho}, 1 \leq i \leq n$. We use vague priors
$\rho \sim \Unif{0, 1}$ and $\lambda \sim \myGamma{0.01, 0.01}$. 

\noindent It can be shown via standard algebraic manipulations that the
full conditionals for $\lambda, \rho$ and $\vr$ are:

$$
\begin{array}{ll}
\lambda | \textbf{rest} &\sim \myGamma{\alpha_\lambda + \vone^T\vx, \beta_\lambda + \vone^T\vr}, \\
\rho | \textbf{rest} &\sim \Beta{\alpha_\rho + \vone^T \vr, \beta_\rho + \vone^T(\vone - \vr)}, \\
r_i | \textbf{rest} &\sim \Bernoulli{\text{expit}(\eta_i)}, 1 \leq i \leq n
\end{array}
$$

% Step Two: Assume q(r_i) = Bernoulli(\rho_i), 1 \leq i \leq n for some known \rho_i. Find the
% variational Bayes updates of the q-densities q(\lambda) and q(\rho) corresponding to the
% factorisation
% q(\vr, \lambda, \rho) = q(\lambda) q(\rho) \sum_{i=1}^n q(r_i)

\noindent We assume a factorised approximation of the form

$$
q(\lambda, \rho, \vp) = q(\lambda) q(\rho) \prod_{i=1}^n q(p_i), 1 \leq i \leq n
$$

\noindent where $q(\lambda)$ is a Gamma distribution, $q(\rho)$ is a Beta distribution and
$q(p_i)$ are Bernoulli distributions.

\noindent This leads to the following functional forms of the optimal q-densities

$$
\begin{array}{l}
\mbox{$q^*(\lambda)$ is the $\myGamma{\alpha_{q(\lambda)}, \beta_{q(\lambda)}}$ density function,} \\
\mbox{$q^*(\rho)$ is the $\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})$ density function, and} \\
\mbox{$q^*(r_i)$  is the $\text{Bernoulli}(p_{q(r_i)})$ density function, $1 \leq i \leq n$,}
\end{array}
$$

\noindent where the parameters are updated according to \ldots

%By taking the expectation of each full conditional with respect to 

\subsection{Mean field update equations}
We are now in a position to calculate the mean field update equations for the factorised
variational approximation.

Assuming that $q(r_i) \sim \Bernoulli{p_i}$,

% Mean field update for q(\lambda)
$$
\begin{array}{ll}
q^*(\lambda) &\propto \lambda^{\alpha_\lambda+\vone^T\vx - 1} \exp{\left[ \bE_{-q(\lambda)} \{-(\beta_\lambda + \vone^T\vr) \lambda \} \right]} \\
&\propto \lambda^{\alpha_\lambda+\vone^T\vx - 1} \exp{\{-(\beta_\lambda + \vone^T\vp)\lambda \} } \\
&= \myGamma{\alpha_\lambda+\vone^T\vx, \beta_\lambda+\vone^T\vp}
\end{array}
$$

and

% Mean field update for q(\rho)
$$
\begin{array}{ll}
\log{q^*(\rho)} &\propto \{ \bE_{-q(\rho)} \left[ \vone^T\vr \log{(\rho)} + \vone^T(\vone - \vr) \log{(1 - \rho)} \right] \} \\
&\propto \exp{ \left( \vone^T\vp \log{(\rho)} + \vone^T(\vone - \vp) \log{(1 - \rho)} \right) } \\
&= \Beta{\alpha_\rho + \vone^T\vp, \beta_\rho + \vone^T(\vone - \vp)}
\end{array}
$$

$$
\begin{array}{ll}
\log{q^*(r_i)} &\propto -\bE_{q(\lambda)} [\lambda ] r_i + x_i \log{(r_i)} + r_i \bE_{q(\rho)} [\log{\left(\frac{\rho}{1 - \rho}\right)}]\\
&= -r_i \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + x_i \log{(r_i)} + r_i (\Psi(\alpha_{q(\rho)}) - \Psi(\beta_{q(\rho)}))\\
&= \text{Bernoulli}(p_i)
\end{array}
$$

where

$$
\begin{array}{ll}
p_i &= \frac{\exp{(\eta_i)}}{I(x_i = 0) + \exp{(\eta_i)}} \\
&= \text{expit}(\eta_i)
\end{array}
$$

and $\eta_i = -\frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + \Psi(\alpha_{q(\rho)}) - \Psi(\beta_{q(\rho)})$.

%Note that if $x_i = 0$ then
%$x_i \log{(r_i)} = \log{(x_i^{r_i})} = \log{(0^0)} = \log{(1)} = 0$. Dr John %Ormerod hit
%upon the idea of side-stepping this problem by writing the q-likelihood as

%$$
%q^*(r_i) \propto r_i^{x_i} \exp{(r_i \eta_i)}
%$$


%Now, either $x_i = 0$ or $x_i \ne 0$.

%So

%$$
%q^*(r_i) = \text{Bernoulli}(p_i)
%$$

So the mean field update equations are \ldots

$$
\begin{array}{ll}
p_i &= \ds \frac{\exp{(\eta_i)}}{I(x_i = 0) + \exp{(\eta_i)}} \\
&= \text{expit}(\eta_i)
\end{array}
\begin{array}{cl}
\alpha_{q(\rho)} &\leftarrow \alpha_\rho + \vone^T\vp \\
\beta_{q(\rho)} &\leftarrow \beta_\rho + \vone^T(\vone - \vp) \\
\eta &\leftarrow -\frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} + \Psi(\alpha_{q(\rho)}) - \Psi(\beta_{q(\rho)}) \\
\vp_{q(\vr_0)} &\leftarrow \expit{\eta} \\
\alpha_{q(\lambda)} &\leftarrow \alpha_\lambda+ \vone^T\vx \\
\beta_{q(\lambda)} &\leftarrow \beta_\lambda+ \vone^T\vp
\end{array}
$$

% The formatting of this entire section is horrible.
% It should be organised into subterms, as you will for the multivariate
% model.
\subsection{Lower bound}
The lower bound $\log{\stackrel{\sim}{p}(\theta)}$ is
$$
	\bE_q[\log{p(x, \theta)} - \log{q(\theta)}]
$$

where $q(\theta) = q(\lambda) q(\rho) \prod_{i=1}^n q(r_i)$,
$q(\lambda) \sim \text{Gamma}{(\alpha_{q(\lambda)}, \beta_{q(\lambda)})}$,
$q(\rho) \sim \text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})$ and
$q(r_i) = 1$ if $x_i \ne 0$, and $p_i$ if $x_i = 0$, where $p_i$ is
calculated for each iteration as specified above.

In this case, the lower bound can be calculated directly to be

$$
\begin{array}{ll}
T_1 &=
\alpha_\lambda \log{(\beta_\lambda)} + (\alpha_\lambda - 1) \bE_q[\log{(\lambda)}] - \beta_\lambda \bE_q [\lambda] - \log{(\Gamma(\alpha_\lambda))} \\
&\quad + \sum_{i=1}^n ( -\bE_q [\lambda] \bE_q [r_i] + \bE_q[x_i \log{(\lambda r_i)}] - \log{(x_i!)}
\end{array}
$$

where

% T_1 terms
$$
\begin{array}{ll}
\bE [\log{\lambda}] &= -\{ \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma(\alpha_{q(\lambda)})} + (1 - \alpha_{q(\lambda)}) \psi{(\alpha_{q(\lambda)})} \} \\
\bE [\lambda] & \ds = \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} \\
\bE[\log{\rho}] &= - \{ \log{\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\psi{(\beta_{q(\rho)})} \\
    & \ds \qquad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \} \\
\bE [\lambda] &= \frac{\alpha_{q(\lambda)}}{\beta_{q(\lambda)}} \\
\end{array}
$$

$$
\begin{array}{ll}
T_2 &=\bE_q[r_i] \bE_q[\log{(\rho)}] + \bE_q[(1 - r_i)] \bE_q[\log{(1 - \rho)}] ) \\
& \quad - \bE_q \log{q(r)} - \bE_q \log{q(\lambda)} - \bE_q \log{q(\rho)}
\end{array}
$$

where

% T_2 terms
$$
\begin{array}{ll}
\bE[\log{\rho}] &= -\{ \log{(\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)}))} - (\alpha_{q(\rho)} - 1) \psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\psi{(\beta_{q(\rho)})} \\
&\quad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \} 
\end{array}
$$

By symmetry, we see that $(1 - \rho)$ has the same entropy as $\rho$,
and so $\bE [\log{(\rho)}] = \bE [\log{(1 - \rho)}]$.
$$
\bE[r_i] = 
	\begin{cases}
	1 & \text{if } x_i \ne 0 \\
	p_i & \text{if } x_i = 0 \\
	\end{cases}
$$

$$
\begin{array}{ll}
-\bE_q \log{q(r)} &= \sum_{i=1}^n I(x_i = 0) \log{(p_i)} \\ [1ex]
-\bE_q \log{q(\lambda)} &= \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma{(\alpha_{q(\lambda)})}} + (1 - \alpha_{q(\lambda)}) \Psi{(\alpha_{q(\lambda)})} \\ [1ex]
\bE_q[\log{q(\rho)}] &= - \{ \log{(\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \Psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\Psi{(\beta_{q(\rho)})} \\ [1ex]
    & \ds \quad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\Psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \} \\
-\bE_q \log{q(r)} &= \sum_{i=1}^n I(x_i = 0) \log{(p_i)} \\
-\bE_q \log{q(\lambda)} &= \alpha_{q(\lambda)} - \log{(\beta_{q(\lambda)})} + \log{\Gamma{(\alpha_{q(\lambda)})}} + (1 - \alpha_{q(\lambda)}) \Psi{(\alpha_{q(\lambda)})} \\
\bE_q[\log{q(\rho)}] &= - \{ \log{(\text{Beta}(\alpha_{q(\rho)}, \beta_{q(\rho)})} - (\alpha_{q(\rho)} - 1) \Psi{(\alpha_{q(\rho)})} - (\beta_{q(\rho)} - 1)\Psi{(\beta_{q(\rho)})} \\
&\quad + (\alpha_{q(\rho)} + \beta_{q(\rho)} - 2)\Psi{(\alpha_{q(\rho)} + \beta_{q(\rho)})} \} \\
\end{array}
$$

\subsection{Results}
%\includegraphics[width=100mm,height=100mm]{code/lower_bound_convergence.pdf}

\section{Extending the zero-inflated Poisson model to a regression model}
The above univariate model demonstrates that variational approximations are well-suited
to accelerating the fit of Bayesian zero-inflated models to data. Typically zero-inflated
models arise in applications where we wish to build multivariate regression models. To be able to
construct multivariate models with as much generality as possible, we specify the full
model as a General Design Bayesian Generalized Linear Mixed Model, as in \cite{zhao06}.
This allows us to incorporate within-subject correlation, measurement error, missing data
and smoothing splines in our models.

% TODO: Lower bound graph
% TODO: Accuracy of approximations
% TODO: Application, physical activity data
% Random intercept, longitudinal data
% Graph demonstrating additional zeroes

% Idea: We can use an approximation of the from q(\beta, \u, \Sigma) q(\rho) \Product q(r_i)
% and use GVA on q(\beta, \u, \Sigma) and mean field updates on \rho and r_i

\subsection{Model}
Let $\mR = \diag{(\vr)}$. Let $\mC = [X Z], \vnu = [\vbeta^T \vu^T]^T$.

$$
\begin{array}{ll}
\log{p(y|\vr, \vbeta, \vu)} &= \vy^T \mR (C\vnu) - \vr^T \exp{(C\vnu)} - \vone^T \log{\Gamma{(\vy + \vone)}} \\
\log{p(\vnu|\sigma_{u}^2)} &= - \half \log{|2\pi \sigma_{\vu}^2 \mI|} - \half \vnu^T(\sigma_{\vu}^2 I)^{-1} \vnu \\
\log{p(\vr)} &= \vr^T \log{(\rho \vone)} + (\vone - \vr)^T\log{(\vone - \vone \rho)} \\
p(\rho) &\propto 1 \\
\log{p(\sigma^2)} &= \alpha_{\sigma_\vu^2} \log{(\beta_{\sigma_\vu^2})} - \log{\Gamma{(\alpha_{\sigma_\vu^2})}} - (\alpha_{\sigma_\vu^2} + 1) \log{(\sigma_{\vu}^2)} - \frac{\beta_{\sigma_\vu^2}}{\sigma_{\vu}^2}\\
\end{array}
$$

\subsection{Approximation}
Let $r_0 = \{ r_i : y_i = 0 \}$.
We assume an approximation of the form
$$
q(r_0, \vnu, \sigma_{\vu}^2, \rho) = q(\vnu) q(\sigma_{\vu}^2) q(\rho) q(r_0) \\
$$

where

%\begin{align*}
$$
\begin{array}{rl}
q(\vnu) &= N(\vmu, \mLambda) \\
q(\sigma_{\vu}^2) &= IG\left(\alpha_{\sigma_u^2} + \frac{m}{2}, \beta_{\sigma_u^2} + \frac{\|\vmu_\vu\|^2}{2} + \frac{\tr(\mLambda_{\vu\vu})}{2}\right) \\
q(r_i) &\propto \exp{\{\bE_{-r_i} [-r_i \exp{(c_i^T\vnu)}]\} + r_i [\Psi(\alpha_\rho) - \Psi(\beta_\rho)]} \\
\end{array}
$$
%\end{align*}

Hence $q(r_i) = \Bernoulli{(p_i)}$ where
$$
p_i = \expit{\Psi{(\alpha_{q(\rho)})} - \Psi{(\beta_{q(\rho)})} - \exp{\{c_i^T\vnu + \half c_i^T \mLambda c_i\}}}
$$

The optimal approximation for $\vr$ is
$$
\begin{array}{ll}
q(\vr) &\propto \exp{\{\bE_{-q(\vr)}y^T\mR(\mC\vmu) - \vr^T\exp{(C\vnu)}-\half \vnu^T \text{diag}(\sigma_{\vu}^2)^{-1} \vnu\}}
\end{array}
$$

$$
\begin{array}{ll}
&\bE_{-q(\vr)} [\vy^T\mR(C\vnu) - \vr^T\exp{(\mC\vnu)}-\half \vnu^T D(\sigma_{\vu}^2)^{-1} \vnu]\\
=&\vy^T\mR\mC \vmu - \vp^T \exp{\{\mC \vmu + \half \text{diag}(\mC \mLambda \mC^T)\}} - \half \vmu^T \hat{D} \vmu - \half \text{tr}(\mLambda \hat{D})
\end{array}
$$

where $\hat{D} = \frac{\alpha_{q(\sigma_{\vu}^2)}}{\beta_{q(\sigma_{\vu}^2)}} \mI$. This can be seen to be very close to a Poisson
regression model.

\subsection{Lower bound}
% Explain how most of the lower bound for the univariate model can simply be re-used, but \lambda is replaced
% with \vbeta, \vu
% You can build the lower bound out of re-usable terms like T_1, T_2, T_3, ...
% We're just introducing a replacement T_1
$$
\begin{array}{ll}
\bE_q [\log{p(y, \theta)} - \log{q(\theta)}] &= 
\vy \mP^T C \vmu - \vp^T\exp{(C\vmu + \half \vmu^T \mLambda \vmu)} - \vone^T\log{(\vy + \vone)} + \\
&\quad \half \log{|\mLambda|} + \frac{p + m}{2} (1 + \log{(2 \pi)})
\end{array}
$$

%For $\sigma^2_u$,
%$$
%\bE_q [\log{p(\sigma_u^2)} - \log{q(\sigma_u^2)}] = \frac{m}{2} \bE_q [\log{\sigma_u^{-2}}] - (\half \|\mu_u\|^2 + \tr(\Lambda_{uu})) \bE_q [\sigma_u^{-2}] - \log{\Gamma(\alpha_{\sigma_u^2})} + \log{\Gamma(\alpha_{\sigma_u^2} + \frac{m}{2} - 1)}
%$$
%
%where
%
%$$
%\bE_q[\log{(\sigma_u^2)}] = - \{\alpha_{\sigma_u^2}^2 - \log{\beta_{q(\sigma_u^2)}} + \log{\Gamma(\alpha_{q(\sigma_u^2)})} + (1-\alpha_{q(\sigma_u^2)})\Psi{(\alpha_{q(\sigma_u^2)})}\}
%$$
%
%and
%
%$$
%\bE_q[\sigma_u^{-2}] = \frac{\alpha_{q(\sigma_u^2)}}{\beta_{q(\sigma_u^2)}}
%$$

For $\vnu$,

% Where are the priors for \vbeta and \vu

$$
\begin{array}{ll}
\bE_q[\log{p(\vnu)} - \log{q(\vnu)}] &= \vy^T \mP \mC \vmu - \vp^T \exp{(\mC \vmu + \half \vmu^T \mLambda \vmu)} - \vone^T\log \Gamma{(\vy + \vone)} + \\
& \quad \frac{p + m}{2} (1 + \log{2 \pi}) + \half \log{|\mLambda|}
\end{array}
$$

% This should be denoted as T_3

For $\sigma^2_u$,
$$
\begin{array}{ll}
\bE_q[\log{p(\sigma^2_u)} - \log{q(\sigma^2_u)}] &=
\alpha_{\sigma^2_u} \log{\beta_{\sigma^2_u}} - \log{\Gamma{(\alpha_{\sigma^2_u})}} +
(\alpha_{\sigma^2_u} - 1) \bE_q	\log{\sigma^2_u}\\
& \quad - \beta_{\sigma^2_u} \bE_q [\sigma^2_u] +
\log{\Gamma(\alpha_{q(\sigma^2_u)})} - (\alpha_{q(\sigma^2_u)} - 1) \Psi{(\alpha_{q(\sigma^2_u)})} - \log{(\beta_{q(\sigma^2_u)})}\\
& \quad - \log{(\alpha_{q(\sigma^2_u)} + \beta_{q(\sigma^2_u)})}\\ [2ex]
&=\alpha_{\sigma^2_u} \log{\beta_{\sigma^2_u}} - \log{\Gamma{(\alpha_{\sigma^2_u})}} +
(\alpha_{\sigma^2_u} - 1) \Psi{\alpha_{q(\sigma^2_u)}} - \log{(\beta_{q(\sigma^2_u)})} \\
& \quad - \beta_{\sigma^2_u} \frac{\alpha_{q(\sigma^2_u)}}{\beta_{q(\sigma^2_u)}} + 
\log{\Gamma(\alpha_{q(\sigma^2_u)})} - (\alpha_{q(\sigma^2_u)} - 1) \Psi{(\alpha_{q(\sigma^2_u)})} - \log{(\beta_{q(\sigma^2_u)})}\\
& \quad- \log{(\alpha_{q(\sigma^2_u)} + \beta_{q(\sigma^2_u)})}
\end{array}
$$

where

$\alpha_{q(\sigma^2_u)} = \alpha_{\sigma^2_u} + \frac{m}{2}$ and
$\beta_{q(\sigma^2_u)} = \beta_{\sigma^2_u} + \frac{\|\vmu_\vu\|^2}{2} + \frac{\tr{(\mLambda_{\vu \vu})}}{2}$

\subsection{Results}

\section{Theory of variational approximation of zero-inflated Poisson models}
Let $Z_i = R_i Y_i$.

Then the probability that $Z_i = 0$ is
$$
\begin{array}{ll}
P(Z_i = 0) &= P(R_i = 0)P(Y_i = y) + P(R_i = 1) P(Y_i = 0) - P(R_i = 0) P(Y_i = 0) \\
&= (1 - \rho) + \rho e^{-\lambda} - (1 - \rho) e^{-\lambda} \\
&= (1 - \rho) + e^{-\lambda}(2 \rho - 1)
\end{array}
$$

\section{Appendix - Algebraic derivations of conditional likelihoods}

The joint likelihood is:

$$
p(\vx, \vr, \lambda, \rho) = \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)}}{\Gamma{(a)}} \prod_{i=1}^n \frac{\exp{(-\lambda r_i)} (\lambda r_i)^{x_i}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i}.
$$

%\begin{align*}
%p(\lambda|\vx, \vr, \rho) &= \frac{\prod_{i=1}^n p(x_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)}{\int \prod_{i=1}^n p(x_i | \lambda, r_i) p(\lambda) p(r_i|\rho) p(\rho)) d \lambda}.
%\end{align*}
%
%Concentrating for now on the denominator in this expression, we re-arrange and collect
%like terms to obtain
%$$
%\prod_{i=1}^n \rho^r_i (1 - \rho)^{1 - r_i} \frac{r_i^{x_i}}{x_i !}
%	\int \frac{b^a \lambda^{a - 1} \exp{(-b \lambda)} \lambda^{x_i} \exp{(-\lambda r_i)}}{\Gamma{(a)}} d \lambda
%$$
%
%The integral in this expression is
%\begin{align*}
%& \int \frac{b^a \lambda^{(a + x_i) - 1} \exp{(-\lambda(b + r_i))}}{\Gamma{(a + x_i)}} d \lambda \frac{\Gamma{(a+ x_i)}}{\Gamma{(a)} b^{-x_i}} \\
%=& \frac{\Gamma{(a+ x_i)}}{\Gamma{(a)} b^{-x_i}}.
%\end{align*}
%
%Collecting the multiplicands in the integral over $\lambda$ together, we obtain
%$$
%\prod_{i=1}^n \rho^{r_i} (1 - \rho)^{1 - r_i} \frac{r_i^{x_i}}{x_i!}
%	\int \frac{b^{na + \sum_{i=1}^n x_i} \lambda^{(na + \sum_{i=1}^n x_i) - 1} \exp{(-\lambda(nb + \sum_{i=1}^n r_i))}}{\Gamma{(na + \sum_{i=1}^n x_i)}} d \lambda
%	\frac{\Gamma{(na + \sum_{i=1}^n x_i)}}{\Gamma{(na)} b^{-\sum_{i=1}^n x_i}}.
%$$

%By cancelling like terms in the numerator and denominator of the full likelihood we arrive 
%at

$$
\beta_{\lambda}^{\alpha_\lambda+\vone^T\vx} \lambda^{(\alpha_\lambda + \vone^T\vx) - 1} \exp{\left(-(\beta_\lambda + \vone^T\vr) \lambda \right)}
$$

$$
\frac{\rho^{\vone^T\vr} (1 - \rho)^{\vone^T(\vone - \vr)}}{\Beta{\alpha_\rho + \vone^T \vr, \beta_\rho + \vone^T(\vone - \vr)}}
$$

$$
\begin{array}{ll}
p(r_i | \text{rest}) & \ds \propto \frac{(\lambda r_i)^{x_i} \exp{(-\lambda r_i)}}{x_i !} \rho^{r_i} (1 - \rho)^{1 - r_i} \\
& \ds \propto r_i^{x_i} (e^{-\lambda})^{r_i} \rho^{r_i} (1 - \rho)^{1 - r_i}
\end{array}
$$

We make use of the fact that if $x_i = 0$, $r_i = 0$, and if $x_i \ne 0$,
$r_i = 1$. So $x_i^{r_i} = I(x \ne 0)$ and hence the likelihood can be re-written as

$$
\begin{array}{ll}
p(r_i | \text{rest}) & \ds = \frac{(e^{-\lambda + \logit{(\rho)}})^{r_i}}{I(x_i = 0) + (e^{-\lambda + \logit{(\rho)}})^{r_i}}
\end{array}
$$
\bibliographystyle{plain}
\bibliography{Chapter_1_zero_inflated_models}

\end{document}
