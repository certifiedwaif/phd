% \documentclass{article}[12pt]
\documentclass[times, doublespace]{anzsauth}

\addtolength{\oddsidemargin}{-.75in}%
\addtolength{\evensidemargin}{-.75in}%
\addtolength{\textwidth}{1.5in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%
\addtolength{\marginparpush}{-.75in}%
% \setlength\parindent{0pt}
% \setlength{\bibsep}{0pt plus 0.3ex}

% \usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}

\usepackage{graphicx,verbatim,subfigure,amsfonts,color}

\input{include.tex}
\input{Definitions.tex}

\newcommand{\mgc}[1]{{\color{blue}#1}}
\newcommand{\joc}[1]{{\color{red}#1}}
 

\begin{document}

\title{Variational Approximation for Zero-Inflated Semiparametric Regression Models}
\author{Mark Greenaway and John T. Ormerod}
% \maketitle

\begin{abstract}
	
	\noindent We consider variational inference for zero--inflated Poisson regression models using a latent
	variable representation. The model is extended to include random effects which allow simple incorporation of
	spline and other modelling structures. Several variational approximations to the resulting set of models are
	presented, including a novel approach based on the inverse covariance matrix rather than the covariance matrix
	of the approximate posterior density for the random effects. This parameterisation improves upon the
	computational cost and numerical stability of previous methods. We demonstrate these approximations on
	simulated and real data sets.
	
\end{abstract}
 

\noindent Keywords: Approximate Bayesian inference ; mixed model ; Markov chain Monte Carlo ; Stan ; penalized splines.

\section{Introduction}
\label{sec:introduction}

Count data with a large number of zero counts arises in many areas of application, such as data arising from
physical activity studies, insurance claims, hospital visits or defects in manufacturing processes. Zero
inflation is a frequent cause of overdispersion in Poisson data, and not accounting for the extra zeroes leads
may lead to biased parameter estimates. These models have been used for many applications, including defects
in manufacturing in \cite{lambert1992}, horticulture in \cite{BIOM:BIOM1030} and \cite{Hall2000}, length of
stay data from hospital admissions in \cite{BIMJ:BIMJ200390024}, psychology in \cite{JOFP:rethink},
pharmaceutical studies in \cite{Min01042005}, traffic accidents on roadways in \cite{Shankar1997829} and
longitudinal studies in \cite{LeeWangScottYauMcLachlan2006}.

\noindent The strength of the approach derives from modelling the zero and non-zero count data seperately as a
mixture of distributions for the zero and non-zero components, allowing analysis of both the proportion of
zeroes in the data set and the conditions for the transition from zero to non-zero. When combined with a
multivariate mixed model regression framework, an extremely rich class of models can be fit allowing a broad
range of applications to be addressed. Often the transition from zero to non-zero has a direct interpretation
in the area of application, and is interesting in its' own right. 

\noindent In this paper, we build upon the earlier work on Bayesian zero-inflated models of
\cite{Ghosh20061360} and \cite{VatsaWilson2014}. While simple forms of these models are easy to fit with
standard maximum likelihood techniques, more general models incorporating random effects, splines and missing
data typically have no closed form solutions and hence present a greater computational challenge to fit.

\noindent Fitting these models is typically done with Monte Carlo Markov Chain techniques, but these can be
slow and prone to convergence problems. Semiparametric mean field Variational Bayes is an approximate Bayesian
inference method as detailed in \cite{ormerod10} and \cite{RohdeWand2015}. We build upon a latent variable
representation of these models to allow a tractable semiparametric mean field Variational Bayes approximation
to be derived. Semiparametric mean field Variational Bayes is an approximate Bayesian inference method as
detailed in \cite{ormerod10} and \cite{RohdeWand2015}, which allows us to fit close approximations to these
models using a deterministic algorithm which converges much more quickly.

\noindent We allow a flexible regression modelling approach by using a Gaussian Variational Approximation as
defined in \cite{ormerod09} on the regression parameters to allow a non-conjugate Gaussian prior to be used,
making the resulting Gaussian posterior of the regression parameters easy to interpret. We adopt a Mean Field
Variational Bayes (VB) approach on the other parameters in the model to derive the rest of the approximation.

\noindent The focus of this paper is on developing a method of fitting flexible ZIP regression models
accurately, and showing the advantages of our method to previously presented methods. In Section
\ref{sec:methodology} we briefly introduce Variational Bayes methodogy. In Section \ref{sec:model_and_data} we
define our model and provide a framework for our approach incorporating regression modelling and random
effects. In Section \ref{sec:algorithms} we present algorithms for fitting these models, along with a new
parameterisation which offers substantial advantages in accuracy, numerical stability and computational speed.
In Section \ref{sec:results} we perform numerical experiments on simulated data which show how our approach
offers computational advantages over existing approaches. In Section \ref{sec:application} we show an
application of our method to a multi-level longitudinal study of pest control in apartments. Finally, in
Section \ref{sec:discussion} we conclude with a discussion of the results. An appendix contains details of the
derivation of the variational lower bound for our model.

\section{Methodology}
\label{sec:methodology}

In this section we present a VB approach to a Bayesian zero-inflated Poisson model for count data with extra
zeroes. After introducing the latent variable representation of Bayesian zero-inflated models and a short summary 
of VB methodology we derive several variational approxiamtions for the model.

\subsection{Definitions}

Let $p$ be the dimension of the space of fixed effects, $m$ be the number of individuals in the random effects
and $b$ be the block size for each of those individuals.
We use $\vone_p$ and $\vzero_p$ to denote the $p \times 1$ column vector with all entries equal to 1 or 0,
respectively.
Let $\vy$ by the $n \times 1$ vector.
The norm of a column vector $\vv$, defined to be $\sqrt{\vv^\top \vv}$, is denoted by $\|\vv\|$.
For a $p \times 1$ vector $\va$, we let $\diag{(\va)}$ denote the $p \times p$.
Let $\vtheta$ be the parameter vector in $\R^p$,
$\mX$ be a matrix with dimensions $n \times p$,
$\mZ$ be a matrix with dimensions $n \times m b$
and $\mC = [ \mX \mZ ]$.
Let $\vbeta$ be the $p \times 1$ column vector representing the fixed effects,
$\vu$ be the $m b \times 1$ column vector representing the random effects
and $\vnu = [\vbeta^\top, \vu^\top]$.
Let $\mSigma$, $\mPsi$ and $\mLambda$ be $(p + m b + 1) \times (p + m b + 1)$ matrices.
Let $\vp$ be the $n \times 1$ column vector of probabilities that each observation in $\vy$ is non-zero.
Let $\expit(x)$ denote the function $\tfrac{1}{1 + \exp(-x)}$, the inverse of the logit function.
Let $p(x)$ be the full probability distribution, and $q(x)$ be the approximating probability distribution.
Let $q^*(x)$ be the optimal approximating probability distribution.
Let $\text{Bernoulli}(\pi)$  denote the probability distribution $\pi^k (1 - \pi)^{1-k}$
and $\text{Inverse Wishart}(\mPsi, v)$ denote the probability distribution
$\tfrac{|\mPsi|^\frac{v}{2}}{2^{\frac{vp}{2}} \Gamma_p{(\tfrac{v}{2})}} |\mX|^{-\tfrac{v + p + 1}{2}} \exp{[-\half \tr{(\mPsi \mX^{-1})}]}$ where $\Gamma_p{(x)}$ denotes the multivariate gamma function and
$\tr$ is the trace function.


\subsection{Semiparametric Mean Field Variational Bayes}

\subsubsection{Definitions}

The density function of a random vector $\vu$ is denoted by $p(\vu)$.  The conditional density function of a
random vector $\vu$ given $\vv$ is denoted by $p(\vu|\vv)$. Consider a generic Bayesian model with parameter
vector $\vtheta \in \Theta$. Throughout this section we assume that $\vy$ and $\vtheta$ are continuous random
vectors. The KL divergence between the probability distributions $p$ and $q$ is defined as

$$
\KL(q || p) \equiv \int q(\vtheta) \log \{ \frac{q(\vtheta)}{p(\vtheta | \vy)} \} d \vtheta.
$$

\noindent The approximation is fit by iteratively minimising the Kullback-Leibler divergence between the true
posterior and an approximating distribution. Thus the optimal approximation $q^*(\vtheta)$ is

$$
q^*(\vtheta) = \argmin_{\vxi \in \Xi} \text{KL} \{ {q(\vtheta|\vxi) || p(\vtheta|\vy)} \}.
$$

\noindent A common approach is to assume an approximation in a factorised form

$$q(\vtheta) = \Pi_{i=1}^M q(\theta_i).$$

\noindent The variational lower bound is maximised iteratively. On each iteration, the value of each parameter
in the model is calculated as the expectation of the full likelihood relative to the other parameters in the
model, which is referred to as the mean field update:

$$q_i^*(\theta_i) \propto \exp{\{ \bE_{-q(\theta_i)} \log p(\vy, \vtheta) \}}$$

\noindent This is done for each parameter in the model in turn until the variational lower bound's increase is
negligible and convergence is achieved.

\noindent This approach works well for classes of models where all of the parameters are conjugate. For more
general classes of models, mean field updates are not analytically tractable and general gradient-based
optimisation methods must be used, as for the Gaussian Variational Approximation (see \cite{ormerod09}) used
in this paper. These methods are generally difficult to apply in practice, as the problems can involve the
optimisation of many parameters over high-dimensional, constrained spaces whose constraints cannot be simply
expressed.

\newpage 

\joc{
	Comments: 
	\begin{itemize}
		\item Need to organize in terms of a flow of ideas. What are we approximating?
		      
		\item I believe that we are using a semiparametric mean field variational Bayes approach discussed by Rohde \& Wand (2015).
		      However, I am not sure that we are using the their formalisms. (see page 3-6 of Rohde and Wand 2015).
	\end{itemize}	
}

\noindent 
Much of Bayesian inference is based on the posterior distribution of a model's parameters given observed data 
defined by $p(\vtheta|\vy) = p(\vy|\vtheta)p(\vtheta)/p(\vy)$ where $\vy$ is a vector of observed data,
$\vtheta$ are the model parameters $p(\vy|\vtheta)$ is the model distribution, $p(\vtheta)$ is a prior 
distribution on $\vtheta$ and $p(\vy)=\int p(\vy|\vtheta)p(\vtheta)d\vtheta$. Here the integral is performed
over the domain of $\vtheta$. If a subset of $\vtheta$ are discrete random variables then the integral over
these parameters is replaced with a combinatorial sum over all possible values of these discrete random 
variables.

\noindent The calculation of the posterior distribution is usually either computationally intractable or no
closed from exists for the posterior distribution and approximation is required. Variational approximation is
a  class of methods for dealing with this problem by transforming the problem into an optimization problem.
Most, but not all variational approximations are bsed on minimising the Kullback-Leibler divergence between
the true posterior $p(\vtheta|\vy)$ and an approximating distribution $q(\vtheta)$, sometimes called a
$q$-density. Suppose that $q(\vtheta)$ is parameterised by $\vxi$ and write $q(\vtheta)\equiv
q(\vtheta;\vxi)$. We attempt to solve

$$
\ds q^*(\vtheta) = \argmin_{\vxi \in \Xi} \text{KL} \{ {q(\vtheta;\vxi) || p(\vtheta|\vy)} \}.
$$

\noindent Noting that the Kullback-Leibler divergence between two distributions is zero if and only if
the two distributions are equal almost everywhere the above problem is no easier than the original problem
of finding $p(\vtheta|\vy)$. This problem is mitigated by imposing structure on the form of $q$. Two
strategies for doing this are:
\begin{enumerate}
	\item[(A)] Specifying the parametric form of $q$; or 
	\item[(B)] choosing $q$ to be of the form $q(\vtheta) = \prod_{i=1}^M q(\theta_i)$,
\end{enumerate}

\noindent where it can be shown (see Ormerod \& Wand, 2010, for example) that the optimal form of the
$q_i$'s are of the form
\begin{equation}\label{eq:consistency}
	q_i^*(\theta_i) \propto \exp{\{ \bE_{-q(\theta_i)} \log p(\vy, \vtheta) \}},  \quad 1\le i\le M,
\end{equation}

\noindent where the above set of equations, sometimes called consistency conditions, need to be 
satisfied simultaneously. It can be shown that by calculating $q_i^*(\theta_i)$ for a particular
$i$ with the remaining $q_j^*(\theta_j)$, $j\ne i$ fixed, results in a monotonic decrease in the 
Kullback-Leibler divergence. Note that a combination of (A) and (B) can be used which has recently
been formalized by Rohde \& Wand (2015). 

\noindent Note that it can easily be shown that

$$
\ds \log p(\vy) = \int q(\vtheta;\vxi)\frac{p(\vy|\vtheta)p(\vtheta)}{q(\vtheta;\vxi)} d\vtheta + \text{KL}(q(\vtheta;\vxi)||p(\vtheta|\vy)).
$$

\noindent Noting the Kullback-Leibler divergence is strictly positive the first term on the right hand side
defined a lower bound on the marginal log-likelihood which we will denfine by
$$
\ds \log \underline{p}(\vy;\vxi) \equiv \int q(\vtheta;\vxi)\frac{p(\vy|\vtheta)p(\vtheta)}{q(\vtheta;\vxi)} d\vtheta
$$

\noindent and maximizing $\log \underline{p}(\vy;\vxi)$ with respect to $\vxi$ is eqivalent to minimizing
$\text{KL}(q(\vtheta;\vxi)||p(\vtheta|\vy))$.

%This approach works well for classes of models where all of the parameters are conjugate. For more general
%classes of models, mean field updates are not analytically tractable and general gradient-based optimisation
%methods must be used, as for the Gaussian Variational Approximation (see \cite{ormerod09}) used in this paper.
%These methods are generally difficult to apply in practice, as the problems can involve the optimisation of
%many parameters over high-dimensional, constrained spaces whose constraints cannot be simply expressed.

\subsection{Modeling zero-inflated Poisson data}

Suppose we have a sample of counts $y_i$, $1\le i\le n$, where there are excessive number of zeros 
for a Poisson model, but the sample otherwise behaves like a Poisson sample.
% There are two main 
% parameterizations for modelling such data. The first approach models the probability of a zero
% by $\rho$ and adjusts for counts greater than zero. This model uses
% $$
% P(Y_i = 0) = \rho, 
% \quad \mbox{and} \quad 
% P(Y_j = y_i) = \left( \frac{1 - \rho}{1 - e^{-\lambda}} \right) \frac{\lambda^{y_i} e^{-\lambda}} {y_i!},\qquad y_i \ge 1,
% $$

% \noindent where $\lambda>0$ models the size of the counts. Here, $\bE(y_i) = ???$ 
% and $\mbox{Var}(y_i) = ???$.

\noindent An approach using latent variables considers the data as the product of two processes, a binary process
that generates structural zeros, and a second process governed by a Poisson distribution. This leads to the model
\begin{equation}\label{eq:modelTwo}
	P(Y_i = 0) = \rho + (1 - \rho) e^{-\lambda}, 
	\quad \mbox{and} \quad 
	P(Y_i = y_i) = (1 - \rho) \frac{\lambda^{y_i} e^{-\lambda}} {y_i!},\qquad y_i \ge 1,
\end{equation}

\noindent where $\bE(y_i) = (1 - \rho)\lambda$ 
and $\mbox{Var}(y_i) =  \lambda(1 - \rho)(1 + \rho\lambda)$.

\noindent An auxiliary variable representation of this parameterization introduces the auxiliary
variables $r_i$ which equals $1$ when $y_i>0$ and is unobserved otherwise. This leads to the
specification 
$$
P(Y_i=y_i|r_i) = \frac{\exp(-\lambda r_i)(\lambda r_i)^{y_i}}{y_i!} \quad \mbox{and} \quad r_i \sim \mbox{Bernoulli}(1-\rho)
$$

\noindent Marginalization over $r_i$ leads to (\ref{eq:modelTwo}).

\noindent We can add covariates using a log-link where we replace $\lambda$ with $\vx_i^T\vbeta$ where
$\vx_i,\vbeta\in\bR^p$ with $\vx_i$ being a vector of observed predictors and $\vbeta$ is 
a vector of regression coefficients. Let $\vr = (r_1,\ldots,r_n)$
 
\begin{equation}\label{eq:main}
	\begin{array}{rl}
		\log p(\vy|\vr, \vbeta) 
		    & = \vy^T \mR (\mX\vbeta)                              
		- \vr^T \exp{(\mX\vbeta)} 
		- \vone^T \log{\Gamma{(\vy + \vone)}}, \quad \mbox{ and }\\ [1ex]
		r_i & \sim \text{Bernoulli}(1-\rho), \quad 1 \leq i \leq n \\
	\end{array}
\end{equation} 

\noindent where $\mX$ is the $n\times p$ matrix whose $i$th rows equal $\vx_i$.


\subsection{Mixed model architecture}

The $j$th predictor/response pair for the $i$th group is denoted by $(\vx_{ij}, \vy_{ij}), 1 \leq j \leq n_i, 1 \leq i \leq m$, where $\vx_{ij} \in \R$, and the $\vy_{ij}$ are nonnegative integers.

\noindent For each $1 \leq i \leq m$, define the $n_i \times 1$ vectors $\vy_{ij} = [\vy_{i 1}, \ldots, \vy_{i
	n_i}]^\top$ and let $\vone$ define a vector of ones of appropriate length, where the first of these vectors is
the response. It is reasonable to assume that the vectors $\vy_1, \ldots, \vy_m$ are independent of each
other.

\joc{The next paragraph is too abrupt. How did this model come about? Where is the Poisson component? How is this combined with the zero-inflation?
Start with a single observation before using matrix/vector notation.}

\noindent The log--likelihood for one observation is then
\begin{equation*}
	\begin{array}{rl}
		\log p(y_i | \vx, \vr, \vz) & = y_i r_i (\vx_i^\top \vbeta + \vz_i^\top \vu) - r_i \exp (\vx_i^\top \vbeta + \vz_i^\top \vu) - \log \Gamma (y_i + 1), \\
		r_i                         & \sim \text{Bernoulli}(\rho), 1 \leq i \leq n, \text{ and }                                                                            \\
		\rho                        & \sim \mbox{Uniform}(0, 1).                                                                                              \\
	\end{array}
\end{equation*}

\noindent We now extend this to multiple observations. Let $\mR = \diag{(\vr)}$, $\mC = [\mX, \mZ]$ and $\vnu = [\vbeta^\top, \vu^\top]^\top$. Consider the model


\begin{equation}\label{eq:main}
	\begin{array}{rl}
		\log{p(\vy|\vr, \vbeta, \vu)} & = \vy^\top \mR (\mC\vnu) - \vr^\top \exp{(\mC\vnu)} - \vone^\top \log{\Gamma{(\vy + \vone)}}, \quad \mbox{ and } \\ [1ex]
		r_i                           & \sim \text{Bernoulli}(\rho), 1 \leq i \leq n                                                                     \\
	\end{array}
\end{equation}

\noindent \joc{(The prior structure will depend on the structure of the rrandom effects model)}
with priors

$$ 
\begin{array}{rl}
	\log{p(\mSigma_{\vu \vu})}     & = \text{Inverse Wishart}(\mPsi, v),     \\
	p(\rho)                        & \propto 1                               \\ 
	\mbox{ and } \vnu|\sigma_\vu^2 & \sim \mbox{N}(\vzero, \sigma_\vu^2 \mI) \\
\end{array}
$$


\begin{align*}
	\log{p(\mSigma_{\vu \vu})} & = \text{Inverse Wishart}(\mPsi, v), \\
	\rho \sim \mbox{Uniform}(0, 1),\\
	\vbeta|\sigma^2_\vbeta \sim \N_p(\vzero, \sigma^2_\vbeta \mI)\\
	\mbox{ and } \vu|\mG \sim \N_{mb}(\vzero, \mG)
\end{align*}


\noindent where $\mX$ is $n \times p$, $\mZ$ is $n \times mb$ and $\mSigma_{\vu \vu}$ is $mb \times mb$ and
$\mPsi$ is $b \times b$.

The covariance of $\Cov(\vu) \equiv \blockdiag_{1 \leq i \leq m} (\mSigma) \equiv \mI_m \otimes \mSigma$.

In the random intercept case, $\mSigma = \sigma_u^2 \mI$ while in the random slopes case
$\mSigma = 
\begin{pmatrix}
\sigma_{\vu_1}^2 & \rho_{\vu_1 \vu_2} \sigma_{\vu_1} \sigma_{\vu_2} \\
\rho_{\vu_1 \vu_2} \sigma_{\vu_1} \sigma_{\vu_2} & \sigma_{\vu_2}^2
\end{pmatrix}
$
where $\sigma_{\vu_1}^2$ is the variance of the random intercepts, $\sigma_{\vu_2}^2$ is the variance of the
random slopes and $\rho_\vu$ is the covariance between the random intercepts and random slopes.

% \joc{Shouldn't we specify the structure of $\mSigma_{\vu \vu})$ later
% which is different for the random intercept, slope and spline cases?}
% \joc{(Perhaps it is wroth spelling out all of the various random effects structures that we will be using. Consider templating from Zhao \etal (2006).))}
\mgc{Need more detail, ala Zhao paper page 38, Example 1 and Example 2. Spline case still to go.}

\section{The model}
\label{sec:model_and_data}

Variational approximations are well-suited to accelerating the fit of Bayesian zero-inflated models to data.
Typically zero-inflated models arise in applications where we wish to build multivariate regression models. To
be able to construct multivariate models with as much generality as possible, we specify the full model as a
General Design Bayesian Generalized Linear Mixed Model, as in \cite{zhao06}.

\noindent This allows us to incorporate within-subject correlation, and smoothing splines (as in
\cite{Wand2008}) in our models.

General Design Bayesian Generalized Linear Mixed Model, as in \cite{zhao06}. This allows us to incorporate
within-subject correlation, and smoothing splines (as in \cite{Wand2008}) in our models.


% Idea: We can use an approximation of the from q(\beta, \u, \Sigma) q(\rho) \Product q(r_i)
% and use GVA on q(\beta, \u, \Sigma) and mean field updates on \rho and r_i


\subsection{Approximation}

Our variational approximation for models of the form 
$$
q(\vr_0, \vnu, \sigma_{\vu}^2, \rho) = q(\vnu)q(\rho) q(\vr_0)q(\mSigma_{\vu \vu})  \\
$$

\noindent 
where we define $\vr_0 = \{ r_i : y_i = 0 \}$,
$q(\sigma_{\vu}^2) = \mbox{Inverse Wishart}\left(\mPsi + \sum_{i=1}^m \vmu_i \vmu_i^\top + \mLambda_{\vu_i \vu_i}, v + m + 
p\right)$ \mbox{and } $q(r_i) = \Bernoulli{(p_i)}$

\noindent 
with
$$
p_i = \expit\left[ \psi{(\alpha_{q(\rho)})} - \psi{(\beta_{q(\rho)})} - \exp{(c_i^\top\vmu + \half c_i^\top \mLambda c_i)} \right]
$$

\noindent 
\text{when} $\vy_i = 0$.

%$\propto \exp{\left \{-r_i \bE_{-r_i} [\exp{(c_i^\top\vnu)}] + r_i [\psi(\alpha_\rho) - \psi(\beta_\rho)] \right \} }.\\$

\noindent The optimal approximation \joc{(reword: the ``optimal approximation'' might be called the true posterior)} for $\vr$ is


$$
\begin{array}{rl}
	q(\vr) & \propto \exp \left [ \bE_{-q(\vr)}\vy^\top\mR(\mC\vmu) - \vr^\top\exp{(\mC\vnu)}-\half \vnu^\top \mSigma_{\vu \vu} \vnu \right ]                                                  \\ [1ex]
	       & = \exp{ \left\{ \vy^\top\mR\mC \vmu - \vr^\top \exp{[\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)]} - \half \vmu^\top \mD \vmu - \half \text{tr}(\mLambda \mD ) \right\} } 
\end{array}
$$

\noindent where $\mD = \left[ (\mPsi + \sum_{i=1}^m \vmu_i \vmu_i^\top + \mLambda_{\vu_i\vu_i}) / (v - p - 1) \right]^{-1}$. 

\noindent This is close in form to a Poisson regression model. Poisson regression models with normal priors
have no closed form for their mean field updates due to non- conjugacy, but can be fit using Gaussian
Variational Approximation, as in \cite{ormerod09}. The model can be fit using Algorithm
\ref{alg:algorithm_one} below.

\begin{algorithm}
	\caption[Algorithm 1]{Iterative scheme for obtaining the parameters in the
		optimal densities $q^*(\vmu, \mLambda)$, $q^*(\mSigma_{\vu \vu})$ and $q^*(\rho)$}
	\label{alg:algorithm_one}
	\begin{algorithmic}
		\REQUIRE{$\alpha_{q(\rho)} \leftarrow \alpha_\rho + \vone^\top\vp, p_{q(\mSigma_{\vu \vu})} \leftarrow p + 1$} \\[1ex]
		\WHILE{the increase in $\log{\underline{p}}(\vy;q)$ is significant}
		% \vmu, \mLambda
		\STATE Optimise $\vmu$ and $\mLambda$ using $\vy, \mC, \vp$ and $\mSigma_{\vu \vu}$ \\[1ex]
		% \vp
		\STATE $\beta_{q(\rho)} \leftarrow \beta_\rho + n - \vone^\top\vp$ \\[1ex]
		\STATE $\eta \leftarrow -\exp \left [ \mC \vmu + \half \diag{(\mC\mLambda\mC^\top)} \right ] + \psi{(\alpha_{q{(\rho)}})} - \psi{(\beta_{q{(\rho)}})}$ \\[1ex]
			\STATE $\vp_{q(\vr_0)} \leftarrow \expit{(\eta)}$ \\[1ex]
			% \mSigma_{\vu \vu}
			\STATE $\mPsi_{q(\mSigma_{\vu \vu})} \leftarrow \Psi + \sum_{i=1}^m (\vmu_i \vmu_i^\top + \mLambda_{{\vu}_i {\vu}_i})$ \\[1ex]
			\STATE $\mSigma_{\vu\vu} \leftarrow [\mPsi_{q(\mSigma_{\vu \vu})}/(v - d - 1)]^{-1}$
			\ENDWHILE
		\end{algorithmic}
	\end{algorithm}
	
	\section{Numerical optimisation strategies}
	\label{sec:algorithms}
	
	In this section, we compare the accuracy, stability and speed of a number of algorithms for fitting the
	Gaussian component of our model, $q(\vmu, \mLambda)$.
	
	
	\subsection{Laplace-Variational Approximation}
	
	Laplace's method of approximation uses the second order Taylor expansion of the full log likelihood around
	the mode to find a Gaussian approximation to the full posterior. 
	% The algorithm is very quick to execute, but the resulting approximate posterior
	% distributions are not as accurate as those produced by the other algorithms considered in this article.
	
	% NR
	% Detail the function and its derivatives
	
	Taylor expanding the full log likelihood once around the mode yields the following approximation to the
	log-liklihood:
	\begin{align*}
		\log \underline{p}(\vmu, \mLambda; \vy) = \vy^\top\mP\mC\vmu - \vp^\top\exp \left (\mC \vmu \right ) - \half \vmu^\top \mSigma^{-1} \vmu. 
	\end{align*}
	
	This can be iteratively optimised  where with respect to $\vmu$ and $\mLambda$ using a Newton-Raphson style
	algorithm.

	\begin{align*}
		\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \vmu}     & \approx \mP \mC (\vy - \exp{(\mC \vmu)}) - \mSigma^{-1} \vmu \text{ and} \\
		\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \mLambda} & \approx - \mC^\top \text{diag}(\vp e^{(\mC \vmu)}) \mC - \mSigma^{-1}.   
	\end{align*}
	
	The steps of the algorithm are shown in Algorithm \ref{alg:laplace_alg}.
	
	\begin{algorithm}
		\caption{Laplace scheme for optimising $\log \underline{p}(\vmu, \mLambda; \vy)$}
		\label{alg:laplace_alg}
		\begin{algorithmic}
			% Fit \vmu, \mLambda using Laplace approximation
			\WHILE{the increase in $\log \underline{p}(\vmu, \mLambda; \vy)$ is significant}
			% \vmu, \mLambda
			\STATE $\mLambda \leftarrow \left [\mP \mC^\top \text{diag}(\exp{(\mC \vmu)}) \mC + \mSigma^{-1} \right ]^{-1}$ \\ [1ex] 
			\STATE $\vmu \leftarrow \vmu + \mLambda \left [ \frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \vmu} \right ]$ \\ [1ex]
			\ENDWHILE
		\end{algorithmic}
		where $\frac{\partial \log p(\vmu, \mLambda; \vy)}{\partial \mLambda} \approx - \mC^\top \text{diag}(\vp e^{(\mC \vmu)}) \mC - \mSigma^{-1}$.
	\end{algorithm}
	
	\subsection{Gaussian Variational Approximation}
	
	% Detail techniques used for fitting models.
	
	\subsubsection{Covariance parameterisation}
	
	Assuming $q(\vnu) \sim N(\vmu, \mLambda)$, the Gaussian variational lower bound in Algorithm
	\ref{alg:algorithm_one} can be optimised using a variety of algorithms. The variational lower bound is not
	necessarily unimodal if $\vp$ and $\mLambda$ are free to vary, leading to potential difficulty in optimising
	to the global maximum. However, for fixed $\vp$ and $\mSigma$, the variational lower bound is log-concave
	with respect to $\vmu$ and $\mLambda$, and so standard optimisation methods such as L-BFGS-B as described
	in, for example, \cite{Liu1989}, work well. This leads to an extremely accurate approximation of the true
	posterior at the expense of some additional computational effort.
	
	\noindent We fit the Gaussian part of our approximation $q(\vbeta, \vu) \sim \N(\vmu, \mLambda)$ by maximising 
	the variational lower bound	
	\begin{align*}
		\log \underline{p}(\vmu, \mLambda; \vy) & = \quad \vy^\top\mP \mC \vmu - \vp^\top \exp[\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)] - \half \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} + \log{|\mR|} \\
		                                        & \quad - \tfrac{p}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \tfrac{p}{2} \log{(2 \pi)} + \tfrac{p}{2}                                                                              \\
	\end{align*}

	\noindent with respect to $\vmu, \mLambda$, keeping $\vp, \mSigma, \rho$ fixed.
	
	\noindent To ensure symmetry of $\mLambda$, we parameterise the optimisation problem in terms of $\mLambda$'s
	Cholesky factor  $\mLambda = \mR^\top \mR$. We optimise over the space $(\vmu, \overline{\mR})$, where $\vmu
	\in \R^{p + mq}$ and $\overline{\mR}$ is a lower-triangular $(p + mq) \times (p + mq)$ matrix. Then
	
	\begin{equation*}
		\mR_{ij} =
		\begin{cases}
			\exp(\overline{\mR}_{ij}), & i = j             \\
			\overline{\mR}_{ij},       & i > j             \\
			0,                         & \text{otherwise}, 
		\end{cases}
	\end{equation*}
	
	\noindent exponentiating the diagonal to ensure positive-definiteness of $\mR$.
	
  \noindent This parameterisation can lead to numeric overflow when the diagonals of $\overline{\mR}$ become
	moderately large, which can lead to singular matrices when   attempting to invert $\mLambda$. We dealt with
	this issue by thresholding the values of the diagonals.  This   problem was mitigated by  instead working with
	the Cholesky factorisation of $\mLambda^{-1}$, allowing us to   solve rather than invert and multiply, which
	is also faster and more numerically stable. We use knowledge of   the regression model we are fitting to
	specify a sparse matrix structure, greatly reducing the dimension of   the problem and thus improving both
	computational speed and numeric accuracy.
	
	The first variant of the Gaussian Variational Approximation algorithm that we present optimises the Gaussian
	variational lower bound of the log likelihood with respect to $\vmu$ and the Cholesky decomposition $\mR$ of
	$\mLambda$, that is, $\mLambda = \mR \mR^\top$. This algorithm trades the computational complexity of
	numerically evaluating an integral for greatly increased accuracy in the approximating posterior
	distribution. The resulting function is below and can be optimised with L-BFGS-B.
	
	% Detail the function and its derivatives
	\begin{align*}
		\log \underline{p}(\vmu, \mLambda; \vy) & = \quad \vy^\top\mP \mC \vmu - \vp^\top \exp[\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)] - \half \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} + \log{|\mR|} \\
		                                        & \quad - \tfrac{p}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \tfrac{p}{2} \log{(2 \pi)} + \tfrac{p}{2}                                                                              \\
	\end{align*}
	
	using the derivatives
	\begin{align*}
		\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu}     & = \mP \mC (\vy - \mC^\top \exp(\mC \vmu + \half \text{diag}{(\mC \mLambda \mC^\top)})) - \mSigma^{-1} \vmu \text{ and}                \\
		\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} & = \left [\mLambda^{-1} - \mP \mC^\top \exp(\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)) \mP \mC) - \mSigma^{-1} \right ] \mR. 
	\end{align*}
	
	\subsubsection{Precision parameterisation}
	
	\noindent The second variant of the Gaussian Variational Approximation algorithm is similiar to the first, but
	instead of optimising the Gaussian variational lower bound with respect to $\vmu$ and the Cholesky factor
	$\mR$ of $\mLambda$, we instead optimise the Cholesky factor of the inverse of $\mLambda$ i.e. $\mLambda =
	(\mR \mR^\top)^{-1}$. We optimise over the space $(\vmu, \overline{\mR})$ as before, but now 
	
	\begin{equation*}
		\mR_{ij} =
		\begin{cases}
			\exp(-\overline{\mR}_{ij}), & i = j             \\
			\overline{\mR}_{ij},        & i > j             \\
			0,                          & \text{otherwise}, 
		\end{cases}
	\end{equation*}

  \noindent This new choice of parameterisation allows us to calculate $\half \text{diag}(\mC \mLambda
	\mC^\top)$ by solving the linear systems $\mR \va = \mC_{i}, i=1, \ldots, n$ for   $\va$ and then calculating
	$\va^\top\va$ where $\mC_{i} = $ the $i$th row of $\mC$, rather than calculating $\text{diag}(\mC \mLambda
	\mC^\top)$ directly.
	
	Any symmetric matrix $\mSigma$ can be written as a product of its' Cholesky factors, $\mSigma = \mR \mR^\top$
  where $\mR$ is lower triangular. $\mR$ is unique if $\mR_{ii} \geq 0$. 

  \begin{align*}
  	&\begin{pmatrix}
  	\mR_{11}          & 0                                    & 0                                     \\
  	\mR_{21}          & \mR_{22}                             & 0                                     \\
  	\mR_{31}          & \mR_{32}                             & \mR_{33}                              
  	\end{pmatrix}
  	\begin{pmatrix}
  	\mR_{11}          & \mR_{21}                             & \mR_{31}                              \\
  	0                 & \mR_{22}                             & \mR_{32}                              \\
  	0                 & 0                                    & \mR_{33}                              
  	\end{pmatrix}
  	\\
  	=& \begin{pmatrix}
  	\mR_{11}^2        &                                      & \text{symmetric}                      \\
  	\mR_{21}\mR_{11} & \mR_{21}^2 + \mR_{22}^2 \\
  	\mR_{31} \mR_{11} & \mR_{31}\mR_{21} + \mR_{32} \mR_{22} & \mR_{31}^2 + \mR_{32} ^2 + \mR_{33}^2 
  	\end{pmatrix}.
  \end{align*}

  Notice that the lower rows of the product depend on the higher rows of the Cholesky factor. By re-ordering the fixed and random effects in $\mLambda$, we arrive at a covariance structure which is sparse in the first diagonal block. Thus the Cholesky factor of $\mLambda$ that we optimise over is as sparse as possible. 

	\noindent By interchanging the fixed and random effects in $\mC = [\mX \mZ]$ to $\mC = [\mZ \mX]$, and re-
	ordering the dimensions of $\vmu, \mLambda$ and $\mSigma$ in the same manner, the independence between the
	blocks relating to the random effects in $\mZ$ induce sparsity in the Cholesky factor $\mR$ of
	$\mLambda^{-1}$. Thus the Gaussian $q(\vnu) \sim \N(\vmu, \mLambda)$ can be optimised over a space of dimension
	$\half p (p + 1) + pq + \half q (q + 1)$ rather than dimension $\half (p + mq) (p + mq + 1)$ as in the dense
	parameterisation. This leads to subtantial performance gains when $m$ is large, as is typically the case in
	problems of practical importance such as longitudinal or clinical trials or the application presented in
	Section \ref{sec:application}.
	
	We parameterise $\mLambda$ as $\mLambda = \mR \mR^\top$ so that is is guaranteed to be symmetric.
  $\frac{p(p-1)}{2}$ parameters to deal with instead of $p^2$ parameters.
	
	By re-ordering the fixed and random effects in $\mLambda$, we end up with a covariance structure
	which is sparse in the first diagonal block.

	\begin{figure}[p]
		\includegraphics[scale=.25]{mX_mZ_mLambda.pdf}
		\caption{\tiny Covariance matrix -- Fixed effects before random effects}
	\end{figure}

	\begin{figure}[p]
		\includegraphics[scale=.25]{mZ_mX_mLambda.pdf}
		\caption{\tiny Covariance matrix -- Random effects before fixed effects}
	\end{figure}
					      				      			      			      			      	
	\begin{figure}[p]
		\includegraphics[scale=.25]{mX_mZ_cholesky.pdf}
		\caption{\tiny Cholesky factor -- Fixed effects before random effects}
	\end{figure}

	\begin{figure}[p]
		\includegraphics[scale=.25]{mZ_mX_cholesky.pdf}
		\caption{\tiny Cholesky factor -- Random effects before fixed effects}
	\end{figure}
	
	\noindent The Gaussian variational lower bound in this parameterisation is
	\begin{align*}
		\log \underline{p}(\vmu, \mLambda; \vy) & = \quad \vy\mP\mC \vmu - \vp^\top \exp(\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)) - \half \vmu^\top \mSigma^{-1} \vmu - \half \tr{(\mSigma^{-1} \mLambda)} \\
		                                        & \quad- \tfrac{p}{2} \log{(2 \pi)} + \half \log{|\mSigma^{-1}|} + \tfrac{p}{2} \log{(2 \pi)} + \tfrac{p}{2} - \log{|\mR|}                                             
	\end{align*}
	
	\noindent The derivative with respect to $\vmu$ is the same as that in the GVA algorithm, but as the parameterisation of
	$\mLambda$ has changed, the  derivative with respect to $\mLambda$ becomes
	\begin{align*}
		\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda}
		  & = \hphantom{-}(\mLambda^{-1} + \mH)(-\mLambda \mR \mLambda) \\
		  & = -(\mI + \mH\mLambda)\mR\mLambda                           \\
		  & = - (\mR\mLambda + \mH\mLambda\mR\mLambda)                  
	\end{align*} 
	
	\noindent where $\mH = (\mP \mC)^\top \text{diag}(\exp(\mC \vmu + \half \mC \mLambda \mC^\top)) \mP \mC - \mSigma^{-1}$.
	
	\subsubsection{GVA fixed point}
	
	% Fixed point update of \mLambda
	
	This variant of the algorithm uses Newton-Raphson-like fixed point updates on the Gaussian variational lower
	bound. This algorithm is fast, but unstable. The steps are presented in Algorithm \ref{alg:algorithm_nr} where
	\begin{align*}
		\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu}     & = \quad \mC^\top\vp \left [\vy - \mC\exp(\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)) \right ] - \mSigma^{-1} \vmu \text{ and} \\
		\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \mLambda} & = -\mC^\top \text{diag}(\vp^\top \exp(\mC \vmu +\half \text{diag}(\mC \mLambda \mC^\top))) - \mSigma^{-1}.                             
	\end{align*}
	
	\begin{algorithm}
		\caption[Algorithm GVA NR]{Iterative scheme for obtaining optimal $\vmu$ and $\mLambda$
			given $\vy$, $\mC$ and $\vp$}
		\label{alg:algorithm_nr}
		\begin{algorithmic}
			% Fit \vmu, \mLambda using Laplace approximation
			\WHILE{the increase in $\log{\underline{p}}(\vmu, \mLambda; \vy)$ is significant}
			% \vmu, \mLambda
			\STATE $\mLambda \leftarrow \left [ \mP^\top \mC^\top \exp(\mC \vmu + \half \text{diag}(\mC \mLambda \mC^\top)) \mC \mP \right ]^{-1}$ \\ [1ex]
			\STATE $\vmu \leftarrow \vmu + \mLambda \left [ \frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu} \right ]$
			\ENDWHILE
		\end{algorithmic}
		where $\frac{\partial \log \underline{p}(\vmu, \mLambda; \vy)}{\partial \vmu} = \mP \mC (\vy - \mC^\top \exp(\mC \vmu + \half \text{diag}{(\mC \mLambda \mC^\top)})) - \mSigma^{-1} \vmu$.
	\end{algorithm}
	
	% Splines
	
	\section{Computational cost of Gaussian Variational Approximation approaches}

	The main computational cost is the evaluation of the variational lower bound and its' derivatives. By virtue
	of their dimension, the expressions involving $\mLambda$ dominate the computational cost. The key term is
	$\frac{1}{2} \diag(\mC \mLambda \mC^\top)$. The naive way to calculate this is to ignore the symmetry in
	this expression and simply calculate the product $\mC \mLambda \mC^\top$, which takes $2 n \times (p + m
	b)^2$ floating point operations, and take the diagonal entries of the result. This is obviously wasteful, as
	all of the off--diagonal entries of the resulting product are immediately discarded.

	By parameterising $\mLambda$ in terms of its' Cholesky factors and realising that

	\[
		\mC \mLambda \mC^\top = \mC \mR \mR^\top \mC^\top
	\]

	and that

	\[
		\diag(\mC \mLambda \mC^\top)_{ii} = \mC_{i .} \mR \mR^\top \mC_{i .}^\top, 1 \leq i \leq n
	\]

	we can calculate the products $\mC_{i .} \mR, 1 \leq i \leq n$, using $n \times \frac{1}{2}(p + m b)(p + m b
	+ 1)$ floating point operations, and store them in a vector $\va$ and then calculate $\diag(\mC \mLambda
	\mC^\top) = \va^\top \va$.

	Moreover, by exploiting our knwoledge of the model, we can encode $\mR$ as a sparse matrix, and further reduce
	this depending on the model. For example, in the random intercept case, only the diagonals of the random
	effects blog need to be non-zero, and hence the above expression can be performed in
	$n \times [m + \half{1}{2} p (p + 1) + p \times m b]$.

	$\log |\mR|$ can be calculated using only $p + m b$ floating point operations, as $\mR$ is lower triangular.

	For the precision parameterisation, we observe that in this parameterisation

	\[
		\diag(\mC \mLambda \mC^\top)_{ii} = \mC_{i .} \mR^{-T} \mR^{-1} \mC_{i .}^\top
	\]

	and so by solving

	\[
		\mR \va = \mC_{i .}^\top
	\]

	for $\va$ at a cost of $n \times \frac{1}{2} (p + m b) (p + m b + 1)$ floating point operations, and then
	calculate

	\[
		\diag(\mC \mLambda \mC^\top)_{ii} = \va^\top \va
	\]

	to complete the computation.

	As above, by using our knowledge of the model being fit we can encode $\mR$ sparsely to decreate the required
	computation still further. In the random intercept model case, the computational cost will drop to
	$n \times [m + \half{1}{2} p (p + 1) + p \times m b]$.

	\section{Numerical results}
	\label{sec:results}
	
	The accuracy of each model fitting algorithm presented in Section \ref{sec:algorithms} was assessed by
	comparing the approximating distribution of each parameter with the posterior distribution of Monte Carlo
	Markov Chain samples of that parameter. 1 million Monte Carlo Markov Chain samples were generated using Stan.
	The accuracy of examples of random intercept, random slope and spline models were evaluated using this method.
	
	\subsection{Simulated data}
	
	For each of these simulations, the model is as presented in Section \ref{sec:methodology}.
	
	\noindent Several common application scenarios were simulated and their accuracy evaluated. A random intercept model was simulated with $\vbeta = (2, 1)^\top$, $\rho = 0.5$, $m = 20$, $n_i = 10$ and $b = 1$. The results are
	presented in Table \ref{tab:accuracy_int}. A random slope model was simulated with $\vbeta = (2, 1)^\top$,
	$\rho = 0.5$, $m = 20$, $n_i = 10$ and $b = 2$. The results are presented in Table \ref{tab:accuracy_slope}.
	Spline model was fit to a data set generated from the function $3 + 3 \sin{(\pi x)}$ on the interval $[-1,
	1]$. The resulting model fits are presented in Figure \ref{fig:spline}.
	
	% The stability of the algorithms was confirmed by running them on 10,000 different data sets that were randomly
	% generated after having initialised the random number generator with different seeds.
	
	\noindent Median accuracy of the algorithms was assessed by running them on 100 randomly generated data sets. The	results are presented in Figure \ref{fig:median_accuracy_intercept} and Figure
	\ref{fig:median_accuracy_slope}.
	
	% Figure: Median accuracy graph intercept
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.7\textwidth, height=100mm]{code/results/median_accuracy_combined_intercept.pdf}
			\caption{Median accuracy of random intercept}
			\label{fig:median_accuracy_intercept}
		\end{center}
	\end{figure}
	
	% Figure: Median accuracy graph slope
	\begin{figure}
		\caption{Median accuracy of slope}
		\label{fig:median_accuracy_slope}
		\includegraphics[width=120mm, height=120mm]{code/results/median_accuracy_combined_slope.pdf}
	\end{figure}
	
	% Table of accuracy results - intercept model
	\begin{table}
		\caption{Table of accuracy - Random intercept model}
		\label{tab:accuracy_int}
		\begin{tabular}{|l|rrrr|}
			\hline
			                   & Laplace's Method & GVA $(\mLambda = \mR \mR^\top)$ & GVA NP $(\mLambda = (\mR \mR^\top)^{-1})$ & GVA FP   \\
			\hline
			$\vbeta_1$         & $83\%$           & $91\%$                          & $91\%$                                    & $91\%$   \\ 
			$\vbeta_2$         & $77\%$           & $99\%$                          & $99\%$                                    & $99\%$   \\ 
			Mean of $\vu$      & $81\%$           & $95\%$                          & $95\%$                                    & $95\%$   \\
			$\sigma^2_{\vu_1}$ & $63.0\%$         & $63.4\%$                        & $63.4\%$                                  & $63.4\%$ \\ 
			$\rho$             & $98\%$           & $97\%$                          & $97\%$                                    & $97\%$   \\ 
			\hline
		\end{tabular}
	\end{table}
	
	\begin{table}
		\caption{Table of accuracy - Random slope model}
		\label{tab:accuracy_slope}
		\begin{tabular}{|l|rrrr|}
			\hline
			                   & Laplace's Method & GVA $(\mLambda = \mR \mR^\top)$ & GVA $(\mLambda = (\mR \mR^\top)^{-1})$ & GVA FP \\
			\hline
			$\vbeta_1$         & 66\%             & 88\%                            & 89\%                                   & 88\%   \\
			$\vbeta_2$         & 69\%             & 88\%                            & 90\%                                   & 88\%   \\
			Mean of $\vu$      & 72\%             & 91\%                            & 91\%                                   & 91\%   \\
			$\sigma^2_{\vu_1}$ & 69.6\%           & 71.5\%                          & 71.5\%                                 & 71.5\% \\
			$\sigma^2_{\vu_2}$ & 65.9\%           & 67.6\%                          & 67.6\%                                 & 67.6\% \\
			$\rho$             & 91\%             & 90\%                            & 90\%                                   & 90\%   \\
			\hline
		\end{tabular}
	\end{table}
	
	% \begin{table}
	% \caption{Table of accuracy - Splines}
	% \label{tab:accuracy_spline}
	% \begin{tabular}{|l|l|}
	% \hline
	% Approximation & Accuracy \\
	% \hline
	% Laplace's Method & 0.969 \\
	% GVA & 0.969 \\
	% GVA NP & 0.969 \\
	% GVA NR & 0.969 \\
	% \hline
	% \end{tabular}
	% \end{table}
	
	\begin{figure}
		\label{fig:spline}
		\caption{Comparison of VB and MCMC spline fits with the true function}
		\includegraphics[width=100mm, height=100mm]{code/results/accuracy_plots_spline_gva2.pdf}
	\end{figure}
	
	% Graphs - exactly what sort of graphs do we need?
	% Median accuracy
	% Increase in lower bound
	% MCMC posterior, with approximating posterior for at least one or two of the
	% key parameters, such as, say, vbeta[2]
	
	\subsection{Stability results}
	
	The numerical stability of each fitting algorithm in Section \ref{sec:algorithms} was assessed by initialising
	each algorithm from a range of different starting points. Errors due to numerical instability and the fitted
	$\vmu$ were recorded for each starting point.
	
	\noindent A data set of 100 individuals in ten groups (m=10) was generated from a model with a fixed intercept
	and slope, and a random intercept. $\vmu$ was initialised from a grid of points on the interval
	$[-4.5, 5]$ for intercept and slope. The error counts are presented in Table
	\ref{tab:stability_results}.
	
	\begin{table}
		\caption{Count of numerical errors for each algorithm during stability tests}
		\label{tab:stability_results}
		\begin{tabular}{|l|r|}
			\hline
			Algorithm           & Error count \\
			\hline
			Laplace's algorithm & 12          \\
			GVA                 & 1,771       \\
			GVA NP              & 537         \\
			GVA FP              & 992         \\
			\hline
		\end{tabular}
	\end{table}
	
	\section{Application}
	\label{sec:application}
	
	% TODO: You need to describe the data set and the model.
	
	The model fitting was applied to the cockroach data set from \cite{Gelman2007} taken from a study on the
	effect of integrated pest management in controlling cockroach levels in urban apartments. The data set
	contains data on 160 treatment and 104 control apartments, along with the response $y_i$ in each apartment of
	the number of cockroaches caught in a set of traps. The apartments had the traps deployed for different
	numbers of days, referred to as trap days, which was handled by using a log offset \cite{Agresti2002}. The
	predictors in the data set included the pre-treatment roach level, a treatment indicator, the time of the
	observation and an indicator for whether the apartment is in a senior building restricted to the elderly.
	
	\noindent In the example application presented in this paper, the zero component represents an apartment completely free of roaches, while the non-zero component represents an apartment where roaches have been able to live and reproduce, possibly in spite of pest control treatment aimed at preventing them from doing so.
	
	\noindent The GVA NP algorithm was used to fit a random intercept model to the Roaches data set provided by Andrew Gelman. The fitted co-efficients and accuracy results are presented in Table
	\ref{tab:application_roaches}.
	
	%       lci  uci
	% 1  3.179 3.157 3.201
	% 2 -0.046 -0.053 -0.039
	% 3 -0.420 -0.434 -0.406
	% 1 -0.976 -1.015 -0.936
	% 2 -0.309 -0.323 -0.295
	% 3 -0.947 -0.963 -0.930
	% 4 -2.129 -2.384 -1.874
	% 5 -3.230 -3.490 -2.970
	% 6 -3.099 -3.404 -2.794
	% 7 -1.290 -1.326 -1.255
	% 8 -0.956 -0.991 -0.921
	% 9 -2.404 -2.600 -2.209
	% 10 -1.076 -1.123 -1.029
	% 11 -1.079 -1.107 -1.052
	% 12 -1.681 -1.737 -1.624
	
	%> round(cbind(fit1$vmu, lci, uci), 3)
	% fit1$a_rho
	% [1] 377.2375
	% > fit1$b_rho
	% [1] 152.7625
	
	\begin{table}
		\caption{Table of results - Roaches}
		\label{tab:application_roaches}
		\begin{tabular}{|l|rrrr|}
			\hline
			Covariate          & Posterior Mean & Lower 95\% CI & Upper 95\% CI & Accuracy \\
			\hline
			Intercept          & 3.18           & 3.16          & 3.20          & 90\%     \\
			Time               & $-$0.05        & $-$0.05       & $-$0.04       & 97\%     \\
			Time:Treatment     & $-$0.43        & $-$0.43       & $-$0.41       & 93\%     \\
			Random intercept   & $-$1.60        & $-$1.71       & $-$1.49       & 90\%     \\
			$\sigma^2_{\vu_1}$ & 0.58           & 0.57          & 0.57          & 57\%     \\
			$\rho$             & 0.71           & 0.67          & 0.75          & 88\%     \\
			\hline
		\end{tabular}
	\end{table}
	
	\begin{figure}
		\caption{Accuracy graphs for roach model}
		\label{fig:accuracy_roach}
		\centering
		% \includepdf[width=75mm,height=75mm,pages={1,2,3,16},nup=2x2]{code/results/accuracy_plots_application_GVA2.pdf}
		\begin{tabular}{@{}c@{\hspace{.5cm}}c@{}}
			\includegraphics[page=1,width=.45\textwidth]{code/results/accuracy_plots_application_GVA_NP.pdf} &   
			\includegraphics[page=2,width=.45\textwidth]{code/results/accuracy_plots_application_GVA_NP.pdf} \\[.5cm]
			\includegraphics[page=3,width=.45\textwidth]{code/results/accuracy_plots_application_GVA_NP.pdf} &   
			\includegraphics[page=16,width=.45\textwidth]{code/results/accuracy_plots_application_GVA_NP.pdf} \\[.5cm]
		\end{tabular}
	\end{figure}
	
	\noindent To assess the speed of each approach, a test case was constructed of a random slope model with $m=50$ groups,	each containing $n_i = 100$ individuals. A model was then fit to this data set ten times using each algorithm, and the results averaged. They are presented in Table \ref{tab:application_slope_speed}.
	
	\begin{table}
		\caption{Table of results - Speed}
		\label{tab:application_slope_speed}
		\begin{tabular}{|l|rr|}
			\hline
			Algorithm        & Mean (seconds) & Standard deviation (seconds) \\
			\hline
			Laplace's method & 18.79 s        & 0.07 s                       \\
			GVA              & 76.18 s        & 1.24 s                       \\
			GVA NP           & 27.55 s        & 0.66 s                       \\
			GVA FP           & 4.83 s         & 0.07 s                       \\
			\hline
		\end{tabular}
	\end{table}
	
	\section{Discussion}
	\label{sec:discussion}
	
	We have described a Variational Bayes approximation to Zero-Inflated Poisson regression models which allows
	such models to be fit with considerable generality. We have also devised and extensively tested a number of
	alternative approaches for fitting such models, and extended one of these alternative approaches with a new
	parameterisation. Using MCMC methods as the gold standard to test against, we have assessed the accuracy and
	computational speed of these algoritms.
	
	\noindent The use of Mean Field Variational Bayes allows estimation of Bayesian ZIP models in a fraction of the time taken to fit the same model using even the best MCMC methods available, with only a small loss of accuracy.
	This is of great utility in applications where speed matters, such as model selection or when applied
	statisticians are experimenting with many models, as is typical in practice.
	
	\noindent The new parameterisation of Gaussian Variational Approximation using the Cholesky factorisation of the inverse of $\mLambda$ presented in Section \ref{sec:algorithms} provides significant advantages.  It is well known that the inverse of a sparse matrix need not be sparse. Fitting mixed models generally leads to covariance matrices with an arrow head structure. Due to the sparsity of our parameterisation of $\mLambda^{-1}$,  our algorithm for fitting these models leads to an optimisation problem of significantly lower dimension. This allows the model to be fit more quickly, and with greatly improved numerical stability and with no loss of
	accuracy.
	
	\noindent The implementation of these algorithms was not without its' challenges, chiefly numerical issues encountered during testing and verification of the accuracy of the model fitting. Using the exponential function to parameterise the main diagonal coupled with L-BFGS-B's unconstrained line search and optim()'s lack of robustness to infinities lead to many overflow problems which may have been lessened or dealt with entirely by using a function with a less aggressive growth rate, such as an appropriate piecewise quadratic.
	
	Some of the algorithms which we experimented with were found to be very sensitive to their starting points.
	While these algorithms are typically initialised with a starting point as close as possible to the final
	solution, this gives some sense of the stability of each algorithm.
	
	This article presents the essential ideas necessary for a performant implementation implementing model fitting
	for ZIP regression models, but the performance would be even better if our algorithm was re-implemented in a
	compiled language with good numeric libraries such as C++ with Eigen. The majority of the performance
	improvements over existing approaches come from avoiding unneccessary matrix inversion, which is a
	computationally expensive $O(p^3)$ and numerically unstable process, and from constructing and calculating
	with sparse matrices. The gains of these approaches, particularly from sparse matrix techniques, can be
	difficult to fully realise in R without expert knowledge of the underlying implementation and libraries.
	
	Our application of these ideas to Andrew Gelman's data showed that the new parameterisation very effectively
	speeds up fitting zero-inflated mixed models to real world data with a large number of groups, while still
	maintaining excellent accuracy versus an MCMC approach. This demonstrates the applicability of the ideas
	presented within this paper to real world data sets.
	
	\newpage
	\section{Appendix} 
	% TODO: Mean field updates?
	\subsection{Calculation of the Variational Lower bound}
	% Where are the priors for \vbeta and \vu
	
	The variational lower bound is equal to $\bE_q[\log{p(\vy, \vtheta)} - \log{q(\vtheta)}] = T_1 + T_2 + T_3$,
	where
	
	% This is the new T_1
	$$
	\begin{array}{rl}
		T_1 & = \quad \bE_q[\log{p(\vy, \vnu)} - \log{q(\vnu)}]                                                                                                                                                  \\
		    & = \quad \vy \mP \mC \vmu - \vp^\top \exp{\left[ \mC \vmu + \half \text{diag} (\mC \mLambda \mC^\top) \right]} - \vone^\top\log \Gamma{(\vy + \vone)}                                               \\
		    & \quad + \frac{p + m}{2} (1 + \log{2 \pi}) + \half \log{|\mLambda|},                                                                                                                                \\
		T_2 & = \quad \bE_q \left[ \log p(\mSigma_{\vu \vu}) - \log q(\mSigma_{\vu \vu}) \right]                                                                                                                 \\
		    & = \quad \bE_q \big[ v/2(\log |\Psi| - \log |\Psi + \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu}|) + \half \log 2 + \half \log|\mSigma_{\vu \vu}| + \log \Gamma_{p+1}(v/2) - \log \Gamma_{p}(v/2)    \\
		    & \quad + \half \tr((\vmu_{\vu} \vmu_{\vu}^\top + \mLambda_{\vu \vu}) \mSigma_{\vu \vu}^{-1}) \big]                                                                                                  \\
		    & = \quad v/2\big(\log |\Psi| - \log |\Psi + \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu}|\big) + \half \log 2 + \half \bE_q \log |\mSigma_{\vu \vu}| + \log \Gamma_{p+1}(v/2) - \log \Gamma_{p}(v/2) \\
		    & \quad + \half \tr\big(\mI_m + \Psi(\Psi+ \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu})^{-1}/(v + p + 2)\big)                                                                                        \\
		T_3 & = - \vp^\top \log \vp - (\vone - \vp)^\top \log (\vone - \vp) - \log \Beta (\alpha_\rho, \beta_\rho) + \log \Beta (\alpha_q, \beta_q)                                                              
	\end{array}
	$$
	
	with $\bE_q \big[ \log |\mSigma_{\vu \vu}| \big] = m \log 2 + \log \left | \Psi + \vmu_\vu \vmu_\vu^\top + \mLambda_{\vu \vu} \right | + \sum_{i=1}^m \Psi \left ( \frac{v - i + 1}{2} \right )$
	
	\subsection{Numerical stability of fitting algorithms with respect to starting point}
	
	% TODO: Generate images using local_solutions.R and place here
	
	% \bibliographystyle{elsarticle-harv}
	\bibliography{Chapter_1_zero_inflated_models}
	
\end{document}
